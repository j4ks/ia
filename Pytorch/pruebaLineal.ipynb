{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[50.0000],\n",
       "         [49.8000],\n",
       "         [49.6000],\n",
       "         [49.4000],\n",
       "         [49.2000],\n",
       "         [49.0000],\n",
       "         [48.8000],\n",
       "         [48.6000],\n",
       "         [48.4000],\n",
       "         [48.2000]]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create *known* parameters\n",
    "weight = -10.0\n",
    "bias = 50.0\n",
    "\n",
    "# Create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "  \n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14});\n",
    "plot_predictions();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "    \n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([-1.1224], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0312], requires_grad=True)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set manual seed since nn.Parameter are randomly initialzied\n",
    "torch.manual_seed(45)\n",
    "\n",
    "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check the nn.Parameter(s) within the nn.Module subclass we created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([-1.1224])), ('bias', tensor([0.0312]))])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters \n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing samples: 10\n",
      "Number of predictions made: 10\n",
      "Predicted values:\n",
      "tensor([[-0.8667],\n",
      "        [-0.8892],\n",
      "        [-0.9116],\n",
      "        [-0.9341],\n",
      "        [-0.9565],\n",
      "        [-0.9790],\n",
      "        [-1.0014],\n",
      "        [-1.0239],\n",
      "        [-1.0463],\n",
      "        [-1.0687]])\n"
     ]
    }
   ],
   "source": [
    "# Check the predictions\n",
    "print(f\"Number of testing samples: {len(X_test)}\") \n",
    "print(f\"Number of predictions made: {len(y_preds)}\")\n",
    "print(f\"Predicted values:\\n{y_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 46.506553649902344 | MAE Test Loss: 42.05426788330078 \n",
      "Epoch: 10 | MAE Train Loss: 46.39133834838867 | MAE Test Loss: 41.9195556640625 \n",
      "Epoch: 20 | MAE Train Loss: 46.27613067626953 | MAE Test Loss: 41.78484344482422 \n",
      "Epoch: 30 | MAE Train Loss: 46.16092300415039 | MAE Test Loss: 41.6501350402832 \n",
      "Epoch: 40 | MAE Train Loss: 46.04570770263672 | MAE Test Loss: 41.515419006347656 \n",
      "Epoch: 50 | MAE Train Loss: 45.93050003051758 | MAE Test Loss: 41.380714416503906 \n",
      "Epoch: 60 | MAE Train Loss: 45.81528854370117 | MAE Test Loss: 41.246002197265625 \n",
      "Epoch: 70 | MAE Train Loss: 45.70008087158203 | MAE Test Loss: 41.11129379272461 \n",
      "Epoch: 80 | MAE Train Loss: 45.584869384765625 | MAE Test Loss: 40.976585388183594 \n",
      "Epoch: 90 | MAE Train Loss: 45.46965789794922 | MAE Test Loss: 40.84186935424805 \n",
      "Epoch: 100 | MAE Train Loss: 45.35444641113281 | MAE Test Loss: 40.70716094970703 \n",
      "Epoch: 110 | MAE Train Loss: 45.239234924316406 | MAE Test Loss: 40.57245635986328 \n",
      "Epoch: 120 | MAE Train Loss: 45.1240234375 | MAE Test Loss: 40.437744140625 \n",
      "Epoch: 130 | MAE Train Loss: 45.008819580078125 | MAE Test Loss: 40.30303192138672 \n",
      "Epoch: 140 | MAE Train Loss: 44.89360809326172 | MAE Test Loss: 40.1683235168457 \n",
      "Epoch: 150 | MAE Train Loss: 44.77840042114258 | MAE Test Loss: 40.03361892700195 \n",
      "Epoch: 160 | MAE Train Loss: 44.66319274902344 | MAE Test Loss: 39.898902893066406 \n",
      "Epoch: 170 | MAE Train Loss: 44.54798126220703 | MAE Test Loss: 39.76419448852539 \n",
      "Epoch: 180 | MAE Train Loss: 44.43277359008789 | MAE Test Loss: 39.629486083984375 \n",
      "Epoch: 190 | MAE Train Loss: 44.31755828857422 | MAE Test Loss: 39.494773864746094 \n",
      "Epoch: 200 | MAE Train Loss: 44.20235061645508 | MAE Test Loss: 39.360069274902344 \n",
      "Epoch: 210 | MAE Train Loss: 44.08713912963867 | MAE Test Loss: 39.22535705566406 \n",
      "Epoch: 220 | MAE Train Loss: 43.97193145751953 | MAE Test Loss: 39.09064865112305 \n",
      "Epoch: 230 | MAE Train Loss: 43.856719970703125 | MAE Test Loss: 38.955936431884766 \n",
      "Epoch: 240 | MAE Train Loss: 43.741512298583984 | MAE Test Loss: 38.82122802734375 \n",
      "Epoch: 250 | MAE Train Loss: 43.62630081176758 | MAE Test Loss: 38.68651580810547 \n",
      "Epoch: 260 | MAE Train Loss: 43.51108932495117 | MAE Test Loss: 38.55180740356445 \n",
      "Epoch: 270 | MAE Train Loss: 43.39588165283203 | MAE Test Loss: 38.41709518432617 \n",
      "Epoch: 280 | MAE Train Loss: 43.28067398071289 | MAE Test Loss: 38.282386779785156 \n",
      "Epoch: 290 | MAE Train Loss: 43.165462493896484 | MAE Test Loss: 38.147674560546875 \n",
      "Epoch: 300 | MAE Train Loss: 43.050254821777344 | MAE Test Loss: 38.01296615600586 \n",
      "Epoch: 310 | MAE Train Loss: 42.93504333496094 | MAE Test Loss: 37.87825393676758 \n",
      "Epoch: 320 | MAE Train Loss: 42.81983184814453 | MAE Test Loss: 37.74354553222656 \n",
      "Epoch: 330 | MAE Train Loss: 42.70462417602539 | MAE Test Loss: 37.60883331298828 \n",
      "Epoch: 340 | MAE Train Loss: 42.58940887451172 | MAE Test Loss: 37.474124908447266 \n",
      "Epoch: 350 | MAE Train Loss: 42.47420120239258 | MAE Test Loss: 37.33941650390625 \n",
      "Epoch: 360 | MAE Train Loss: 42.35899353027344 | MAE Test Loss: 37.204708099365234 \n",
      "Epoch: 370 | MAE Train Loss: 42.24378204345703 | MAE Test Loss: 37.06999588012695 \n",
      "Epoch: 380 | MAE Train Loss: 42.12857437133789 | MAE Test Loss: 36.9352912902832 \n",
      "Epoch: 390 | MAE Train Loss: 42.013362884521484 | MAE Test Loss: 36.80057907104492 \n",
      "Epoch: 400 | MAE Train Loss: 41.89815139770508 | MAE Test Loss: 36.665870666503906 \n",
      "Epoch: 410 | MAE Train Loss: 41.78293991088867 | MAE Test Loss: 36.531150817871094 \n",
      "Epoch: 420 | MAE Train Loss: 41.667728424072266 | MAE Test Loss: 36.396446228027344 \n",
      "Epoch: 430 | MAE Train Loss: 41.552513122558594 | MAE Test Loss: 36.2617301940918 \n",
      "Epoch: 440 | MAE Train Loss: 41.43730163574219 | MAE Test Loss: 36.127017974853516 \n",
      "Epoch: 450 | MAE Train Loss: 41.32209014892578 | MAE Test Loss: 35.992305755615234 \n",
      "Epoch: 460 | MAE Train Loss: 41.206878662109375 | MAE Test Loss: 35.85759353637695 \n",
      "Epoch: 470 | MAE Train Loss: 41.09166717529297 | MAE Test Loss: 35.72288513183594 \n",
      "Epoch: 480 | MAE Train Loss: 40.9764518737793 | MAE Test Loss: 35.588165283203125 \n",
      "Epoch: 490 | MAE Train Loss: 40.86124038696289 | MAE Test Loss: 35.453453063964844 \n",
      "Epoch: 500 | MAE Train Loss: 40.74602508544922 | MAE Test Loss: 35.31874465942383 \n",
      "Epoch: 510 | MAE Train Loss: 40.63081741333008 | MAE Test Loss: 35.18403244018555 \n",
      "Epoch: 520 | MAE Train Loss: 40.515602111816406 | MAE Test Loss: 35.04932403564453 \n",
      "Epoch: 530 | MAE Train Loss: 40.400390625 | MAE Test Loss: 34.914608001708984 \n",
      "Epoch: 540 | MAE Train Loss: 40.285179138183594 | MAE Test Loss: 34.7798957824707 \n",
      "Epoch: 550 | MAE Train Loss: 40.16996383666992 | MAE Test Loss: 34.64518356323242 \n",
      "Epoch: 560 | MAE Train Loss: 40.05475616455078 | MAE Test Loss: 34.510467529296875 \n",
      "Epoch: 570 | MAE Train Loss: 39.93954086303711 | MAE Test Loss: 34.37575912475586 \n",
      "Epoch: 580 | MAE Train Loss: 39.82433319091797 | MAE Test Loss: 34.24104309082031 \n",
      "Epoch: 590 | MAE Train Loss: 39.7091178894043 | MAE Test Loss: 34.10633087158203 \n",
      "Epoch: 600 | MAE Train Loss: 39.59390640258789 | MAE Test Loss: 33.971622467041016 \n",
      "Epoch: 610 | MAE Train Loss: 39.47869110107422 | MAE Test Loss: 33.83690643310547 \n",
      "Epoch: 620 | MAE Train Loss: 39.36348342895508 | MAE Test Loss: 33.70219039916992 \n",
      "Epoch: 630 | MAE Train Loss: 39.248268127441406 | MAE Test Loss: 33.567481994628906 \n",
      "Epoch: 640 | MAE Train Loss: 39.133056640625 | MAE Test Loss: 33.43276596069336 \n",
      "Epoch: 650 | MAE Train Loss: 39.01784133911133 | MAE Test Loss: 33.298057556152344 \n",
      "Epoch: 660 | MAE Train Loss: 38.90262985229492 | MAE Test Loss: 33.1633415222168 \n",
      "Epoch: 670 | MAE Train Loss: 38.78742218017578 | MAE Test Loss: 33.02863693237305 \n",
      "Epoch: 680 | MAE Train Loss: 38.67220687866211 | MAE Test Loss: 32.893917083740234 \n",
      "Epoch: 690 | MAE Train Loss: 38.5569953918457 | MAE Test Loss: 32.75920486450195 \n",
      "Epoch: 700 | MAE Train Loss: 38.44178009033203 | MAE Test Loss: 32.62449264526367 \n",
      "Epoch: 710 | MAE Train Loss: 38.326568603515625 | MAE Test Loss: 32.48978042602539 \n",
      "Epoch: 720 | MAE Train Loss: 38.21135711669922 | MAE Test Loss: 32.355064392089844 \n",
      "Epoch: 730 | MAE Train Loss: 38.09614181518555 | MAE Test Loss: 32.22035598754883 \n",
      "Epoch: 740 | MAE Train Loss: 37.98093032836914 | MAE Test Loss: 32.085636138916016 \n",
      "Epoch: 750 | MAE Train Loss: 37.86571502685547 | MAE Test Loss: 31.950927734375 \n",
      "Epoch: 760 | MAE Train Loss: 37.75050354003906 | MAE Test Loss: 31.81621742248535 \n",
      "Epoch: 770 | MAE Train Loss: 37.63529586791992 | MAE Test Loss: 31.681503295898438 \n",
      "Epoch: 780 | MAE Train Loss: 37.52008056640625 | MAE Test Loss: 31.54679298400879 \n",
      "Epoch: 790 | MAE Train Loss: 37.404869079589844 | MAE Test Loss: 31.412078857421875 \n",
      "Epoch: 800 | MAE Train Loss: 37.28965377807617 | MAE Test Loss: 31.27736473083496 \n",
      "Epoch: 810 | MAE Train Loss: 37.17444610595703 | MAE Test Loss: 31.142654418945312 \n",
      "Epoch: 820 | MAE Train Loss: 37.059226989746094 | MAE Test Loss: 31.0079402923584 \n",
      "Epoch: 830 | MAE Train Loss: 36.94402313232422 | MAE Test Loss: 30.873226165771484 \n",
      "Epoch: 840 | MAE Train Loss: 36.82880401611328 | MAE Test Loss: 30.738515853881836 \n",
      "Epoch: 850 | MAE Train Loss: 36.713592529296875 | MAE Test Loss: 30.603801727294922 \n",
      "Epoch: 860 | MAE Train Loss: 36.59838104248047 | MAE Test Loss: 30.469085693359375 \n",
      "Epoch: 870 | MAE Train Loss: 36.4831657409668 | MAE Test Loss: 30.334375381469727 \n",
      "Epoch: 880 | MAE Train Loss: 36.36795425415039 | MAE Test Loss: 30.199665069580078 \n",
      "Epoch: 890 | MAE Train Loss: 36.252742767333984 | MAE Test Loss: 30.064950942993164 \n",
      "Epoch: 900 | MAE Train Loss: 36.13753128051758 | MAE Test Loss: 29.930240631103516 \n",
      "Epoch: 910 | MAE Train Loss: 36.022315979003906 | MAE Test Loss: 29.7955265045166 \n",
      "Epoch: 920 | MAE Train Loss: 35.907108306884766 | MAE Test Loss: 29.660808563232422 \n",
      "Epoch: 930 | MAE Train Loss: 35.791893005371094 | MAE Test Loss: 29.526098251342773 \n",
      "Epoch: 940 | MAE Train Loss: 35.67668533325195 | MAE Test Loss: 29.391387939453125 \n",
      "Epoch: 950 | MAE Train Loss: 35.56147003173828 | MAE Test Loss: 29.256677627563477 \n",
      "Epoch: 960 | MAE Train Loss: 35.446258544921875 | MAE Test Loss: 29.121963500976562 \n",
      "Epoch: 970 | MAE Train Loss: 35.33104705810547 | MAE Test Loss: 28.98724365234375 \n",
      "Epoch: 980 | MAE Train Loss: 35.2158317565918 | MAE Test Loss: 28.8525333404541 \n",
      "Epoch: 990 | MAE Train Loss: 35.100616455078125 | MAE Test Loss: 28.717823028564453 \n",
      "Epoch: 1000 | MAE Train Loss: 34.98540496826172 | MAE Test Loss: 28.58310890197754 \n",
      "Epoch: 1010 | MAE Train Loss: 34.87019729614258 | MAE Test Loss: 28.44839859008789 \n",
      "Epoch: 1020 | MAE Train Loss: 34.754981994628906 | MAE Test Loss: 28.313684463500977 \n",
      "Epoch: 1030 | MAE Train Loss: 34.639766693115234 | MAE Test Loss: 28.178974151611328 \n",
      "Epoch: 1040 | MAE Train Loss: 34.52455520629883 | MAE Test Loss: 28.04425621032715 \n",
      "Epoch: 1050 | MAE Train Loss: 34.40934371948242 | MAE Test Loss: 27.9095458984375 \n",
      "Epoch: 1060 | MAE Train Loss: 34.29413604736328 | MAE Test Loss: 27.77483558654785 \n",
      "Epoch: 1070 | MAE Train Loss: 34.17892074584961 | MAE Test Loss: 27.640121459960938 \n",
      "Epoch: 1080 | MAE Train Loss: 34.0637092590332 | MAE Test Loss: 27.505407333374023 \n",
      "Epoch: 1090 | MAE Train Loss: 33.94849395751953 | MAE Test Loss: 27.37069320678711 \n",
      "Epoch: 1100 | MAE Train Loss: 33.833282470703125 | MAE Test Loss: 27.235980987548828 \n",
      "Epoch: 1110 | MAE Train Loss: 33.71807098388672 | MAE Test Loss: 27.101268768310547 \n",
      "Epoch: 1120 | MAE Train Loss: 33.60285568237305 | MAE Test Loss: 26.9665584564209 \n",
      "Epoch: 1130 | MAE Train Loss: 33.48764419555664 | MAE Test Loss: 26.831844329833984 \n",
      "Epoch: 1140 | MAE Train Loss: 33.37242889404297 | MAE Test Loss: 26.697132110595703 \n",
      "Epoch: 1150 | MAE Train Loss: 33.25722122192383 | MAE Test Loss: 26.562419891357422 \n",
      "Epoch: 1160 | MAE Train Loss: 33.142005920410156 | MAE Test Loss: 26.42770767211914 \n",
      "Epoch: 1170 | MAE Train Loss: 33.02679443359375 | MAE Test Loss: 26.292993545532227 \n",
      "Epoch: 1180 | MAE Train Loss: 32.91157913208008 | MAE Test Loss: 26.158283233642578 \n",
      "Epoch: 1190 | MAE Train Loss: 32.79636764526367 | MAE Test Loss: 26.023569107055664 \n",
      "Epoch: 1200 | MAE Train Loss: 32.68115997314453 | MAE Test Loss: 25.88885498046875 \n",
      "Epoch: 1210 | MAE Train Loss: 32.56594467163086 | MAE Test Loss: 25.7541446685791 \n",
      "Epoch: 1220 | MAE Train Loss: 32.45073318481445 | MAE Test Loss: 25.619426727294922 \n",
      "Epoch: 1230 | MAE Train Loss: 32.33551788330078 | MAE Test Loss: 25.484716415405273 \n",
      "Epoch: 1240 | MAE Train Loss: 32.220306396484375 | MAE Test Loss: 25.350004196166992 \n",
      "Epoch: 1250 | MAE Train Loss: 32.10509490966797 | MAE Test Loss: 25.215293884277344 \n",
      "Epoch: 1260 | MAE Train Loss: 31.989879608154297 | MAE Test Loss: 25.080577850341797 \n",
      "Epoch: 1270 | MAE Train Loss: 31.874670028686523 | MAE Test Loss: 24.945865631103516 \n",
      "Epoch: 1280 | MAE Train Loss: 31.75945472717285 | MAE Test Loss: 24.811153411865234 \n",
      "Epoch: 1290 | MAE Train Loss: 31.644241333007812 | MAE Test Loss: 24.676441192626953 \n",
      "Epoch: 1300 | MAE Train Loss: 31.529027938842773 | MAE Test Loss: 24.541728973388672 \n",
      "Epoch: 1310 | MAE Train Loss: 31.413818359375 | MAE Test Loss: 24.40701675415039 \n",
      "Epoch: 1320 | MAE Train Loss: 31.298608779907227 | MAE Test Loss: 24.272302627563477 \n",
      "Epoch: 1330 | MAE Train Loss: 31.183395385742188 | MAE Test Loss: 24.137590408325195 \n",
      "Epoch: 1340 | MAE Train Loss: 31.06818199157715 | MAE Test Loss: 24.00287628173828 \n",
      "Epoch: 1350 | MAE Train Loss: 30.952966690063477 | MAE Test Loss: 23.8681640625 \n",
      "Epoch: 1360 | MAE Train Loss: 30.837759017944336 | MAE Test Loss: 23.733449935913086 \n",
      "Epoch: 1370 | MAE Train Loss: 30.722543716430664 | MAE Test Loss: 23.598739624023438 \n",
      "Epoch: 1380 | MAE Train Loss: 30.60733413696289 | MAE Test Loss: 23.464025497436523 \n",
      "Epoch: 1390 | MAE Train Loss: 30.49212074279785 | MAE Test Loss: 23.329313278198242 \n",
      "Epoch: 1400 | MAE Train Loss: 30.376907348632812 | MAE Test Loss: 23.19460105895996 \n",
      "Epoch: 1410 | MAE Train Loss: 30.261693954467773 | MAE Test Loss: 23.059886932373047 \n",
      "Epoch: 1420 | MAE Train Loss: 30.146480560302734 | MAE Test Loss: 22.925172805786133 \n",
      "Epoch: 1430 | MAE Train Loss: 30.031269073486328 | MAE Test Loss: 22.790462493896484 \n",
      "Epoch: 1440 | MAE Train Loss: 29.916057586669922 | MAE Test Loss: 22.655750274658203 \n",
      "Epoch: 1450 | MAE Train Loss: 29.80084800720215 | MAE Test Loss: 22.521038055419922 \n",
      "Epoch: 1460 | MAE Train Loss: 29.68563461303711 | MAE Test Loss: 22.38632583618164 \n",
      "Epoch: 1470 | MAE Train Loss: 29.570423126220703 | MAE Test Loss: 22.251611709594727 \n",
      "Epoch: 1480 | MAE Train Loss: 29.455209732055664 | MAE Test Loss: 22.116899490356445 \n",
      "Epoch: 1490 | MAE Train Loss: 29.339996337890625 | MAE Test Loss: 21.982187271118164 \n",
      "Epoch: 1500 | MAE Train Loss: 29.224782943725586 | MAE Test Loss: 21.84747314453125 \n",
      "Epoch: 1510 | MAE Train Loss: 29.109569549560547 | MAE Test Loss: 21.712757110595703 \n",
      "Epoch: 1520 | MAE Train Loss: 28.99435806274414 | MAE Test Loss: 21.578046798706055 \n",
      "Epoch: 1530 | MAE Train Loss: 28.8791446685791 | MAE Test Loss: 21.443334579467773 \n",
      "Epoch: 1540 | MAE Train Loss: 28.763931274414062 | MAE Test Loss: 21.308622360229492 \n",
      "Epoch: 1550 | MAE Train Loss: 28.648717880249023 | MAE Test Loss: 21.173908233642578 \n",
      "Epoch: 1560 | MAE Train Loss: 28.53350830078125 | MAE Test Loss: 21.039196014404297 \n",
      "Epoch: 1570 | MAE Train Loss: 28.418292999267578 | MAE Test Loss: 20.904483795166016 \n",
      "Epoch: 1580 | MAE Train Loss: 28.303081512451172 | MAE Test Loss: 20.769771575927734 \n",
      "Epoch: 1590 | MAE Train Loss: 28.1878719329834 | MAE Test Loss: 20.635059356689453 \n",
      "Epoch: 1600 | MAE Train Loss: 28.072656631469727 | MAE Test Loss: 20.500347137451172 \n",
      "Epoch: 1610 | MAE Train Loss: 27.957443237304688 | MAE Test Loss: 20.365633010864258 \n",
      "Epoch: 1620 | MAE Train Loss: 27.842233657836914 | MAE Test Loss: 20.230920791625977 \n",
      "Epoch: 1630 | MAE Train Loss: 27.727020263671875 | MAE Test Loss: 20.096206665039062 \n",
      "Epoch: 1640 | MAE Train Loss: 27.611806869506836 | MAE Test Loss: 19.96149444580078 \n",
      "Epoch: 1650 | MAE Train Loss: 27.496593475341797 | MAE Test Loss: 19.8267822265625 \n",
      "Epoch: 1660 | MAE Train Loss: 27.381383895874023 | MAE Test Loss: 19.69207000732422 \n",
      "Epoch: 1670 | MAE Train Loss: 27.266170501708984 | MAE Test Loss: 19.557357788085938 \n",
      "Epoch: 1680 | MAE Train Loss: 27.150955200195312 | MAE Test Loss: 19.422643661499023 \n",
      "Epoch: 1690 | MAE Train Loss: 27.035741806030273 | MAE Test Loss: 19.287931442260742 \n",
      "Epoch: 1700 | MAE Train Loss: 26.9205322265625 | MAE Test Loss: 19.15321922302246 \n",
      "Epoch: 1710 | MAE Train Loss: 26.805322647094727 | MAE Test Loss: 19.018505096435547 \n",
      "Epoch: 1720 | MAE Train Loss: 26.690109252929688 | MAE Test Loss: 18.883792877197266 \n",
      "Epoch: 1730 | MAE Train Loss: 26.57489585876465 | MAE Test Loss: 18.749080657958984 \n",
      "Epoch: 1740 | MAE Train Loss: 26.45968246459961 | MAE Test Loss: 18.614368438720703 \n",
      "Epoch: 1750 | MAE Train Loss: 26.344470977783203 | MAE Test Loss: 18.47965431213379 \n",
      "Epoch: 1760 | MAE Train Loss: 26.229257583618164 | MAE Test Loss: 18.34494400024414 \n",
      "Epoch: 1770 | MAE Train Loss: 26.114044189453125 | MAE Test Loss: 18.21023178100586 \n",
      "Epoch: 1780 | MAE Train Loss: 25.99883460998535 | MAE Test Loss: 18.075517654418945 \n",
      "Epoch: 1790 | MAE Train Loss: 25.883621215820312 | MAE Test Loss: 17.94080352783203 \n",
      "Epoch: 1800 | MAE Train Loss: 25.768407821655273 | MAE Test Loss: 17.806093215942383 \n",
      "Epoch: 1810 | MAE Train Loss: 25.653194427490234 | MAE Test Loss: 17.67137908935547 \n",
      "Epoch: 1820 | MAE Train Loss: 25.537982940673828 | MAE Test Loss: 17.536666870117188 \n",
      "Epoch: 1830 | MAE Train Loss: 25.422771453857422 | MAE Test Loss: 17.401954650878906 \n",
      "Epoch: 1840 | MAE Train Loss: 25.307559967041016 | MAE Test Loss: 17.26723861694336 \n",
      "Epoch: 1850 | MAE Train Loss: 25.192346572875977 | MAE Test Loss: 17.132526397705078 \n",
      "Epoch: 1860 | MAE Train Loss: 25.077133178710938 | MAE Test Loss: 16.99781608581543 \n",
      "Epoch: 1870 | MAE Train Loss: 24.9619197845459 | MAE Test Loss: 16.863101959228516 \n",
      "Epoch: 1880 | MAE Train Loss: 24.84670639038086 | MAE Test Loss: 16.728389739990234 \n",
      "Epoch: 1890 | MAE Train Loss: 24.731496810913086 | MAE Test Loss: 16.593677520751953 \n",
      "Epoch: 1900 | MAE Train Loss: 24.616283416748047 | MAE Test Loss: 16.45896339416504 \n",
      "Epoch: 1910 | MAE Train Loss: 24.50107192993164 | MAE Test Loss: 16.324251174926758 \n",
      "Epoch: 1920 | MAE Train Loss: 24.385860443115234 | MAE Test Loss: 16.18954086303711 \n",
      "Epoch: 1930 | MAE Train Loss: 24.270647048950195 | MAE Test Loss: 16.05482292175293 \n",
      "Epoch: 1940 | MAE Train Loss: 24.15543556213379 | MAE Test Loss: 15.920114517211914 \n",
      "Epoch: 1950 | MAE Train Loss: 24.04022216796875 | MAE Test Loss: 15.785402297973633 \n",
      "Epoch: 1960 | MAE Train Loss: 23.92500877380371 | MAE Test Loss: 15.650688171386719 \n",
      "Epoch: 1970 | MAE Train Loss: 23.809795379638672 | MAE Test Loss: 15.515975952148438 \n",
      "Epoch: 1980 | MAE Train Loss: 23.694583892822266 | MAE Test Loss: 15.381261825561523 \n",
      "Epoch: 1990 | MAE Train Loss: 23.57937240600586 | MAE Test Loss: 15.246548652648926 \n",
      "Epoch: 2000 | MAE Train Loss: 23.464157104492188 | MAE Test Loss: 15.111837387084961 \n",
      "Epoch: 2010 | MAE Train Loss: 23.34894561767578 | MAE Test Loss: 14.97712516784668 \n",
      "Epoch: 2020 | MAE Train Loss: 23.233734130859375 | MAE Test Loss: 14.84241008758545 \n",
      "Epoch: 2030 | MAE Train Loss: 23.118520736694336 | MAE Test Loss: 14.707697868347168 \n",
      "Epoch: 2040 | MAE Train Loss: 23.003307342529297 | MAE Test Loss: 14.572985649108887 \n",
      "Epoch: 2050 | MAE Train Loss: 22.88809585571289 | MAE Test Loss: 14.438273429870605 \n",
      "Epoch: 2060 | MAE Train Loss: 22.772884368896484 | MAE Test Loss: 14.303560256958008 \n",
      "Epoch: 2070 | MAE Train Loss: 22.657669067382812 | MAE Test Loss: 14.168848991394043 \n",
      "Epoch: 2080 | MAE Train Loss: 22.542455673217773 | MAE Test Loss: 14.034133911132812 \n",
      "Epoch: 2090 | MAE Train Loss: 22.42724609375 | MAE Test Loss: 13.899423599243164 \n",
      "Epoch: 2100 | MAE Train Loss: 22.312034606933594 | MAE Test Loss: 13.76470947265625 \n",
      "Epoch: 2110 | MAE Train Loss: 22.196821212768555 | MAE Test Loss: 13.629995346069336 \n",
      "Epoch: 2120 | MAE Train Loss: 22.081607818603516 | MAE Test Loss: 13.495285034179688 \n",
      "Epoch: 2130 | MAE Train Loss: 21.96639633178711 | MAE Test Loss: 13.360569953918457 \n",
      "Epoch: 2140 | MAE Train Loss: 21.851184844970703 | MAE Test Loss: 13.225857734680176 \n",
      "Epoch: 2150 | MAE Train Loss: 21.735971450805664 | MAE Test Loss: 13.091145515441895 \n",
      "Epoch: 2160 | MAE Train Loss: 21.620759963989258 | MAE Test Loss: 12.956433296203613 \n",
      "Epoch: 2170 | MAE Train Loss: 21.50554847717285 | MAE Test Loss: 12.821718215942383 \n",
      "Epoch: 2180 | MAE Train Loss: 21.39033317565918 | MAE Test Loss: 12.687006950378418 \n",
      "Epoch: 2190 | MAE Train Loss: 21.275121688842773 | MAE Test Loss: 12.552294731140137 \n",
      "Epoch: 2200 | MAE Train Loss: 21.159908294677734 | MAE Test Loss: 12.417581558227539 \n",
      "Epoch: 2210 | MAE Train Loss: 21.044694900512695 | MAE Test Loss: 12.282869338989258 \n",
      "Epoch: 2220 | MAE Train Loss: 20.929485321044922 | MAE Test Loss: 12.148157119750977 \n",
      "Epoch: 2230 | MAE Train Loss: 20.814271926879883 | MAE Test Loss: 12.013443946838379 \n",
      "Epoch: 2240 | MAE Train Loss: 20.699060440063477 | MAE Test Loss: 11.878730773925781 \n",
      "Epoch: 2250 | MAE Train Loss: 20.583845138549805 | MAE Test Loss: 11.7440185546875 \n",
      "Epoch: 2260 | MAE Train Loss: 20.468631744384766 | MAE Test Loss: 11.609306335449219 \n",
      "Epoch: 2270 | MAE Train Loss: 20.35342025756836 | MAE Test Loss: 11.474592208862305 \n",
      "Epoch: 2280 | MAE Train Loss: 20.238208770751953 | MAE Test Loss: 11.339879989624023 \n",
      "Epoch: 2290 | MAE Train Loss: 20.122997283935547 | MAE Test Loss: 11.205167770385742 \n",
      "Epoch: 2300 | MAE Train Loss: 20.007783889770508 | MAE Test Loss: 11.070453643798828 \n",
      "Epoch: 2310 | MAE Train Loss: 19.8925724029541 | MAE Test Loss: 10.935742378234863 \n",
      "Epoch: 2320 | MAE Train Loss: 19.777359008789062 | MAE Test Loss: 10.801029205322266 \n",
      "Epoch: 2330 | MAE Train Loss: 19.66214942932129 | MAE Test Loss: 10.666316032409668 \n",
      "Epoch: 2340 | MAE Train Loss: 19.546934127807617 | MAE Test Loss: 10.53160572052002 \n",
      "Epoch: 2350 | MAE Train Loss: 19.431724548339844 | MAE Test Loss: 10.396896362304688 \n",
      "Epoch: 2360 | MAE Train Loss: 19.316511154174805 | MAE Test Loss: 10.262186050415039 \n",
      "Epoch: 2370 | MAE Train Loss: 19.2012996673584 | MAE Test Loss: 10.12747859954834 \n",
      "Epoch: 2380 | MAE Train Loss: 19.086090087890625 | MAE Test Loss: 9.992770195007324 \n",
      "Epoch: 2390 | MAE Train Loss: 18.970882415771484 | MAE Test Loss: 9.858060836791992 \n",
      "Epoch: 2400 | MAE Train Loss: 18.855670928955078 | MAE Test Loss: 9.723352432250977 \n",
      "Epoch: 2410 | MAE Train Loss: 18.740459442138672 | MAE Test Loss: 9.588644027709961 \n",
      "Epoch: 2420 | MAE Train Loss: 18.625247955322266 | MAE Test Loss: 9.453934669494629 \n",
      "Epoch: 2430 | MAE Train Loss: 18.510038375854492 | MAE Test Loss: 9.319228172302246 \n",
      "Epoch: 2440 | MAE Train Loss: 18.39482879638672 | MAE Test Loss: 9.184518814086914 \n",
      "Epoch: 2450 | MAE Train Loss: 18.279617309570312 | MAE Test Loss: 9.049810409545898 \n",
      "Epoch: 2460 | MAE Train Loss: 18.164403915405273 | MAE Test Loss: 8.915102005004883 \n",
      "Epoch: 2470 | MAE Train Loss: 18.0491943359375 | MAE Test Loss: 8.780393600463867 \n",
      "Epoch: 2480 | MAE Train Loss: 17.93398666381836 | MAE Test Loss: 8.645685195922852 \n",
      "Epoch: 2490 | MAE Train Loss: 17.818775177001953 | MAE Test Loss: 8.51097583770752 \n",
      "Epoch: 2500 | MAE Train Loss: 17.703563690185547 | MAE Test Loss: 8.37626838684082 \n",
      "Epoch: 2510 | MAE Train Loss: 17.58835220336914 | MAE Test Loss: 8.241559982299805 \n",
      "Epoch: 2520 | MAE Train Loss: 17.473140716552734 | MAE Test Loss: 8.106851577758789 \n",
      "Epoch: 2530 | MAE Train Loss: 17.35793113708496 | MAE Test Loss: 7.972142219543457 \n",
      "Epoch: 2540 | MAE Train Loss: 17.242721557617188 | MAE Test Loss: 7.8374342918396 \n",
      "Epoch: 2550 | MAE Train Loss: 17.12751007080078 | MAE Test Loss: 7.702725887298584 \n",
      "Epoch: 2560 | MAE Train Loss: 17.01230239868164 | MAE Test Loss: 7.56801700592041 \n",
      "Epoch: 2570 | MAE Train Loss: 16.897090911865234 | MAE Test Loss: 7.433309078216553 \n",
      "Epoch: 2580 | MAE Train Loss: 16.781877517700195 | MAE Test Loss: 7.298600673675537 \n",
      "Epoch: 2590 | MAE Train Loss: 16.666667938232422 | MAE Test Loss: 7.163891792297363 \n",
      "Epoch: 2600 | MAE Train Loss: 16.551456451416016 | MAE Test Loss: 7.029182434082031 \n",
      "Epoch: 2610 | MAE Train Loss: 16.436246871948242 | MAE Test Loss: 6.894474029541016 \n",
      "Epoch: 2620 | MAE Train Loss: 16.321035385131836 | MAE Test Loss: 6.759765625 \n",
      "Epoch: 2630 | MAE Train Loss: 16.205825805664062 | MAE Test Loss: 6.625056266784668 \n",
      "Epoch: 2640 | MAE Train Loss: 16.090614318847656 | MAE Test Loss: 6.490348815917969 \n",
      "Epoch: 2650 | MAE Train Loss: 15.97540283203125 | MAE Test Loss: 6.355640411376953 \n",
      "Epoch: 2660 | MAE Train Loss: 15.860194206237793 | MAE Test Loss: 6.2209320068359375 \n",
      "Epoch: 2670 | MAE Train Loss: 15.744982719421387 | MAE Test Loss: 6.086223602294922 \n",
      "Epoch: 2680 | MAE Train Loss: 15.629773139953613 | MAE Test Loss: 5.951515197753906 \n",
      "Epoch: 2690 | MAE Train Loss: 15.514561653137207 | MAE Test Loss: 5.816805839538574 \n",
      "Epoch: 2700 | MAE Train Loss: 15.39935302734375 | MAE Test Loss: 5.682097911834717 \n",
      "Epoch: 2710 | MAE Train Loss: 15.284143447875977 | MAE Test Loss: 5.547389507293701 \n",
      "Epoch: 2720 | MAE Train Loss: 15.168930053710938 | MAE Test Loss: 5.4126811027526855 \n",
      "Epoch: 2730 | MAE Train Loss: 15.053718566894531 | MAE Test Loss: 5.277972221374512 \n",
      "Epoch: 2740 | MAE Train Loss: 14.938507080078125 | MAE Test Loss: 5.143264293670654 \n",
      "Epoch: 2750 | MAE Train Loss: 14.823298454284668 | MAE Test Loss: 5.0085554122924805 \n",
      "Epoch: 2760 | MAE Train Loss: 14.708086967468262 | MAE Test Loss: 4.873847007751465 \n",
      "Epoch: 2770 | MAE Train Loss: 14.592877388000488 | MAE Test Loss: 4.739138603210449 \n",
      "Epoch: 2780 | MAE Train Loss: 14.477666854858398 | MAE Test Loss: 4.604430198669434 \n",
      "Epoch: 2790 | MAE Train Loss: 14.362457275390625 | MAE Test Loss: 4.469721794128418 \n",
      "Epoch: 2800 | MAE Train Loss: 14.247245788574219 | MAE Test Loss: 4.335012912750244 \n",
      "Epoch: 2810 | MAE Train Loss: 14.132034301757812 | MAE Test Loss: 4.2003045082092285 \n",
      "Epoch: 2820 | MAE Train Loss: 14.016822814941406 | MAE Test Loss: 4.065595626831055 \n",
      "Epoch: 2830 | MAE Train Loss: 13.901611328125 | MAE Test Loss: 3.9308879375457764 \n",
      "Epoch: 2840 | MAE Train Loss: 13.786401748657227 | MAE Test Loss: 3.7961788177490234 \n",
      "Epoch: 2850 | MAE Train Loss: 13.671191215515137 | MAE Test Loss: 3.661470890045166 \n",
      "Epoch: 2860 | MAE Train Loss: 13.555981636047363 | MAE Test Loss: 3.526762008666992 \n",
      "Epoch: 2870 | MAE Train Loss: 13.440771102905273 | MAE Test Loss: 3.3920531272888184 \n",
      "Epoch: 2880 | MAE Train Loss: 13.3255615234375 | MAE Test Loss: 3.2573447227478027 \n",
      "Epoch: 2890 | MAE Train Loss: 13.210350036621094 | MAE Test Loss: 3.122636318206787 \n",
      "Epoch: 2900 | MAE Train Loss: 13.09514045715332 | MAE Test Loss: 2.9879276752471924 \n",
      "Epoch: 2910 | MAE Train Loss: 12.979928970336914 | MAE Test Loss: 2.853219509124756 \n",
      "Epoch: 2920 | MAE Train Loss: 12.864715576171875 | MAE Test Loss: 2.718510866165161 \n",
      "Epoch: 2930 | MAE Train Loss: 12.749506950378418 | MAE Test Loss: 2.5838024616241455 \n",
      "Epoch: 2940 | MAE Train Loss: 12.634295463562012 | MAE Test Loss: 2.449094295501709 \n",
      "Epoch: 2950 | MAE Train Loss: 12.519085884094238 | MAE Test Loss: 2.3143856525421143 \n",
      "Epoch: 2960 | MAE Train Loss: 12.403874397277832 | MAE Test Loss: 2.1796767711639404 \n",
      "Epoch: 2970 | MAE Train Loss: 12.288664817810059 | MAE Test Loss: 2.044969081878662 \n",
      "Epoch: 2980 | MAE Train Loss: 12.173452377319336 | MAE Test Loss: 1.9102599620819092 \n",
      "Epoch: 2990 | MAE Train Loss: 12.058243751525879 | MAE Test Loss: 1.7902027368545532 \n",
      "Epoch: 3000 | MAE Train Loss: 11.943033218383789 | MAE Test Loss: 1.6831378936767578 \n",
      "Epoch: 3010 | MAE Train Loss: 11.827821731567383 | MAE Test Loss: 1.576073408126831 \n",
      "Epoch: 3020 | MAE Train Loss: 11.712610244750977 | MAE Test Loss: 1.4839534759521484 \n",
      "Epoch: 3030 | MAE Train Loss: 11.597400665283203 | MAE Test Loss: 1.4043766260147095 \n",
      "Epoch: 3040 | MAE Train Loss: 11.482190132141113 | MAE Test Loss: 1.324798583984375 \n",
      "Epoch: 3050 | MAE Train Loss: 11.366978645324707 | MAE Test Loss: 1.2595252990722656 \n",
      "Epoch: 3060 | MAE Train Loss: 11.251769065856934 | MAE Test Loss: 1.207279920578003 \n",
      "Epoch: 3070 | MAE Train Loss: 11.136558532714844 | MAE Test Loss: 1.1550346612930298 \n",
      "Epoch: 3080 | MAE Train Loss: 11.021347999572754 | MAE Test Loss: 1.115513563156128 \n",
      "Epoch: 3090 | MAE Train Loss: 10.906137466430664 | MAE Test Loss: 1.0904433727264404 \n",
      "Epoch: 3100 | MAE Train Loss: 10.790925979614258 | MAE Test Loss: 1.0653736591339111 \n",
      "Epoch: 3110 | MAE Train Loss: 10.675715446472168 | MAE Test Loss: 1.050514578819275 \n",
      "Epoch: 3120 | MAE Train Loss: 10.560505867004395 | MAE Test Loss: 1.0524654388427734 \n",
      "Epoch: 3130 | MAE Train Loss: 10.445294380187988 | MAE Test Loss: 1.0544143915176392 \n",
      "Epoch: 3140 | MAE Train Loss: 10.330083847045898 | MAE Test Loss: 1.063124418258667 \n",
      "Epoch: 3150 | MAE Train Loss: 10.214874267578125 | MAE Test Loss: 1.0919376611709595 \n",
      "Epoch: 3160 | MAE Train Loss: 10.099661827087402 | MAE Test Loss: 1.1207516193389893 \n",
      "Epoch: 3170 | MAE Train Loss: 9.984452247619629 | MAE Test Loss: 1.151939034461975 \n",
      "Epoch: 3180 | MAE Train Loss: 9.869241714477539 | MAE Test Loss: 1.2074600458145142 \n",
      "Epoch: 3190 | MAE Train Loss: 9.754030227661133 | MAE Test Loss: 1.2629821300506592 \n",
      "Epoch: 3200 | MAE Train Loss: 9.638826370239258 | MAE Test Loss: 1.3185001611709595 \n",
      "Epoch: 3210 | MAE Train Loss: 9.52363395690918 | MAE Test Loss: 1.397613525390625 \n",
      "Epoch: 3220 | MAE Train Loss: 9.408443450927734 | MAE Test Loss: 1.4796749353408813 \n",
      "Epoch: 3230 | MAE Train Loss: 9.29325008392334 | MAE Test Loss: 1.5617363452911377 \n",
      "Epoch: 3240 | MAE Train Loss: 9.178059577941895 | MAE Test Loss: 1.660972237586975 \n",
      "Epoch: 3250 | MAE Train Loss: 9.062868118286133 | MAE Test Loss: 1.7694259881973267 \n",
      "Epoch: 3260 | MAE Train Loss: 8.947675704956055 | MAE Test Loss: 1.8778789043426514 \n",
      "Epoch: 3270 | MAE Train Loss: 8.832484245300293 | MAE Test Loss: 1.9961456060409546 \n",
      "Epoch: 3280 | MAE Train Loss: 8.717292785644531 | MAE Test Loss: 2.1308350563049316 \n",
      "Epoch: 3290 | MAE Train Loss: 8.602102279663086 | MAE Test Loss: 2.2655246257781982 \n",
      "Epoch: 3300 | MAE Train Loss: 8.486910820007324 | MAE Test Loss: 2.4002139568328857 \n",
      "Epoch: 3310 | MAE Train Loss: 8.383467674255371 | MAE Test Loss: 2.5264430046081543 \n",
      "Epoch: 3320 | MAE Train Loss: 8.280908584594727 | MAE Test Loss: 2.6526718139648438 \n",
      "Epoch: 3330 | MAE Train Loss: 8.178350448608398 | MAE Test Loss: 2.7789008617401123 \n",
      "Epoch: 3340 | MAE Train Loss: 8.079851150512695 | MAE Test Loss: 2.9017772674560547 \n",
      "Epoch: 3350 | MAE Train Loss: 7.989064693450928 | MAE Test Loss: 3.019622325897217 \n",
      "Epoch: 3360 | MAE Train Loss: 7.89827823638916 | MAE Test Loss: 3.137467861175537 \n",
      "Epoch: 3370 | MAE Train Loss: 7.807491302490234 | MAE Test Loss: 3.255312442779541 \n",
      "Epoch: 3380 | MAE Train Loss: 7.722316741943359 | MAE Test Loss: 3.3681838512420654 \n",
      "Epoch: 3390 | MAE Train Loss: 7.642457008361816 | MAE Test Loss: 3.477738618850708 \n",
      "Epoch: 3400 | MAE Train Loss: 7.562596321105957 | MAE Test Loss: 3.587294101715088 \n",
      "Epoch: 3410 | MAE Train Loss: 7.482735633850098 | MAE Test Loss: 3.6968491077423096 \n",
      "Epoch: 3420 | MAE Train Loss: 7.406081199645996 | MAE Test Loss: 3.8031249046325684 \n",
      "Epoch: 3430 | MAE Train Loss: 7.336324214935303 | MAE Test Loss: 3.90448260307312 \n",
      "Epoch: 3440 | MAE Train Loss: 7.266568183898926 | MAE Test Loss: 4.005839824676514 \n",
      "Epoch: 3450 | MAE Train Loss: 7.196810722351074 | MAE Test Loss: 4.107199192047119 \n",
      "Epoch: 3460 | MAE Train Loss: 7.1270551681518555 | MAE Test Loss: 4.208555698394775 \n",
      "Epoch: 3470 | MAE Train Loss: 7.0641374588012695 | MAE Test Loss: 4.303423881530762 \n",
      "Epoch: 3480 | MAE Train Loss: 7.003686428070068 | MAE Test Loss: 4.396668910980225 \n",
      "Epoch: 3490 | MAE Train Loss: 6.943234443664551 | MAE Test Loss: 4.489913463592529 \n",
      "Epoch: 3500 | MAE Train Loss: 6.88278341293335 | MAE Test Loss: 4.583159923553467 \n",
      "Epoch: 3510 | MAE Train Loss: 6.822332859039307 | MAE Test Loss: 4.676405906677246 \n",
      "Epoch: 3520 | MAE Train Loss: 6.768187046051025 | MAE Test Loss: 4.7632293701171875 \n",
      "Epoch: 3530 | MAE Train Loss: 6.7162628173828125 | MAE Test Loss: 4.848446846008301 \n",
      "Epoch: 3540 | MAE Train Loss: 6.6643385887146 | MAE Test Loss: 4.933665752410889 \n",
      "Epoch: 3550 | MAE Train Loss: 6.6124162673950195 | MAE Test Loss: 5.01888370513916 \n",
      "Epoch: 3560 | MAE Train Loss: 6.560492038726807 | MAE Test Loss: 5.10410213470459 \n",
      "Epoch: 3570 | MAE Train Loss: 6.510485649108887 | MAE Test Loss: 5.1869401931762695 \n",
      "Epoch: 3580 | MAE Train Loss: 6.466330051422119 | MAE Test Loss: 5.264224052429199 \n",
      "Epoch: 3590 | MAE Train Loss: 6.422173976898193 | MAE Test Loss: 5.341507911682129 \n",
      "Epoch: 3600 | MAE Train Loss: 6.378018379211426 | MAE Test Loss: 5.418792247772217 \n",
      "Epoch: 3610 | MAE Train Loss: 6.3338623046875 | MAE Test Loss: 5.496077060699463 \n",
      "Epoch: 3620 | MAE Train Loss: 6.289706230163574 | MAE Test Loss: 5.573360919952393 \n",
      "Epoch: 3630 | MAE Train Loss: 6.24705171585083 | MAE Test Loss: 5.648289680480957 \n",
      "Epoch: 3640 | MAE Train Loss: 6.209925174713135 | MAE Test Loss: 5.717724800109863 \n",
      "Epoch: 3650 | MAE Train Loss: 6.172799587249756 | MAE Test Loss: 5.7871599197387695 \n",
      "Epoch: 3660 | MAE Train Loss: 6.135672569274902 | MAE Test Loss: 5.856594562530518 \n",
      "Epoch: 3670 | MAE Train Loss: 6.098546028137207 | MAE Test Loss: 5.926030158996582 \n",
      "Epoch: 3680 | MAE Train Loss: 6.061420440673828 | MAE Test Loss: 5.99546480178833 \n",
      "Epoch: 3690 | MAE Train Loss: 6.024293899536133 | MAE Test Loss: 6.0649003982543945 \n",
      "Epoch: 3700 | MAE Train Loss: 5.989920139312744 | MAE Test Loss: 6.130456924438477 \n",
      "Epoch: 3710 | MAE Train Loss: 5.959103584289551 | MAE Test Loss: 6.192136287689209 \n",
      "Epoch: 3720 | MAE Train Loss: 5.928287029266357 | MAE Test Loss: 6.253815650939941 \n",
      "Epoch: 3730 | MAE Train Loss: 5.897469997406006 | MAE Test Loss: 6.315495014190674 \n",
      "Epoch: 3740 | MAE Train Loss: 5.8666534423828125 | MAE Test Loss: 6.377173900604248 \n",
      "Epoch: 3750 | MAE Train Loss: 5.835837364196777 | MAE Test Loss: 6.4388532638549805 \n",
      "Epoch: 3760 | MAE Train Loss: 5.805020332336426 | MAE Test Loss: 6.500532627105713 \n",
      "Epoch: 3770 | MAE Train Loss: 5.774204254150391 | MAE Test Loss: 6.562211513519287 \n",
      "Epoch: 3780 | MAE Train Loss: 5.748140335083008 | MAE Test Loss: 6.616995334625244 \n",
      "Epoch: 3790 | MAE Train Loss: 5.722932815551758 | MAE Test Loss: 6.671011924743652 \n",
      "Epoch: 3800 | MAE Train Loss: 5.69772481918335 | MAE Test Loss: 6.725028991699219 \n",
      "Epoch: 3810 | MAE Train Loss: 5.672516822814941 | MAE Test Loss: 6.779045104980469 \n",
      "Epoch: 3820 | MAE Train Loss: 5.647309303283691 | MAE Test Loss: 6.833062648773193 \n",
      "Epoch: 3830 | MAE Train Loss: 5.622101306915283 | MAE Test Loss: 6.887078762054443 \n",
      "Epoch: 3840 | MAE Train Loss: 5.596893310546875 | MAE Test Loss: 6.941096305847168 \n",
      "Epoch: 3850 | MAE Train Loss: 5.571684837341309 | MAE Test Loss: 6.995113372802734 \n",
      "Epoch: 3860 | MAE Train Loss: 5.547724723815918 | MAE Test Loss: 7.046856880187988 \n",
      "Epoch: 3870 | MAE Train Loss: 5.527442932128906 | MAE Test Loss: 7.093295097351074 \n",
      "Epoch: 3880 | MAE Train Loss: 5.5071611404418945 | MAE Test Loss: 7.139736175537109 \n",
      "Epoch: 3890 | MAE Train Loss: 5.486879825592041 | MAE Test Loss: 7.186175346374512 \n",
      "Epoch: 3900 | MAE Train Loss: 5.466597557067871 | MAE Test Loss: 7.232614040374756 \n",
      "Epoch: 3910 | MAE Train Loss: 5.446316242218018 | MAE Test Loss: 7.279054164886475 \n",
      "Epoch: 3920 | MAE Train Loss: 5.426034450531006 | MAE Test Loss: 7.325491905212402 \n",
      "Epoch: 3930 | MAE Train Loss: 5.405752658843994 | MAE Test Loss: 7.3719329833984375 \n",
      "Epoch: 3940 | MAE Train Loss: 5.385470867156982 | MAE Test Loss: 7.418371677398682 \n",
      "Epoch: 3950 | MAE Train Loss: 5.365189552307129 | MAE Test Loss: 7.4648118019104 \n",
      "Epoch: 3960 | MAE Train Loss: 5.3466997146606445 | MAE Test Loss: 7.507504463195801 \n",
      "Epoch: 3970 | MAE Train Loss: 5.330679416656494 | MAE Test Loss: 7.546452522277832 \n",
      "Epoch: 3980 | MAE Train Loss: 5.314658164978027 | MAE Test Loss: 7.585397243499756 \n",
      "Epoch: 3990 | MAE Train Loss: 5.298636436462402 | MAE Test Loss: 7.624345302581787 \n",
      "Epoch: 4000 | MAE Train Loss: 5.28261661529541 | MAE Test Loss: 7.663291931152344 \n",
      "Epoch: 4010 | MAE Train Loss: 5.266595840454102 | MAE Test Loss: 7.702237606048584 \n",
      "Epoch: 4020 | MAE Train Loss: 5.250574588775635 | MAE Test Loss: 7.741185188293457 \n",
      "Epoch: 4030 | MAE Train Loss: 5.234553813934326 | MAE Test Loss: 7.780130863189697 \n",
      "Epoch: 4040 | MAE Train Loss: 5.218532562255859 | MAE Test Loss: 7.819077968597412 \n",
      "Epoch: 4050 | MAE Train Loss: 5.202512264251709 | MAE Test Loss: 7.858025550842285 \n",
      "Epoch: 4060 | MAE Train Loss: 5.186490535736084 | MAE Test Loss: 7.896971225738525 \n",
      "Epoch: 4070 | MAE Train Loss: 5.170470237731934 | MAE Test Loss: 7.93591833114624 \n",
      "Epoch: 4080 | MAE Train Loss: 5.15773868560791 | MAE Test Loss: 7.967465400695801 \n",
      "Epoch: 4090 | MAE Train Loss: 5.145330905914307 | MAE Test Loss: 7.999014377593994 \n",
      "Epoch: 4100 | MAE Train Loss: 5.1329216957092285 | MAE Test Loss: 8.030561447143555 \n",
      "Epoch: 4110 | MAE Train Loss: 5.120512962341309 | MAE Test Loss: 8.062108993530273 \n",
      "Epoch: 4120 | MAE Train Loss: 5.1081037521362305 | MAE Test Loss: 8.093655586242676 \n",
      "Epoch: 4130 | MAE Train Loss: 5.095695495605469 | MAE Test Loss: 8.125203132629395 \n",
      "Epoch: 4140 | MAE Train Loss: 5.083286762237549 | MAE Test Loss: 8.156750679016113 \n",
      "Epoch: 4150 | MAE Train Loss: 5.070878028869629 | MAE Test Loss: 8.188298225402832 \n",
      "Epoch: 4160 | MAE Train Loss: 5.058468818664551 | MAE Test Loss: 8.21984577178955 \n",
      "Epoch: 4170 | MAE Train Loss: 5.046060085296631 | MAE Test Loss: 8.251392364501953 \n",
      "Epoch: 4180 | MAE Train Loss: 5.033651828765869 | MAE Test Loss: 8.282940864562988 \n",
      "Epoch: 4190 | MAE Train Loss: 5.021242618560791 | MAE Test Loss: 8.314488410949707 \n",
      "Epoch: 4200 | MAE Train Loss: 5.008833885192871 | MAE Test Loss: 8.346035957336426 \n",
      "Epoch: 4210 | MAE Train Loss: 4.996855735778809 | MAE Test Loss: 8.37611198425293 \n",
      "Epoch: 4220 | MAE Train Loss: 4.9874372482299805 | MAE Test Loss: 8.400306701660156 \n",
      "Epoch: 4230 | MAE Train Loss: 4.978019714355469 | MAE Test Loss: 8.4245023727417 \n",
      "Epoch: 4240 | MAE Train Loss: 4.968602180480957 | MAE Test Loss: 8.448698043823242 \n",
      "Epoch: 4250 | MAE Train Loss: 4.959183692932129 | MAE Test Loss: 8.472890853881836 \n",
      "Epoch: 4260 | MAE Train Loss: 4.949765682220459 | MAE Test Loss: 8.497086524963379 \n",
      "Epoch: 4270 | MAE Train Loss: 4.940348148345947 | MAE Test Loss: 8.521280288696289 \n",
      "Epoch: 4280 | MAE Train Loss: 4.930930137634277 | MAE Test Loss: 8.545476913452148 \n",
      "Epoch: 4290 | MAE Train Loss: 4.921513557434082 | MAE Test Loss: 8.569671630859375 \n",
      "Epoch: 4300 | MAE Train Loss: 4.912095069885254 | MAE Test Loss: 8.593866348266602 \n",
      "Epoch: 4310 | MAE Train Loss: 4.902677059173584 | MAE Test Loss: 8.618061065673828 \n",
      "Epoch: 4320 | MAE Train Loss: 4.893259525299072 | MAE Test Loss: 8.642255783081055 \n",
      "Epoch: 4330 | MAE Train Loss: 4.883841514587402 | MAE Test Loss: 8.666450500488281 \n",
      "Epoch: 4340 | MAE Train Loss: 4.874423503875732 | MAE Test Loss: 8.690645217895508 \n",
      "Epoch: 4350 | MAE Train Loss: 4.865006446838379 | MAE Test Loss: 8.71484088897705 \n",
      "Epoch: 4360 | MAE Train Loss: 4.855587959289551 | MAE Test Loss: 8.739034652709961 \n",
      "Epoch: 4370 | MAE Train Loss: 4.846170425415039 | MAE Test Loss: 8.763230323791504 \n",
      "Epoch: 4380 | MAE Train Loss: 4.837324619293213 | MAE Test Loss: 8.785259246826172 \n",
      "Epoch: 4390 | MAE Train Loss: 4.830267906188965 | MAE Test Loss: 8.80223274230957 \n",
      "Epoch: 4400 | MAE Train Loss: 4.823211193084717 | MAE Test Loss: 8.819208145141602 \n",
      "Epoch: 4410 | MAE Train Loss: 4.816154479980469 | MAE Test Loss: 8.836179733276367 \n",
      "Epoch: 4420 | MAE Train Loss: 4.809098243713379 | MAE Test Loss: 8.853154182434082 \n",
      "Epoch: 4430 | MAE Train Loss: 4.8020405769348145 | MAE Test Loss: 8.87012767791748 \n",
      "Epoch: 4440 | MAE Train Loss: 4.794983863830566 | MAE Test Loss: 8.887102127075195 \n",
      "Epoch: 4450 | MAE Train Loss: 4.787927150726318 | MAE Test Loss: 8.904077529907227 \n",
      "Epoch: 4460 | MAE Train Loss: 4.7808709144592285 | MAE Test Loss: 8.921051025390625 \n",
      "Epoch: 4470 | MAE Train Loss: 4.773813247680664 | MAE Test Loss: 8.938023567199707 \n",
      "Epoch: 4480 | MAE Train Loss: 4.766756534576416 | MAE Test Loss: 8.954998016357422 \n",
      "Epoch: 4490 | MAE Train Loss: 4.759699821472168 | MAE Test Loss: 8.97197151184082 \n",
      "Epoch: 4500 | MAE Train Loss: 4.752642631530762 | MAE Test Loss: 8.988946914672852 \n",
      "Epoch: 4510 | MAE Train Loss: 4.745586395263672 | MAE Test Loss: 9.005918502807617 \n",
      "Epoch: 4520 | MAE Train Loss: 4.738530158996582 | MAE Test Loss: 9.022892951965332 \n",
      "Epoch: 4530 | MAE Train Loss: 4.731472969055176 | MAE Test Loss: 9.03986644744873 \n",
      "Epoch: 4540 | MAE Train Loss: 4.7244157791137695 | MAE Test Loss: 9.056840896606445 \n",
      "Epoch: 4550 | MAE Train Loss: 4.7173590660095215 | MAE Test Loss: 9.073816299438477 \n",
      "Epoch: 4560 | MAE Train Loss: 4.710302829742432 | MAE Test Loss: 9.090789794921875 \n",
      "Epoch: 4570 | MAE Train Loss: 4.703245639801025 | MAE Test Loss: 9.107762336730957 \n",
      "Epoch: 4580 | MAE Train Loss: 4.696188449859619 | MAE Test Loss: 9.124736785888672 \n",
      "Epoch: 4590 | MAE Train Loss: 4.689131736755371 | MAE Test Loss: 9.14171028137207 \n",
      "Epoch: 4600 | MAE Train Loss: 4.682969093322754 | MAE Test Loss: 9.15440845489502 \n",
      "Epoch: 4610 | MAE Train Loss: 4.677672386169434 | MAE Test Loss: 9.164253234863281 \n",
      "Epoch: 4620 | MAE Train Loss: 4.672374725341797 | MAE Test Loss: 9.174099922180176 \n",
      "Epoch: 4630 | MAE Train Loss: 4.667078018188477 | MAE Test Loss: 9.18394660949707 \n",
      "Epoch: 4640 | MAE Train Loss: 4.661781311035156 | MAE Test Loss: 9.193792343139648 \n",
      "Epoch: 4650 | MAE Train Loss: 4.656484127044678 | MAE Test Loss: 9.203639030456543 \n",
      "Epoch: 4660 | MAE Train Loss: 4.651186943054199 | MAE Test Loss: 9.213485717773438 \n",
      "Epoch: 4670 | MAE Train Loss: 4.645890235900879 | MAE Test Loss: 9.223332405090332 \n",
      "Epoch: 4680 | MAE Train Loss: 4.6405930519104 | MAE Test Loss: 9.233177185058594 \n",
      "Epoch: 4690 | MAE Train Loss: 4.635295867919922 | MAE Test Loss: 9.243023872375488 \n",
      "Epoch: 4700 | MAE Train Loss: 4.629999160766602 | MAE Test Loss: 9.252870559692383 \n",
      "Epoch: 4710 | MAE Train Loss: 4.624701976776123 | MAE Test Loss: 9.262716293334961 \n",
      "Epoch: 4720 | MAE Train Loss: 4.619405269622803 | MAE Test Loss: 9.272562026977539 \n",
      "Epoch: 4730 | MAE Train Loss: 4.614108085632324 | MAE Test Loss: 9.282408714294434 \n",
      "Epoch: 4740 | MAE Train Loss: 4.608810901641846 | MAE Test Loss: 9.292255401611328 \n",
      "Epoch: 4750 | MAE Train Loss: 4.603514194488525 | MAE Test Loss: 9.302102088928223 \n",
      "Epoch: 4760 | MAE Train Loss: 4.598217010498047 | MAE Test Loss: 9.3119478225708 \n",
      "Epoch: 4770 | MAE Train Loss: 4.592919826507568 | MAE Test Loss: 9.321792602539062 \n",
      "Epoch: 4780 | MAE Train Loss: 4.587623596191406 | MAE Test Loss: 9.331639289855957 \n",
      "Epoch: 4790 | MAE Train Loss: 4.5823259353637695 | MAE Test Loss: 9.341485977172852 \n",
      "Epoch: 4800 | MAE Train Loss: 4.577029228210449 | MAE Test Loss: 9.35133171081543 \n",
      "Epoch: 4810 | MAE Train Loss: 4.571732521057129 | MAE Test Loss: 9.361178398132324 \n",
      "Epoch: 4820 | MAE Train Loss: 4.56643533706665 | MAE Test Loss: 9.371025085449219 \n",
      "Epoch: 4830 | MAE Train Loss: 4.561138153076172 | MAE Test Loss: 9.380871772766113 \n",
      "Epoch: 4840 | MAE Train Loss: 4.555841445922852 | MAE Test Loss: 9.390716552734375 \n",
      "Epoch: 4850 | MAE Train Loss: 4.550543785095215 | MAE Test Loss: 9.40056324005127 \n",
      "Epoch: 4860 | MAE Train Loss: 4.545247554779053 | MAE Test Loss: 9.410409927368164 \n",
      "Epoch: 4870 | MAE Train Loss: 4.539950370788574 | MAE Test Loss: 9.420255661010742 \n",
      "Epoch: 4880 | MAE Train Loss: 4.534653663635254 | MAE Test Loss: 9.43010139465332 \n",
      "Epoch: 4890 | MAE Train Loss: 4.529356479644775 | MAE Test Loss: 9.439948081970215 \n",
      "Epoch: 4900 | MAE Train Loss: 4.5243353843688965 | MAE Test Loss: 9.447678565979004 \n",
      "Epoch: 4910 | MAE Train Loss: 4.520209789276123 | MAE Test Loss: 9.450474739074707 \n",
      "Epoch: 4920 | MAE Train Loss: 4.516083717346191 | MAE Test Loss: 9.453268051147461 \n",
      "Epoch: 4930 | MAE Train Loss: 4.511957168579102 | MAE Test Loss: 9.456064224243164 \n",
      "Epoch: 4940 | MAE Train Loss: 4.5078301429748535 | MAE Test Loss: 9.458858489990234 \n",
      "Epoch: 4950 | MAE Train Loss: 4.503704071044922 | MAE Test Loss: 9.461652755737305 \n",
      "Epoch: 4960 | MAE Train Loss: 4.49957799911499 | MAE Test Loss: 9.464448928833008 \n",
      "Epoch: 4970 | MAE Train Loss: 4.495451927185059 | MAE Test Loss: 9.467243194580078 \n",
      "Epoch: 4980 | MAE Train Loss: 4.491325378417969 | MAE Test Loss: 9.470038414001465 \n",
      "Epoch: 4990 | MAE Train Loss: 4.487199306488037 | MAE Test Loss: 9.472834587097168 \n",
      "Epoch: 5000 | MAE Train Loss: 4.4830732345581055 | MAE Test Loss: 9.475628852844238 \n",
      "Epoch: 5010 | MAE Train Loss: 4.478947639465332 | MAE Test Loss: 9.478423118591309 \n",
      "Epoch: 5020 | MAE Train Loss: 4.4748215675354 | MAE Test Loss: 9.481218338012695 \n",
      "Epoch: 5030 | MAE Train Loss: 4.470694065093994 | MAE Test Loss: 9.484013557434082 \n",
      "Epoch: 5040 | MAE Train Loss: 4.4665679931640625 | MAE Test Loss: 9.486806869506836 \n",
      "Epoch: 5050 | MAE Train Loss: 4.462441921234131 | MAE Test Loss: 9.489602088928223 \n",
      "Epoch: 5060 | MAE Train Loss: 4.458315372467041 | MAE Test Loss: 9.492398262023926 \n",
      "Epoch: 5070 | MAE Train Loss: 4.454189777374268 | MAE Test Loss: 9.495192527770996 \n",
      "Epoch: 5080 | MAE Train Loss: 4.4500627517700195 | MAE Test Loss: 9.497987747192383 \n",
      "Epoch: 5090 | MAE Train Loss: 4.445936679840088 | MAE Test Loss: 9.50078296661377 \n",
      "Epoch: 5100 | MAE Train Loss: 4.4418110847473145 | MAE Test Loss: 9.503578186035156 \n",
      "Epoch: 5110 | MAE Train Loss: 4.437684535980225 | MAE Test Loss: 9.506372451782227 \n",
      "Epoch: 5120 | MAE Train Loss: 4.433557987213135 | MAE Test Loss: 9.509167671203613 \n",
      "Epoch: 5130 | MAE Train Loss: 4.429431915283203 | MAE Test Loss: 9.511961936950684 \n",
      "Epoch: 5140 | MAE Train Loss: 4.425304889678955 | MAE Test Loss: 9.51475715637207 \n",
      "Epoch: 5150 | MAE Train Loss: 4.421179294586182 | MAE Test Loss: 9.517552375793457 \n",
      "Epoch: 5160 | MAE Train Loss: 4.41705322265625 | MAE Test Loss: 9.52034854888916 \n",
      "Epoch: 5170 | MAE Train Loss: 4.41292667388916 | MAE Test Loss: 9.523141860961914 \n",
      "Epoch: 5180 | MAE Train Loss: 4.4088006019592285 | MAE Test Loss: 9.5259370803833 \n",
      "Epoch: 5190 | MAE Train Loss: 4.404674530029297 | MAE Test Loss: 9.528732299804688 \n",
      "Epoch: 5200 | MAE Train Loss: 4.400547981262207 | MAE Test Loss: 9.531526565551758 \n",
      "Epoch: 5210 | MAE Train Loss: 4.396422863006592 | MAE Test Loss: 9.534322738647461 \n",
      "Epoch: 5220 | MAE Train Loss: 4.392295837402344 | MAE Test Loss: 9.537117004394531 \n",
      "Epoch: 5230 | MAE Train Loss: 4.388169765472412 | MAE Test Loss: 9.539911270141602 \n",
      "Epoch: 5240 | MAE Train Loss: 4.3840436935424805 | MAE Test Loss: 9.542707443237305 \n",
      "Epoch: 5250 | MAE Train Loss: 4.379916191101074 | MAE Test Loss: 9.545501708984375 \n",
      "Epoch: 5260 | MAE Train Loss: 4.375790596008301 | MAE Test Loss: 9.548296928405762 \n",
      "Epoch: 5270 | MAE Train Loss: 4.371664524078369 | MAE Test Loss: 9.551091194152832 \n",
      "Epoch: 5280 | MAE Train Loss: 4.3675384521484375 | MAE Test Loss: 9.553885459899902 \n",
      "Epoch: 5290 | MAE Train Loss: 4.363411903381348 | MAE Test Loss: 9.556680679321289 \n",
      "Epoch: 5300 | MAE Train Loss: 4.359285831451416 | MAE Test Loss: 9.559475898742676 \n",
      "Epoch: 5310 | MAE Train Loss: 4.355159282684326 | MAE Test Loss: 9.562271118164062 \n",
      "Epoch: 5320 | MAE Train Loss: 4.3510332107543945 | MAE Test Loss: 9.565065383911133 \n",
      "Epoch: 5330 | MAE Train Loss: 4.346907138824463 | MAE Test Loss: 9.56786060333252 \n",
      "Epoch: 5340 | MAE Train Loss: 4.342779636383057 | MAE Test Loss: 9.570655822753906 \n",
      "Epoch: 5350 | MAE Train Loss: 4.338654518127441 | MAE Test Loss: 9.573450088500977 \n",
      "Epoch: 5360 | MAE Train Loss: 4.334527969360352 | MAE Test Loss: 9.57624626159668 \n",
      "Epoch: 5370 | MAE Train Loss: 4.33040189743042 | MAE Test Loss: 9.57904052734375 \n",
      "Epoch: 5380 | MAE Train Loss: 4.326275825500488 | MAE Test Loss: 9.58183479309082 \n",
      "Epoch: 5390 | MAE Train Loss: 4.322300910949707 | MAE Test Loss: 9.582544326782227 \n",
      "Epoch: 5400 | MAE Train Loss: 4.318774223327637 | MAE Test Loss: 9.578390121459961 \n",
      "Epoch: 5410 | MAE Train Loss: 4.31524658203125 | MAE Test Loss: 9.574236869812012 \n",
      "Epoch: 5420 | MAE Train Loss: 4.311718940734863 | MAE Test Loss: 9.57008171081543 \n",
      "Epoch: 5430 | MAE Train Loss: 4.308192253112793 | MAE Test Loss: 9.565927505493164 \n",
      "Epoch: 5440 | MAE Train Loss: 4.3046650886535645 | MAE Test Loss: 9.561773300170898 \n",
      "Epoch: 5450 | MAE Train Loss: 4.301137447357178 | MAE Test Loss: 9.557619094848633 \n",
      "Epoch: 5460 | MAE Train Loss: 4.297609806060791 | MAE Test Loss: 9.55346393585205 \n",
      "Epoch: 5470 | MAE Train Loss: 4.2940826416015625 | MAE Test Loss: 9.549310684204102 \n",
      "Epoch: 5480 | MAE Train Loss: 4.29055643081665 | MAE Test Loss: 9.545156478881836 \n",
      "Epoch: 5490 | MAE Train Loss: 4.2870283126831055 | MAE Test Loss: 9.541000366210938 \n",
      "Epoch: 5500 | MAE Train Loss: 4.283501625061035 | MAE Test Loss: 9.536846160888672 \n",
      "Epoch: 5510 | MAE Train Loss: 4.279974460601807 | MAE Test Loss: 9.532691955566406 \n",
      "Epoch: 5520 | MAE Train Loss: 4.276447296142578 | MAE Test Loss: 9.528536796569824 \n",
      "Epoch: 5530 | MAE Train Loss: 4.27292013168335 | MAE Test Loss: 9.524382591247559 \n",
      "Epoch: 5540 | MAE Train Loss: 4.269392967224121 | MAE Test Loss: 9.520228385925293 \n",
      "Epoch: 5550 | MAE Train Loss: 4.265865802764893 | MAE Test Loss: 9.516073226928711 \n",
      "Epoch: 5560 | MAE Train Loss: 4.262339115142822 | MAE Test Loss: 9.511919021606445 \n",
      "Epoch: 5570 | MAE Train Loss: 4.2588114738464355 | MAE Test Loss: 9.50776481628418 \n",
      "Epoch: 5580 | MAE Train Loss: 4.255283832550049 | MAE Test Loss: 9.503610610961914 \n",
      "Epoch: 5590 | MAE Train Loss: 4.2517571449279785 | MAE Test Loss: 9.499456405639648 \n",
      "Epoch: 5600 | MAE Train Loss: 4.24822998046875 | MAE Test Loss: 9.4953031539917 \n",
      "Epoch: 5610 | MAE Train Loss: 4.2447028160095215 | MAE Test Loss: 9.491146087646484 \n",
      "Epoch: 5620 | MAE Train Loss: 4.241175174713135 | MAE Test Loss: 9.486991882324219 \n",
      "Epoch: 5630 | MAE Train Loss: 4.237648963928223 | MAE Test Loss: 9.48283863067627 \n",
      "Epoch: 5640 | MAE Train Loss: 4.234120845794678 | MAE Test Loss: 9.478682518005371 \n",
      "Epoch: 5650 | MAE Train Loss: 4.230593681335449 | MAE Test Loss: 9.474528312683105 \n",
      "Epoch: 5660 | MAE Train Loss: 4.2270660400390625 | MAE Test Loss: 9.470375061035156 \n",
      "Epoch: 5670 | MAE Train Loss: 4.223539352416992 | MAE Test Loss: 9.466218948364258 \n",
      "Epoch: 5680 | MAE Train Loss: 4.220012187957764 | MAE Test Loss: 9.462066650390625 \n",
      "Epoch: 5690 | MAE Train Loss: 4.216485023498535 | MAE Test Loss: 9.457911491394043 \n",
      "Epoch: 5700 | MAE Train Loss: 4.212957859039307 | MAE Test Loss: 9.453756332397461 \n",
      "Epoch: 5710 | MAE Train Loss: 4.209430694580078 | MAE Test Loss: 9.449602127075195 \n",
      "Epoch: 5720 | MAE Train Loss: 4.20590353012085 | MAE Test Loss: 9.44544792175293 \n",
      "Epoch: 5730 | MAE Train Loss: 4.202376365661621 | MAE Test Loss: 9.441291809082031 \n",
      "Epoch: 5740 | MAE Train Loss: 4.198849201202393 | MAE Test Loss: 9.437138557434082 \n",
      "Epoch: 5750 | MAE Train Loss: 4.195321559906006 | MAE Test Loss: 9.432984352111816 \n",
      "Epoch: 5760 | MAE Train Loss: 4.1917948722839355 | MAE Test Loss: 9.42883014678955 \n",
      "Epoch: 5770 | MAE Train Loss: 4.188267707824707 | MAE Test Loss: 9.424674987792969 \n",
      "Epoch: 5780 | MAE Train Loss: 4.18474006652832 | MAE Test Loss: 9.42051887512207 \n",
      "Epoch: 5790 | MAE Train Loss: 4.18121337890625 | MAE Test Loss: 9.416364669799805 \n",
      "Epoch: 5800 | MAE Train Loss: 4.1776862144470215 | MAE Test Loss: 9.412211418151855 \n",
      "Epoch: 5810 | MAE Train Loss: 4.174159049987793 | MAE Test Loss: 9.40805721282959 \n",
      "Epoch: 5820 | MAE Train Loss: 4.170631408691406 | MAE Test Loss: 9.403902053833008 \n",
      "Epoch: 5830 | MAE Train Loss: 4.167104721069336 | MAE Test Loss: 9.399747848510742 \n",
      "Epoch: 5840 | MAE Train Loss: 4.163577079772949 | MAE Test Loss: 9.395591735839844 \n",
      "Epoch: 5850 | MAE Train Loss: 4.160049915313721 | MAE Test Loss: 9.391439437866211 \n",
      "Epoch: 5860 | MAE Train Loss: 4.156521797180176 | MAE Test Loss: 9.387284278869629 \n",
      "Epoch: 5870 | MAE Train Loss: 4.152995586395264 | MAE Test Loss: 9.383130073547363 \n",
      "Epoch: 5880 | MAE Train Loss: 4.149468898773193 | MAE Test Loss: 9.378976821899414 \n",
      "Epoch: 5890 | MAE Train Loss: 4.145941257476807 | MAE Test Loss: 9.374820709228516 \n",
      "Epoch: 5900 | MAE Train Loss: 4.142414093017578 | MAE Test Loss: 9.37066650390625 \n",
      "Epoch: 5910 | MAE Train Loss: 4.13888692855835 | MAE Test Loss: 9.366511344909668 \n",
      "Epoch: 5920 | MAE Train Loss: 4.135359764099121 | MAE Test Loss: 9.362357139587402 \n",
      "Epoch: 5930 | MAE Train Loss: 4.131832599639893 | MAE Test Loss: 9.35820198059082 \n",
      "Epoch: 5940 | MAE Train Loss: 4.128305435180664 | MAE Test Loss: 9.354047775268555 \n",
      "Epoch: 5950 | MAE Train Loss: 4.124777793884277 | MAE Test Loss: 9.349894523620605 \n",
      "Epoch: 5960 | MAE Train Loss: 4.121251106262207 | MAE Test Loss: 9.345739364624023 \n",
      "Epoch: 5970 | MAE Train Loss: 4.11772346496582 | MAE Test Loss: 9.341585159301758 \n",
      "Epoch: 5980 | MAE Train Loss: 4.11419677734375 | MAE Test Loss: 9.337430000305176 \n",
      "Epoch: 5990 | MAE Train Loss: 4.110669136047363 | MAE Test Loss: 9.333276748657227 \n",
      "Epoch: 6000 | MAE Train Loss: 4.107142448425293 | MAE Test Loss: 9.329122543334961 \n",
      "Epoch: 6010 | MAE Train Loss: 4.103614807128906 | MAE Test Loss: 9.324966430664062 \n",
      "Epoch: 6020 | MAE Train Loss: 4.100087642669678 | MAE Test Loss: 9.320813179016113 \n",
      "Epoch: 6030 | MAE Train Loss: 4.096561431884766 | MAE Test Loss: 9.316657066345215 \n",
      "Epoch: 6040 | MAE Train Loss: 4.093033313751221 | MAE Test Loss: 9.31250286102295 \n",
      "Epoch: 6050 | MAE Train Loss: 4.089505672454834 | MAE Test Loss: 9.308349609375 \n",
      "Epoch: 6060 | MAE Train Loss: 4.0859785079956055 | MAE Test Loss: 9.304194450378418 \n",
      "Epoch: 6070 | MAE Train Loss: 4.082451820373535 | MAE Test Loss: 9.300039291381836 \n",
      "Epoch: 6080 | MAE Train Loss: 4.078924655914307 | MAE Test Loss: 9.29588508605957 \n",
      "Epoch: 6090 | MAE Train Loss: 4.075397491455078 | MAE Test Loss: 9.291730880737305 \n",
      "Epoch: 6100 | MAE Train Loss: 4.071869850158691 | MAE Test Loss: 9.287576675415039 \n",
      "Epoch: 6110 | MAE Train Loss: 4.068343162536621 | MAE Test Loss: 9.283422470092773 \n",
      "Epoch: 6120 | MAE Train Loss: 4.064815521240234 | MAE Test Loss: 9.279267311096191 \n",
      "Epoch: 6130 | MAE Train Loss: 4.061288356781006 | MAE Test Loss: 9.275113105773926 \n",
      "Epoch: 6140 | MAE Train Loss: 4.057761192321777 | MAE Test Loss: 9.270959854125977 \n",
      "Epoch: 6150 | MAE Train Loss: 4.054234027862549 | MAE Test Loss: 9.266804695129395 \n",
      "Epoch: 6160 | MAE Train Loss: 4.0507073402404785 | MAE Test Loss: 9.262649536132812 \n",
      "Epoch: 6170 | MAE Train Loss: 4.047179698944092 | MAE Test Loss: 9.25849437713623 \n",
      "Epoch: 6180 | MAE Train Loss: 4.043652534484863 | MAE Test Loss: 9.254341125488281 \n",
      "Epoch: 6190 | MAE Train Loss: 4.040125846862793 | MAE Test Loss: 9.250185012817383 \n",
      "Epoch: 6200 | MAE Train Loss: 4.0365986824035645 | MAE Test Loss: 9.246030807495117 \n",
      "Epoch: 6210 | MAE Train Loss: 4.033071041107178 | MAE Test Loss: 9.241877555847168 \n",
      "Epoch: 6220 | MAE Train Loss: 4.029543876647949 | MAE Test Loss: 9.237722396850586 \n",
      "Epoch: 6230 | MAE Train Loss: 4.026017189025879 | MAE Test Loss: 9.23356819152832 \n",
      "Epoch: 6240 | MAE Train Loss: 4.022489547729492 | MAE Test Loss: 9.229413032531738 \n",
      "Epoch: 6250 | MAE Train Loss: 4.0189619064331055 | MAE Test Loss: 9.225259780883789 \n",
      "Epoch: 6260 | MAE Train Loss: 4.015434265136719 | MAE Test Loss: 9.221104621887207 \n",
      "Epoch: 6270 | MAE Train Loss: 4.011908054351807 | MAE Test Loss: 9.216951370239258 \n",
      "Epoch: 6280 | MAE Train Loss: 4.008380889892578 | MAE Test Loss: 9.212797164916992 \n",
      "Epoch: 6290 | MAE Train Loss: 4.004853248596191 | MAE Test Loss: 9.208641052246094 \n",
      "Epoch: 6300 | MAE Train Loss: 4.001326560974121 | MAE Test Loss: 9.204486846923828 \n",
      "Epoch: 6310 | MAE Train Loss: 3.9977996349334717 | MAE Test Loss: 9.200332641601562 \n",
      "Epoch: 6320 | MAE Train Loss: 3.994271755218506 | MAE Test Loss: 9.19617748260498 \n",
      "Epoch: 6330 | MAE Train Loss: 3.9907448291778564 | MAE Test Loss: 9.192022323608398 \n",
      "Epoch: 6340 | MAE Train Loss: 3.987217426300049 | MAE Test Loss: 9.18786907196045 \n",
      "Epoch: 6350 | MAE Train Loss: 3.9836907386779785 | MAE Test Loss: 9.183713912963867 \n",
      "Epoch: 6360 | MAE Train Loss: 3.98016357421875 | MAE Test Loss: 9.179559707641602 \n",
      "Epoch: 6370 | MAE Train Loss: 3.9766361713409424 | MAE Test Loss: 9.175405502319336 \n",
      "Epoch: 6380 | MAE Train Loss: 3.9731087684631348 | MAE Test Loss: 9.17125129699707 \n",
      "Epoch: 6390 | MAE Train Loss: 3.9695816040039062 | MAE Test Loss: 9.167097091674805 \n",
      "Epoch: 6400 | MAE Train Loss: 3.9660542011260986 | MAE Test Loss: 9.162942886352539 \n",
      "Epoch: 6410 | MAE Train Loss: 3.96252703666687 | MAE Test Loss: 9.15878677368164 \n",
      "Epoch: 6420 | MAE Train Loss: 3.9589996337890625 | MAE Test Loss: 9.154632568359375 \n",
      "Epoch: 6430 | MAE Train Loss: 3.9554734230041504 | MAE Test Loss: 9.150477409362793 \n",
      "Epoch: 6440 | MAE Train Loss: 3.9519455432891846 | MAE Test Loss: 9.146323204040527 \n",
      "Epoch: 6450 | MAE Train Loss: 3.948418140411377 | MAE Test Loss: 9.142168998718262 \n",
      "Epoch: 6460 | MAE Train Loss: 3.9448904991149902 | MAE Test Loss: 9.138015747070312 \n",
      "Epoch: 6470 | MAE Train Loss: 3.941364288330078 | MAE Test Loss: 9.133859634399414 \n",
      "Epoch: 6480 | MAE Train Loss: 3.9378368854522705 | MAE Test Loss: 9.129705429077148 \n",
      "Epoch: 6490 | MAE Train Loss: 3.934309482574463 | MAE Test Loss: 9.125551223754883 \n",
      "Epoch: 6500 | MAE Train Loss: 3.9307823181152344 | MAE Test Loss: 9.121397018432617 \n",
      "Epoch: 6510 | MAE Train Loss: 3.927255153656006 | MAE Test Loss: 9.117242813110352 \n",
      "Epoch: 6520 | MAE Train Loss: 3.9237282276153564 | MAE Test Loss: 9.11308765411377 \n",
      "Epoch: 6530 | MAE Train Loss: 3.920200824737549 | MAE Test Loss: 9.108932495117188 \n",
      "Epoch: 6540 | MAE Train Loss: 3.9166741371154785 | MAE Test Loss: 9.104780197143555 \n",
      "Epoch: 6550 | MAE Train Loss: 3.9131462574005127 | MAE Test Loss: 9.100625038146973 \n",
      "Epoch: 6560 | MAE Train Loss: 3.909619092941284 | MAE Test Loss: 9.096470832824707 \n",
      "Epoch: 6570 | MAE Train Loss: 3.9060921669006348 | MAE Test Loss: 9.092315673828125 \n",
      "Epoch: 6580 | MAE Train Loss: 3.9025650024414062 | MAE Test Loss: 9.08816146850586 \n",
      "Epoch: 6590 | MAE Train Loss: 3.8990375995635986 | MAE Test Loss: 9.084005355834961 \n",
      "Epoch: 6600 | MAE Train Loss: 3.8955109119415283 | MAE Test Loss: 9.079852104187012 \n",
      "Epoch: 6610 | MAE Train Loss: 3.8919830322265625 | MAE Test Loss: 9.075697898864746 \n",
      "Epoch: 6620 | MAE Train Loss: 3.888455867767334 | MAE Test Loss: 9.071542739868164 \n",
      "Epoch: 6630 | MAE Train Loss: 3.8849291801452637 | MAE Test Loss: 9.067388534545898 \n",
      "Epoch: 6640 | MAE Train Loss: 3.881401777267456 | MAE Test Loss: 9.063234329223633 \n",
      "Epoch: 6650 | MAE Train Loss: 3.8778748512268066 | MAE Test Loss: 9.059080123901367 \n",
      "Epoch: 6660 | MAE Train Loss: 3.8743464946746826 | MAE Test Loss: 9.054924964904785 \n",
      "Epoch: 6670 | MAE Train Loss: 3.870820999145508 | MAE Test Loss: 9.05008316040039 \n",
      "Epoch: 6680 | MAE Train Loss: 3.8673927783966064 | MAE Test Loss: 9.042497634887695 \n",
      "Epoch: 6690 | MAE Train Loss: 3.863957643508911 | MAE Test Loss: 9.034224510192871 \n",
      "Epoch: 6700 | MAE Train Loss: 3.8605258464813232 | MAE Test Loss: 9.026296615600586 \n",
      "Epoch: 6710 | MAE Train Loss: 3.857088088989258 | MAE Test Loss: 9.018022537231445 \n",
      "Epoch: 6720 | MAE Train Loss: 3.8536574840545654 | MAE Test Loss: 9.01043701171875 \n",
      "Epoch: 6730 | MAE Train Loss: 3.850221633911133 | MAE Test Loss: 9.002507209777832 \n",
      "Epoch: 6740 | MAE Train Loss: 3.8467929363250732 | MAE Test Loss: 8.994235038757324 \n",
      "Epoch: 6750 | MAE Train Loss: 3.8433544635772705 | MAE Test Loss: 8.985960960388184 \n",
      "Epoch: 6760 | MAE Train Loss: 3.8399224281311035 | MAE Test Loss: 8.978375434875488 \n",
      "Epoch: 6770 | MAE Train Loss: 3.8364920616149902 | MAE Test Loss: 8.970101356506348 \n",
      "Epoch: 6780 | MAE Train Loss: 3.8330540657043457 | MAE Test Loss: 8.961828231811523 \n",
      "Epoch: 6790 | MAE Train Loss: 3.829622983932495 | MAE Test Loss: 8.954241752624512 \n",
      "Epoch: 6800 | MAE Train Loss: 3.82619047164917 | MAE Test Loss: 8.945968627929688 \n",
      "Epoch: 6810 | MAE Train Loss: 3.8227524757385254 | MAE Test Loss: 8.937695503234863 \n",
      "Epoch: 6820 | MAE Train Loss: 3.819324016571045 | MAE Test Loss: 8.930108070373535 \n",
      "Epoch: 6830 | MAE Train Loss: 3.815889835357666 | MAE Test Loss: 8.921834945678711 \n",
      "Epoch: 6840 | MAE Train Loss: 3.8124523162841797 | MAE Test Loss: 8.9142484664917 \n",
      "Epoch: 6850 | MAE Train Loss: 3.809025287628174 | MAE Test Loss: 8.905975341796875 \n",
      "Epoch: 6860 | MAE Train Loss: 3.805588483810425 | MAE Test Loss: 8.897703170776367 \n",
      "Epoch: 6870 | MAE Train Loss: 3.8021531105041504 | MAE Test Loss: 8.890115737915039 \n",
      "Epoch: 6880 | MAE Train Loss: 3.79872465133667 | MAE Test Loss: 8.881841659545898 \n",
      "Epoch: 6890 | MAE Train Loss: 3.7952868938446045 | MAE Test Loss: 8.873568534851074 \n",
      "Epoch: 6900 | MAE Train Loss: 3.7918541431427 | MAE Test Loss: 8.865983009338379 \n",
      "Epoch: 6910 | MAE Train Loss: 3.788423538208008 | MAE Test Loss: 8.857708930969238 \n",
      "Epoch: 6920 | MAE Train Loss: 3.7849857807159424 | MAE Test Loss: 8.849435806274414 \n",
      "Epoch: 6930 | MAE Train Loss: 3.781554698944092 | MAE Test Loss: 8.841848373413086 \n",
      "Epoch: 6940 | MAE Train Loss: 3.7781219482421875 | MAE Test Loss: 8.833576202392578 \n",
      "Epoch: 6950 | MAE Train Loss: 3.7746853828430176 | MAE Test Loss: 8.825302124023438 \n",
      "Epoch: 6960 | MAE Train Loss: 3.7712562084198 | MAE Test Loss: 8.817715644836426 \n",
      "Epoch: 6970 | MAE Train Loss: 3.7678215503692627 | MAE Test Loss: 8.809442520141602 \n",
      "Epoch: 6980 | MAE Train Loss: 3.7643840312957764 | MAE Test Loss: 8.801856994628906 \n",
      "Epoch: 6990 | MAE Train Loss: 3.760955810546875 | MAE Test Loss: 8.793583869934082 \n",
      "Epoch: 7000 | MAE Train Loss: 3.7575201988220215 | MAE Test Loss: 8.785654067993164 \n",
      "Epoch: 7010 | MAE Train Loss: 3.7540886402130127 | MAE Test Loss: 8.777381896972656 \n",
      "Epoch: 7020 | MAE Train Loss: 3.750650405883789 | MAE Test Loss: 8.769107818603516 \n",
      "Epoch: 7030 | MAE Train Loss: 3.747218608856201 | MAE Test Loss: 8.76117992401123 \n",
      "Epoch: 7040 | MAE Train Loss: 3.7437853813171387 | MAE Test Loss: 8.753593444824219 \n",
      "Epoch: 7050 | MAE Train Loss: 3.7403552532196045 | MAE Test Loss: 8.745320320129395 \n",
      "Epoch: 7060 | MAE Train Loss: 3.736922025680542 | MAE Test Loss: 8.73739242553711 \n",
      "Epoch: 7070 | MAE Train Loss: 3.733485698699951 | MAE Test Loss: 8.729118347167969 \n",
      "Epoch: 7080 | MAE Train Loss: 3.7300503253936768 | MAE Test Loss: 8.721531867980957 \n",
      "Epoch: 7090 | MAE Train Loss: 3.7266228199005127 | MAE Test Loss: 8.71325969696045 \n",
      "Epoch: 7100 | MAE Train Loss: 3.723184585571289 | MAE Test Loss: 8.704985618591309 \n",
      "Epoch: 7110 | MAE Train Loss: 3.7197508811950684 | MAE Test Loss: 8.69739818572998 \n",
      "Epoch: 7120 | MAE Train Loss: 3.7163214683532715 | MAE Test Loss: 8.689125061035156 \n",
      "Epoch: 7130 | MAE Train Loss: 3.712883472442627 | MAE Test Loss: 8.680851936340332 \n",
      "Epoch: 7140 | MAE Train Loss: 3.7094521522521973 | MAE Test Loss: 8.67326545715332 \n",
      "Epoch: 7150 | MAE Train Loss: 3.7060203552246094 | MAE Test Loss: 8.66499137878418 \n",
      "Epoch: 7160 | MAE Train Loss: 3.7025821208953857 | MAE Test Loss: 8.656718254089355 \n",
      "Epoch: 7170 | MAE Train Loss: 3.699152708053589 | MAE Test Loss: 8.649131774902344 \n",
      "Epoch: 7180 | MAE Train Loss: 3.695718765258789 | MAE Test Loss: 8.640859603881836 \n",
      "Epoch: 7190 | MAE Train Loss: 3.6922812461853027 | MAE Test Loss: 8.633272171020508 \n",
      "Epoch: 7200 | MAE Train Loss: 3.6888535022735596 | MAE Test Loss: 8.624998092651367 \n",
      "Epoch: 7210 | MAE Train Loss: 3.685417890548706 | MAE Test Loss: 8.61672592163086 \n",
      "Epoch: 7220 | MAE Train Loss: 3.6819820404052734 | MAE Test Loss: 8.609140396118164 \n",
      "Epoch: 7230 | MAE Train Loss: 3.6785545349121094 | MAE Test Loss: 8.600866317749023 \n",
      "Epoch: 7240 | MAE Train Loss: 3.675117015838623 | MAE Test Loss: 8.5925931930542 \n",
      "Epoch: 7250 | MAE Train Loss: 3.6716830730438232 | MAE Test Loss: 8.585004806518555 \n",
      "Epoch: 7260 | MAE Train Loss: 3.6682536602020264 | MAE Test Loss: 8.576733589172363 \n",
      "Epoch: 7270 | MAE Train Loss: 3.6648154258728027 | MAE Test Loss: 8.568460464477539 \n",
      "Epoch: 7280 | MAE Train Loss: 3.661383867263794 | MAE Test Loss: 8.560873031616211 \n",
      "Epoch: 7290 | MAE Train Loss: 3.657952070236206 | MAE Test Loss: 8.55259895324707 \n",
      "Epoch: 7300 | MAE Train Loss: 3.6545143127441406 | MAE Test Loss: 8.544326782226562 \n",
      "Epoch: 7310 | MAE Train Loss: 3.6510848999023438 | MAE Test Loss: 8.536739349365234 \n",
      "Epoch: 7320 | MAE Train Loss: 3.647650957107544 | MAE Test Loss: 8.528467178344727 \n",
      "Epoch: 7330 | MAE Train Loss: 3.6442131996154785 | MAE Test Loss: 8.520538330078125 \n",
      "Epoch: 7340 | MAE Train Loss: 3.6407814025878906 | MAE Test Loss: 8.512266159057617 \n",
      "Epoch: 7350 | MAE Train Loss: 3.6373493671417236 | MAE Test Loss: 8.504678726196289 \n",
      "Epoch: 7360 | MAE Train Loss: 3.633918046951294 | MAE Test Loss: 8.496404647827148 \n",
      "Epoch: 7370 | MAE Train Loss: 3.630486249923706 | MAE Test Loss: 8.488476753234863 \n",
      "Epoch: 7380 | MAE Train Loss: 3.6270484924316406 | MAE Test Loss: 8.480203628540039 \n",
      "Epoch: 7390 | MAE Train Loss: 3.6236140727996826 | MAE Test Loss: 8.472618103027344 \n",
      "Epoch: 7400 | MAE Train Loss: 3.620178699493408 | MAE Test Loss: 8.46400260925293 \n",
      "Epoch: 7410 | MAE Train Loss: 3.6167502403259277 | MAE Test Loss: 8.456415176391602 \n",
      "Epoch: 7420 | MAE Train Loss: 3.6133148670196533 | MAE Test Loss: 8.448138236999512 \n",
      "Epoch: 7430 | MAE Train Loss: 3.609877824783325 | MAE Test Loss: 8.440549850463867 \n",
      "Epoch: 7440 | MAE Train Loss: 3.606449842453003 | MAE Test Loss: 8.432273864746094 \n",
      "Epoch: 7450 | MAE Train Loss: 3.6030120849609375 | MAE Test Loss: 8.42399787902832 \n",
      "Epoch: 7460 | MAE Train Loss: 3.5995776653289795 | MAE Test Loss: 8.41640853881836 \n",
      "Epoch: 7470 | MAE Train Loss: 3.5961480140686035 | MAE Test Loss: 8.408132553100586 \n",
      "Epoch: 7480 | MAE Train Loss: 3.5927093029022217 | MAE Test Loss: 8.399858474731445 \n",
      "Epoch: 7490 | MAE Train Loss: 3.5892767906188965 | MAE Test Loss: 8.392269134521484 \n",
      "Epoch: 7500 | MAE Train Loss: 3.5858452320098877 | MAE Test Loss: 8.383993148803711 \n",
      "Epoch: 7510 | MAE Train Loss: 3.5824062824249268 | MAE Test Loss: 8.375718116760254 \n",
      "Epoch: 7520 | MAE Train Loss: 3.5789763927459717 | MAE Test Loss: 8.368127822875977 \n",
      "Epoch: 7530 | MAE Train Loss: 3.575542449951172 | MAE Test Loss: 8.35985279083252 \n",
      "Epoch: 7540 | MAE Train Loss: 3.5721046924591064 | MAE Test Loss: 8.351921081542969 \n",
      "Epoch: 7550 | MAE Train Loss: 3.5686721801757812 | MAE Test Loss: 8.343647956848145 \n",
      "Epoch: 7560 | MAE Train Loss: 3.565239429473877 | MAE Test Loss: 8.33605670928955 \n",
      "Epoch: 7570 | MAE Train Loss: 3.561807155609131 | MAE Test Loss: 8.327781677246094 \n",
      "Epoch: 7580 | MAE Train Loss: 3.5583693981170654 | MAE Test Loss: 8.31950569152832 \n",
      "Epoch: 7590 | MAE Train Loss: 3.5549397468566895 | MAE Test Loss: 8.311917304992676 \n",
      "Epoch: 7600 | MAE Train Loss: 3.5515053272247314 | MAE Test Loss: 8.303640365600586 \n",
      "Epoch: 7610 | MAE Train Loss: 3.5480666160583496 | MAE Test Loss: 8.296052932739258 \n",
      "Epoch: 7620 | MAE Train Loss: 3.5446383953094482 | MAE Test Loss: 8.287775993347168 \n",
      "Epoch: 7630 | MAE Train Loss: 3.5412025451660156 | MAE Test Loss: 8.279500007629395 \n",
      "Epoch: 7640 | MAE Train Loss: 3.537766218185425 | MAE Test Loss: 8.27191162109375 \n",
      "Epoch: 7650 | MAE Train Loss: 3.5343379974365234 | MAE Test Loss: 8.263635635375977 \n",
      "Epoch: 7660 | MAE Train Loss: 3.5308995246887207 | MAE Test Loss: 8.25536060333252 \n",
      "Epoch: 7670 | MAE Train Loss: 3.527466297149658 | MAE Test Loss: 8.247772216796875 \n",
      "Epoch: 7680 | MAE Train Loss: 3.524035692214966 | MAE Test Loss: 8.239494323730469 \n",
      "Epoch: 7690 | MAE Train Loss: 3.520596981048584 | MAE Test Loss: 8.231220245361328 \n",
      "Epoch: 7700 | MAE Train Loss: 3.517165422439575 | MAE Test Loss: 8.223630905151367 \n",
      "Epoch: 7710 | MAE Train Loss: 3.51373291015625 | MAE Test Loss: 8.21535587310791 \n",
      "Epoch: 7720 | MAE Train Loss: 3.5102944374084473 | MAE Test Loss: 8.20707893371582 \n",
      "Epoch: 7730 | MAE Train Loss: 3.5068650245666504 | MAE Test Loss: 8.199490547180176 \n",
      "Epoch: 7740 | MAE Train Loss: 3.503430128097534 | MAE Test Loss: 8.191214561462402 \n",
      "Epoch: 7750 | MAE Train Loss: 3.4999923706054688 | MAE Test Loss: 8.183625221252441 \n",
      "Epoch: 7760 | MAE Train Loss: 3.4965643882751465 | MAE Test Loss: 8.175350189208984 \n",
      "Epoch: 7770 | MAE Train Loss: 3.4931278228759766 | MAE Test Loss: 8.16741943359375 \n",
      "Epoch: 7780 | MAE Train Loss: 3.4896950721740723 | MAE Test Loss: 8.159144401550293 \n",
      "Epoch: 7790 | MAE Train Loss: 3.4862568378448486 | MAE Test Loss: 8.15086841583252 \n",
      "Epoch: 7800 | MAE Train Loss: 3.482827663421631 | MAE Test Loss: 8.143278121948242 \n",
      "Epoch: 7810 | MAE Train Loss: 3.4793930053710938 | MAE Test Loss: 8.135004043579102 \n",
      "Epoch: 7820 | MAE Train Loss: 3.4759552478790283 | MAE Test Loss: 8.127413749694824 \n",
      "Epoch: 7830 | MAE Train Loss: 3.472527027130127 | MAE Test Loss: 8.11913776397705 \n",
      "Epoch: 7840 | MAE Train Loss: 3.469090223312378 | MAE Test Loss: 8.110862731933594 \n",
      "Epoch: 7850 | MAE Train Loss: 3.4656548500061035 | MAE Test Loss: 8.10327434539795 \n",
      "Epoch: 7860 | MAE Train Loss: 3.4622256755828857 | MAE Test Loss: 8.094998359680176 \n",
      "Epoch: 7870 | MAE Train Loss: 3.458787441253662 | MAE Test Loss: 8.086722373962402 \n",
      "Epoch: 7880 | MAE Train Loss: 3.4553542137145996 | MAE Test Loss: 8.079134941101074 \n",
      "Epoch: 7890 | MAE Train Loss: 3.451923370361328 | MAE Test Loss: 8.070856094360352 \n",
      "Epoch: 7900 | MAE Train Loss: 3.448484420776367 | MAE Test Loss: 8.062582015991211 \n",
      "Epoch: 7910 | MAE Train Loss: 3.4450535774230957 | MAE Test Loss: 8.054993629455566 \n",
      "Epoch: 7920 | MAE Train Loss: 3.4416205883026123 | MAE Test Loss: 8.046717643737793 \n",
      "Epoch: 7930 | MAE Train Loss: 3.4381816387176514 | MAE Test Loss: 8.03844165802002 \n",
      "Epoch: 7940 | MAE Train Loss: 3.43475341796875 | MAE Test Loss: 8.030852317810059 \n",
      "Epoch: 7950 | MAE Train Loss: 3.4313178062438965 | MAE Test Loss: 8.022577285766602 \n",
      "Epoch: 7960 | MAE Train Loss: 3.4278855323791504 | MAE Test Loss: 8.01464557647705 \n",
      "Epoch: 7970 | MAE Train Loss: 3.4244468212127686 | MAE Test Loss: 8.006370544433594 \n",
      "Epoch: 7980 | MAE Train Loss: 3.421016216278076 | MAE Test Loss: 7.998782157897949 \n",
      "Epoch: 7990 | MAE Train Loss: 3.4175827503204346 | MAE Test Loss: 7.990506172180176 \n",
      "Epoch: 8000 | MAE Train Loss: 3.4141440391540527 | MAE Test Loss: 7.982229709625244 \n",
      "Epoch: 8010 | MAE Train Loss: 3.4107162952423096 | MAE Test Loss: 7.9746413230896 \n",
      "Epoch: 8020 | MAE Train Loss: 3.4072799682617188 | MAE Test Loss: 7.966364860534668 \n",
      "Epoch: 8030 | MAE Train Loss: 3.403843641281128 | MAE Test Loss: 7.958776950836182 \n",
      "Epoch: 8040 | MAE Train Loss: 3.4004154205322266 | MAE Test Loss: 7.95050048828125 \n",
      "Epoch: 8050 | MAE Train Loss: 3.3969779014587402 | MAE Test Loss: 7.942225456237793 \n",
      "Epoch: 8060 | MAE Train Loss: 3.393543243408203 | MAE Test Loss: 7.934636116027832 \n",
      "Epoch: 8070 | MAE Train Loss: 3.390113353729248 | MAE Test Loss: 7.926361083984375 \n",
      "Epoch: 8080 | MAE Train Loss: 3.3866748809814453 | MAE Test Loss: 7.918084621429443 \n",
      "Epoch: 8090 | MAE Train Loss: 3.3832428455352783 | MAE Test Loss: 7.910496711730957 \n",
      "Epoch: 8100 | MAE Train Loss: 3.3798110485076904 | MAE Test Loss: 7.902219295501709 \n",
      "Epoch: 8110 | MAE Train Loss: 3.3763725757598877 | MAE Test Loss: 7.893943786621094 \n",
      "Epoch: 8120 | MAE Train Loss: 3.3729424476623535 | MAE Test Loss: 7.886355400085449 \n",
      "Epoch: 8130 | MAE Train Loss: 3.3695080280303955 | MAE Test Loss: 7.878079891204834 \n",
      "Epoch: 8140 | MAE Train Loss: 3.366069793701172 | MAE Test Loss: 7.870491027832031 \n",
      "Epoch: 8150 | MAE Train Loss: 3.3626415729522705 | MAE Test Loss: 7.862215518951416 \n",
      "Epoch: 8160 | MAE Train Loss: 3.359205722808838 | MAE Test Loss: 7.853939056396484 \n",
      "Epoch: 8170 | MAE Train Loss: 3.355769395828247 | MAE Test Loss: 7.846350193023682 \n",
      "Epoch: 8180 | MAE Train Loss: 3.352334499359131 | MAE Test Loss: 7.837731838226318 \n",
      "Epoch: 8190 | MAE Train Loss: 3.3489043712615967 | MAE Test Loss: 7.830143928527832 \n",
      "Epoch: 8200 | MAE Train Loss: 3.345470428466797 | MAE Test Loss: 7.821868896484375 \n",
      "Epoch: 8210 | MAE Train Loss: 3.3420326709747314 | MAE Test Loss: 7.814279079437256 \n",
      "Epoch: 8220 | MAE Train Loss: 3.33860445022583 | MAE Test Loss: 7.806004524230957 \n",
      "Epoch: 8230 | MAE Train Loss: 3.3351681232452393 | MAE Test Loss: 7.797728061676025 \n",
      "Epoch: 8240 | MAE Train Loss: 3.3317313194274902 | MAE Test Loss: 7.790138244628906 \n",
      "Epoch: 8250 | MAE Train Loss: 3.328303575515747 | MAE Test Loss: 7.781863212585449 \n",
      "Epoch: 8260 | MAE Train Loss: 3.3248658180236816 | MAE Test Loss: 7.773587226867676 \n",
      "Epoch: 8270 | MAE Train Loss: 3.3214316368103027 | MAE Test Loss: 7.765998840332031 \n",
      "Epoch: 8280 | MAE Train Loss: 3.3180012702941895 | MAE Test Loss: 7.7577223777771 \n",
      "Epoch: 8290 | MAE Train Loss: 3.3145623207092285 | MAE Test Loss: 7.749446868896484 \n",
      "Epoch: 8300 | MAE Train Loss: 3.311131000518799 | MAE Test Loss: 7.741858005523682 \n",
      "Epoch: 8310 | MAE Train Loss: 3.3076980113983154 | MAE Test Loss: 7.73358154296875 \n",
      "Epoch: 8320 | MAE Train Loss: 3.304259777069092 | MAE Test Loss: 7.725306034088135 \n",
      "Epoch: 8330 | MAE Train Loss: 3.300830364227295 | MAE Test Loss: 7.717718601226807 \n",
      "Epoch: 8340 | MAE Train Loss: 3.297395706176758 | MAE Test Loss: 7.709442138671875 \n",
      "Epoch: 8350 | MAE Train Loss: 3.2939581871032715 | MAE Test Loss: 7.701852321624756 \n",
      "Epoch: 8360 | MAE Train Loss: 3.2905304431915283 | MAE Test Loss: 7.693576812744141 \n",
      "Epoch: 8370 | MAE Train Loss: 3.2870941162109375 | MAE Test Loss: 7.685646057128906 \n",
      "Epoch: 8380 | MAE Train Loss: 3.283660888671875 | MAE Test Loss: 7.677371978759766 \n",
      "Epoch: 8390 | MAE Train Loss: 3.2802224159240723 | MAE Test Loss: 7.669095039367676 \n",
      "Epoch: 8400 | MAE Train Loss: 3.2767930030822754 | MAE Test Loss: 7.661506652832031 \n",
      "Epoch: 8410 | MAE Train Loss: 3.273358106613159 | MAE Test Loss: 7.6532301902771 \n",
      "Epoch: 8420 | MAE Train Loss: 3.2699203491210938 | MAE Test Loss: 7.645641326904297 \n",
      "Epoch: 8430 | MAE Train Loss: 3.2664928436279297 | MAE Test Loss: 7.637365818023682 \n",
      "Epoch: 8440 | MAE Train Loss: 3.2630553245544434 | MAE Test Loss: 7.62908935546875 \n",
      "Epoch: 8450 | MAE Train Loss: 3.259619951248169 | MAE Test Loss: 7.6215009689331055 \n",
      "Epoch: 8460 | MAE Train Loss: 3.2561912536621094 | MAE Test Loss: 7.613224983215332 \n",
      "Epoch: 8470 | MAE Train Loss: 3.2527530193328857 | MAE Test Loss: 7.604949951171875 \n",
      "Epoch: 8480 | MAE Train Loss: 3.249319553375244 | MAE Test Loss: 7.597360134124756 \n",
      "Epoch: 8490 | MAE Train Loss: 3.2458884716033936 | MAE Test Loss: 7.589085578918457 \n",
      "Epoch: 8500 | MAE Train Loss: 3.24245023727417 | MAE Test Loss: 7.580809116363525 \n",
      "Epoch: 8510 | MAE Train Loss: 3.2390193939208984 | MAE Test Loss: 7.573219299316406 \n",
      "Epoch: 8520 | MAE Train Loss: 3.235586166381836 | MAE Test Loss: 7.564946174621582 \n",
      "Epoch: 8530 | MAE Train Loss: 3.2321479320526123 | MAE Test Loss: 7.556668281555176 \n",
      "Epoch: 8540 | MAE Train Loss: 3.2287185192108154 | MAE Test Loss: 7.549079895019531 \n",
      "Epoch: 8550 | MAE Train Loss: 3.225283145904541 | MAE Test Loss: 7.540804386138916 \n",
      "Epoch: 8560 | MAE Train Loss: 3.221846342086792 | MAE Test Loss: 7.533215522766113 \n",
      "Epoch: 8570 | MAE Train Loss: 3.2184181213378906 | MAE Test Loss: 7.524939060211182 \n",
      "Epoch: 8580 | MAE Train Loss: 3.2149810791015625 | MAE Test Loss: 7.516663551330566 \n",
      "Epoch: 8590 | MAE Train Loss: 3.211545944213867 | MAE Test Loss: 7.5090742111206055 \n",
      "Epoch: 8600 | MAE Train Loss: 3.2081103324890137 | MAE Test Loss: 7.5004563331604 \n",
      "Epoch: 8610 | MAE Train Loss: 3.204681873321533 | MAE Test Loss: 7.492868900299072 \n",
      "Epoch: 8620 | MAE Train Loss: 3.2012457847595215 | MAE Test Loss: 7.484593391418457 \n",
      "Epoch: 8630 | MAE Train Loss: 3.1978087425231934 | MAE Test Loss: 7.477003574371338 \n",
      "Epoch: 8640 | MAE Train Loss: 3.1943812370300293 | MAE Test Loss: 7.468728065490723 \n",
      "Epoch: 8650 | MAE Train Loss: 3.190943479537964 | MAE Test Loss: 7.460452079772949 \n",
      "Epoch: 8660 | MAE Train Loss: 3.1875085830688477 | MAE Test Loss: 7.4528632164001465 \n",
      "Epoch: 8670 | MAE Train Loss: 3.1840789318084717 | MAE Test Loss: 7.444586277008057 \n",
      "Epoch: 8680 | MAE Train Loss: 3.180640697479248 | MAE Test Loss: 7.4363112449646 \n",
      "Epoch: 8690 | MAE Train Loss: 3.177208423614502 | MAE Test Loss: 7.428723335266113 \n",
      "Epoch: 8700 | MAE Train Loss: 3.173776149749756 | MAE Test Loss: 7.420446872711182 \n",
      "Epoch: 8710 | MAE Train Loss: 3.170337677001953 | MAE Test Loss: 7.412171840667725 \n",
      "Epoch: 8720 | MAE Train Loss: 3.166907787322998 | MAE Test Loss: 7.4045820236206055 \n",
      "Epoch: 8730 | MAE Train Loss: 3.163473606109619 | MAE Test Loss: 7.396307468414307 \n",
      "Epoch: 8740 | MAE Train Loss: 3.1600356101989746 | MAE Test Loss: 7.3887176513671875 \n",
      "Epoch: 8750 | MAE Train Loss: 3.156607151031494 | MAE Test Loss: 7.3804426193237305 \n",
      "Epoch: 8760 | MAE Train Loss: 3.1531710624694824 | MAE Test Loss: 7.372166633605957 \n",
      "Epoch: 8770 | MAE Train Loss: 3.1497347354888916 | MAE Test Loss: 7.364576816558838 \n",
      "Epoch: 8780 | MAE Train Loss: 3.1463000774383545 | MAE Test Loss: 7.355959892272949 \n",
      "Epoch: 8790 | MAE Train Loss: 3.1428706645965576 | MAE Test Loss: 7.348370552062988 \n",
      "Epoch: 8800 | MAE Train Loss: 3.1394362449645996 | MAE Test Loss: 7.340096473693848 \n",
      "Epoch: 8810 | MAE Train Loss: 3.135998249053955 | MAE Test Loss: 7.3325066566467285 \n",
      "Epoch: 8820 | MAE Train Loss: 3.1325697898864746 | MAE Test Loss: 7.324231147766113 \n",
      "Epoch: 8830 | MAE Train Loss: 3.1291332244873047 | MAE Test Loss: 7.315954685211182 \n",
      "Epoch: 8840 | MAE Train Loss: 3.125697374343872 | MAE Test Loss: 7.308365821838379 \n",
      "Epoch: 8850 | MAE Train Loss: 3.1222691535949707 | MAE Test Loss: 7.3000898361206055 \n",
      "Epoch: 8860 | MAE Train Loss: 3.118830919265747 | MAE Test Loss: 7.291813850402832 \n",
      "Epoch: 8870 | MAE Train Loss: 3.1153969764709473 | MAE Test Loss: 7.2842254638671875 \n",
      "Epoch: 8880 | MAE Train Loss: 3.111967086791992 | MAE Test Loss: 7.275949001312256 \n",
      "Epoch: 8890 | MAE Train Loss: 3.1085281372070312 | MAE Test Loss: 7.267672538757324 \n",
      "Epoch: 8900 | MAE Train Loss: 3.1050965785980225 | MAE Test Loss: 7.260085105895996 \n",
      "Epoch: 8910 | MAE Train Loss: 3.1016640663146973 | MAE Test Loss: 7.251809597015381 \n",
      "Epoch: 8920 | MAE Train Loss: 3.0982251167297363 | MAE Test Loss: 7.243533134460449 \n",
      "Epoch: 8930 | MAE Train Loss: 3.0947959423065186 | MAE Test Loss: 7.235943794250488 \n",
      "Epoch: 8940 | MAE Train Loss: 3.0913615226745605 | MAE Test Loss: 7.227668762207031 \n",
      "Epoch: 8950 | MAE Train Loss: 3.0879242420196533 | MAE Test Loss: 7.220080375671387 \n",
      "Epoch: 8960 | MAE Train Loss: 3.0844955444335938 | MAE Test Loss: 7.211804389953613 \n",
      "Epoch: 8970 | MAE Train Loss: 3.081058979034424 | MAE Test Loss: 7.203527927398682 \n",
      "Epoch: 8980 | MAE Train Loss: 3.077623128890991 | MAE Test Loss: 7.195939540863037 \n",
      "Epoch: 8990 | MAE Train Loss: 3.0741944313049316 | MAE Test Loss: 7.1876630783081055 \n",
      "Epoch: 9000 | MAE Train Loss: 3.070755958557129 | MAE Test Loss: 7.17938756942749 \n",
      "Epoch: 9010 | MAE Train Loss: 3.067324161529541 | MAE Test Loss: 7.171457767486572 \n",
      "Epoch: 9020 | MAE Train Loss: 3.0638866424560547 | MAE Test Loss: 7.1638689041137695 \n",
      "Epoch: 9030 | MAE Train Loss: 3.060458183288574 | MAE Test Loss: 7.155592441558838 \n",
      "Epoch: 9040 | MAE Train Loss: 3.057021141052246 | MAE Test Loss: 7.147316932678223 \n",
      "Epoch: 9050 | MAE Train Loss: 3.0535857677459717 | MAE Test Loss: 7.139727592468262 \n",
      "Epoch: 9060 | MAE Train Loss: 3.050156831741333 | MAE Test Loss: 7.1314520835876465 \n",
      "Epoch: 9070 | MAE Train Loss: 3.046718120574951 | MAE Test Loss: 7.123176574707031 \n",
      "Epoch: 9080 | MAE Train Loss: 3.043285369873047 | MAE Test Loss: 7.1155877113342285 \n",
      "Epoch: 9090 | MAE Train Loss: 3.0398545265197754 | MAE Test Loss: 7.107312202453613 \n",
      "Epoch: 9100 | MAE Train Loss: 3.0364158153533936 | MAE Test Loss: 7.099035739898682 \n",
      "Epoch: 9110 | MAE Train Loss: 3.032984972000122 | MAE Test Loss: 7.091446876525879 \n",
      "Epoch: 9120 | MAE Train Loss: 3.0295515060424805 | MAE Test Loss: 7.0831708908081055 \n",
      "Epoch: 9130 | MAE Train Loss: 3.0261130332946777 | MAE Test Loss: 7.074896335601807 \n",
      "Epoch: 9140 | MAE Train Loss: 3.0226845741271973 | MAE Test Loss: 7.067307949066162 \n",
      "Epoch: 9150 | MAE Train Loss: 3.019249439239502 | MAE Test Loss: 7.059031009674072 \n",
      "Epoch: 9160 | MAE Train Loss: 3.015812397003174 | MAE Test Loss: 7.0514421463012695 \n",
      "Epoch: 9170 | MAE Train Loss: 3.0123839378356934 | MAE Test Loss: 7.043166160583496 \n",
      "Epoch: 9180 | MAE Train Loss: 3.008946180343628 | MAE Test Loss: 7.034890651702881 \n",
      "Epoch: 9190 | MAE Train Loss: 3.00551176071167 | MAE Test Loss: 7.0273027420043945 \n",
      "Epoch: 9200 | MAE Train Loss: 3.002082109451294 | MAE Test Loss: 7.0190253257751465 \n",
      "Epoch: 9210 | MAE Train Loss: 2.998643398284912 | MAE Test Loss: 7.010749816894531 \n",
      "Epoch: 9220 | MAE Train Loss: 2.995211124420166 | MAE Test Loss: 7.003161430358887 \n",
      "Epoch: 9230 | MAE Train Loss: 2.991779088973999 | MAE Test Loss: 6.994885444641113 \n",
      "Epoch: 9240 | MAE Train Loss: 2.988346815109253 | MAE Test Loss: 6.986955165863037 \n",
      "Epoch: 9250 | MAE Train Loss: 2.9849085807800293 | MAE Test Loss: 6.9786787033081055 \n",
      "Epoch: 9260 | MAE Train Loss: 2.9814741611480713 | MAE Test Loss: 6.971090793609619 \n",
      "Epoch: 9270 | MAE Train Loss: 2.978044271469116 | MAE Test Loss: 6.9628143310546875 \n",
      "Epoch: 9280 | MAE Train Loss: 2.9746060371398926 | MAE Test Loss: 6.954538822174072 \n",
      "Epoch: 9290 | MAE Train Loss: 2.9711737632751465 | MAE Test Loss: 6.9469499588012695 \n",
      "Epoch: 9300 | MAE Train Loss: 2.9677417278289795 | MAE Test Loss: 6.938673496246338 \n",
      "Epoch: 9310 | MAE Train Loss: 2.964303493499756 | MAE Test Loss: 6.930398464202881 \n",
      "Epoch: 9320 | MAE Train Loss: 2.9608733654022217 | MAE Test Loss: 6.922808647155762 \n",
      "Epoch: 9330 | MAE Train Loss: 2.9574389457702637 | MAE Test Loss: 6.9145331382751465 \n",
      "Epoch: 9340 | MAE Train Loss: 2.954000949859619 | MAE Test Loss: 6.906944274902344 \n",
      "Epoch: 9350 | MAE Train Loss: 2.9505724906921387 | MAE Test Loss: 6.898669242858887 \n",
      "Epoch: 9360 | MAE Train Loss: 2.947136640548706 | MAE Test Loss: 6.890393257141113 \n",
      "Epoch: 9370 | MAE Train Loss: 2.9437007904052734 | MAE Test Loss: 6.882804870605469 \n",
      "Epoch: 9380 | MAE Train Loss: 2.940272569656372 | MAE Test Loss: 6.874528408050537 \n",
      "Epoch: 9390 | MAE Train Loss: 2.936833620071411 | MAE Test Loss: 6.8662519454956055 \n",
      "Epoch: 9400 | MAE Train Loss: 2.9333999156951904 | MAE Test Loss: 6.858664035797119 \n",
      "Epoch: 9410 | MAE Train Loss: 2.9299697875976562 | MAE Test Loss: 6.8503875732421875 \n",
      "Epoch: 9420 | MAE Train Loss: 2.926535129547119 | MAE Test Loss: 6.8424577713012695 \n",
      "Epoch: 9430 | MAE Train Loss: 2.9230990409851074 | MAE Test Loss: 6.834181308746338 \n",
      "Epoch: 9440 | MAE Train Loss: 2.919663190841675 | MAE Test Loss: 6.826592922210693 \n",
      "Epoch: 9450 | MAE Train Loss: 2.9162347316741943 | MAE Test Loss: 6.818317413330078 \n",
      "Epoch: 9460 | MAE Train Loss: 2.9127960205078125 | MAE Test Loss: 6.8100409507751465 \n",
      "Epoch: 9470 | MAE Train Loss: 2.909362554550171 | MAE Test Loss: 6.802452087402344 \n",
      "Epoch: 9480 | MAE Train Loss: 2.9059321880340576 | MAE Test Loss: 6.7941765785217285 \n",
      "Epoch: 9490 | MAE Train Loss: 2.902493715286255 | MAE Test Loss: 6.785901069641113 \n",
      "Epoch: 9500 | MAE Train Loss: 2.899062156677246 | MAE Test Loss: 6.778311729431152 \n",
      "Epoch: 9510 | MAE Train Loss: 2.895629405975342 | MAE Test Loss: 6.770036220550537 \n",
      "Epoch: 9520 | MAE Train Loss: 2.892190933227539 | MAE Test Loss: 6.7617597579956055 \n",
      "Epoch: 9530 | MAE Train Loss: 2.8887619972229004 | MAE Test Loss: 6.754171848297119 \n",
      "Epoch: 9540 | MAE Train Loss: 2.885326623916626 | MAE Test Loss: 6.745896339416504 \n",
      "Epoch: 9550 | MAE Train Loss: 2.8818888664245605 | MAE Test Loss: 6.738306522369385 \n",
      "Epoch: 9560 | MAE Train Loss: 2.8784613609313965 | MAE Test Loss: 6.730032444000244 \n",
      "Epoch: 9570 | MAE Train Loss: 2.8750243186950684 | MAE Test Loss: 6.721754550933838 \n",
      "Epoch: 9580 | MAE Train Loss: 2.8715884685516357 | MAE Test Loss: 6.714166164398193 \n",
      "Epoch: 9590 | MAE Train Loss: 2.8681600093841553 | MAE Test Loss: 6.705890655517578 \n",
      "Epoch: 9600 | MAE Train Loss: 2.8647217750549316 | MAE Test Loss: 6.697615146636963 \n",
      "Epoch: 9610 | MAE Train Loss: 2.861288547515869 | MAE Test Loss: 6.69002628326416 \n",
      "Epoch: 9620 | MAE Train Loss: 2.8578572273254395 | MAE Test Loss: 6.681750297546387 \n",
      "Epoch: 9630 | MAE Train Loss: 2.8544185161590576 | MAE Test Loss: 6.673474311828613 \n",
      "Epoch: 9640 | MAE Train Loss: 2.8509879112243652 | MAE Test Loss: 6.665884971618652 \n",
      "Epoch: 9650 | MAE Train Loss: 2.8475513458251953 | MAE Test Loss: 6.657955169677734 \n",
      "Epoch: 9660 | MAE Train Loss: 2.8441226482391357 | MAE Test Loss: 6.649679660797119 \n",
      "Epoch: 9670 | MAE Train Loss: 2.840683937072754 | MAE Test Loss: 6.6414031982421875 \n",
      "Epoch: 9680 | MAE Train Loss: 2.8372511863708496 | MAE Test Loss: 6.633814334869385 \n",
      "Epoch: 9690 | MAE Train Loss: 2.83381986618042 | MAE Test Loss: 6.6255388259887695 \n",
      "Epoch: 9700 | MAE Train Loss: 2.830380916595459 | MAE Test Loss: 6.617262840270996 \n",
      "Epoch: 9710 | MAE Train Loss: 2.826950788497925 | MAE Test Loss: 6.609673976898193 \n",
      "Epoch: 9720 | MAE Train Loss: 2.823517322540283 | MAE Test Loss: 6.601397514343262 \n",
      "Epoch: 9730 | MAE Train Loss: 2.8200783729553223 | MAE Test Loss: 6.593122959136963 \n",
      "Epoch: 9740 | MAE Train Loss: 2.816649913787842 | MAE Test Loss: 6.58553409576416 \n",
      "Epoch: 9750 | MAE Train Loss: 2.813214063644409 | MAE Test Loss: 6.5772576332092285 \n",
      "Epoch: 9760 | MAE Train Loss: 2.8097774982452393 | MAE Test Loss: 6.569668769836426 \n",
      "Epoch: 9770 | MAE Train Loss: 2.806349277496338 | MAE Test Loss: 6.5613908767700195 \n",
      "Epoch: 9780 | MAE Train Loss: 2.8029110431671143 | MAE Test Loss: 6.553114414215088 \n",
      "Epoch: 9790 | MAE Train Loss: 2.7994766235351562 | MAE Test Loss: 6.545524597167969 \n",
      "Epoch: 9800 | MAE Train Loss: 2.796046733856201 | MAE Test Loss: 6.537247657775879 \n",
      "Epoch: 9810 | MAE Train Loss: 2.7926077842712402 | MAE Test Loss: 6.5289716720581055 \n",
      "Epoch: 9820 | MAE Train Loss: 2.789175510406494 | MAE Test Loss: 6.521380424499512 \n",
      "Epoch: 9830 | MAE Train Loss: 2.785743236541748 | MAE Test Loss: 6.513104438781738 \n",
      "Epoch: 9840 | MAE Train Loss: 2.782304286956787 | MAE Test Loss: 6.504827976226807 \n",
      "Epoch: 9850 | MAE Train Loss: 2.7788748741149902 | MAE Test Loss: 6.4972381591796875 \n",
      "Epoch: 9860 | MAE Train Loss: 2.775440216064453 | MAE Test Loss: 6.488961696624756 \n",
      "Epoch: 9870 | MAE Train Loss: 2.772002696990967 | MAE Test Loss: 6.481371879577637 \n",
      "Epoch: 9880 | MAE Train Loss: 2.76857328414917 | MAE Test Loss: 6.4730939865112305 \n",
      "Epoch: 9890 | MAE Train Loss: 2.765137195587158 | MAE Test Loss: 6.464818477630615 \n",
      "Epoch: 9900 | MAE Train Loss: 2.7617011070251465 | MAE Test Loss: 6.457228183746338 \n",
      "Epoch: 9910 | MAE Train Loss: 2.758272409439087 | MAE Test Loss: 6.448951721191406 \n",
      "Epoch: 9920 | MAE Train Loss: 2.754833936691284 | MAE Test Loss: 6.440674781799316 \n",
      "Epoch: 9930 | MAE Train Loss: 2.7514002323150635 | MAE Test Loss: 6.433084964752197 \n",
      "Epoch: 9940 | MAE Train Loss: 2.7479686737060547 | MAE Test Loss: 6.424809455871582 \n",
      "Epoch: 9950 | MAE Train Loss: 2.744530439376831 | MAE Test Loss: 6.416531562805176 \n",
      "Epoch: 9960 | MAE Train Loss: 2.7410988807678223 | MAE Test Loss: 6.408941745758057 \n",
      "Epoch: 9970 | MAE Train Loss: 2.7376656532287598 | MAE Test Loss: 6.400665283203125 \n",
      "Epoch: 9980 | MAE Train Loss: 2.734227180480957 | MAE Test Loss: 6.392388820648193 \n",
      "Epoch: 9990 | MAE Train Loss: 2.73079776763916 | MAE Test Loss: 6.384799003601074 \n",
      "Epoch: 10000 | MAE Train Loss: 2.7273623943328857 | MAE Test Loss: 6.376522541046143 \n",
      "Epoch: 10010 | MAE Train Loss: 2.7239251136779785 | MAE Test Loss: 6.368931770324707 \n",
      "Epoch: 10020 | MAE Train Loss: 2.72049617767334 | MAE Test Loss: 6.360655784606934 \n",
      "Epoch: 10030 | MAE Train Loss: 2.71705961227417 | MAE Test Loss: 6.352379322052002 \n",
      "Epoch: 10040 | MAE Train Loss: 2.7136242389678955 | MAE Test Loss: 6.344789028167725 \n",
      "Epoch: 10050 | MAE Train Loss: 2.7101948261260986 | MAE Test Loss: 6.336512565612793 \n",
      "Epoch: 10060 | MAE Train Loss: 2.706756114959717 | MAE Test Loss: 6.328235626220703 \n",
      "Epoch: 10070 | MAE Train Loss: 2.7033231258392334 | MAE Test Loss: 6.3206467628479 \n",
      "Epoch: 10080 | MAE Train Loss: 2.6998915672302246 | MAE Test Loss: 6.312368869781494 \n",
      "Epoch: 10090 | MAE Train Loss: 2.696453094482422 | MAE Test Loss: 6.3040924072265625 \n",
      "Epoch: 10100 | MAE Train Loss: 2.6930222511291504 | MAE Test Loss: 6.296502590179443 \n",
      "Epoch: 10110 | MAE Train Loss: 2.689588785171509 | MAE Test Loss: 6.288226127624512 \n",
      "Epoch: 10120 | MAE Train Loss: 2.6861495971679688 | MAE Test Loss: 6.27994966506958 \n",
      "Epoch: 10130 | MAE Train Loss: 2.6827216148376465 | MAE Test Loss: 6.272359371185303 \n",
      "Epoch: 10140 | MAE Train Loss: 2.6792850494384766 | MAE Test Loss: 6.264082908630371 \n",
      "Epoch: 10150 | MAE Train Loss: 2.6758484840393066 | MAE Test Loss: 6.256493091583252 \n",
      "Epoch: 10160 | MAE Train Loss: 2.6724205017089844 | MAE Test Loss: 6.248216152191162 \n",
      "Epoch: 10170 | MAE Train Loss: 2.6689820289611816 | MAE Test Loss: 6.2399396896362305 \n",
      "Epoch: 10180 | MAE Train Loss: 2.6655476093292236 | MAE Test Loss: 6.232349872589111 \n",
      "Epoch: 10190 | MAE Train Loss: 2.6621174812316895 | MAE Test Loss: 6.224072456359863 \n",
      "Epoch: 10200 | MAE Train Loss: 2.6586787700653076 | MAE Test Loss: 6.21579647064209 \n",
      "Epoch: 10210 | MAE Train Loss: 2.6552467346191406 | MAE Test Loss: 6.208207130432129 \n",
      "Epoch: 10220 | MAE Train Loss: 2.6518142223358154 | MAE Test Loss: 6.199930667877197 \n",
      "Epoch: 10230 | MAE Train Loss: 2.6483757495880127 | MAE Test Loss: 6.191652774810791 \n",
      "Epoch: 10240 | MAE Train Loss: 2.6449458599090576 | MAE Test Loss: 6.184062957763672 \n",
      "Epoch: 10250 | MAE Train Loss: 2.6415109634399414 | MAE Test Loss: 6.175786972045898 \n",
      "Epoch: 10260 | MAE Train Loss: 2.638072967529297 | MAE Test Loss: 6.168196678161621 \n",
      "Epoch: 10270 | MAE Train Loss: 2.6346447467803955 | MAE Test Loss: 6.1599202156066895 \n",
      "Epoch: 10280 | MAE Train Loss: 2.6312077045440674 | MAE Test Loss: 6.1516432762146 \n",
      "Epoch: 10290 | MAE Train Loss: 2.6277718544006348 | MAE Test Loss: 6.1440534591674805 \n",
      "Epoch: 10300 | MAE Train Loss: 2.624343156814575 | MAE Test Loss: 6.135776996612549 \n",
      "Epoch: 10310 | MAE Train Loss: 2.6209044456481934 | MAE Test Loss: 6.127500534057617 \n",
      "Epoch: 10320 | MAE Train Loss: 2.61747145652771 | MAE Test Loss: 6.11991024017334 \n",
      "Epoch: 10330 | MAE Train Loss: 2.614039897918701 | MAE Test Loss: 6.111633777618408 \n",
      "Epoch: 10340 | MAE Train Loss: 2.6106014251708984 | MAE Test Loss: 6.103356838226318 \n",
      "Epoch: 10350 | MAE Train Loss: 2.6071701049804688 | MAE Test Loss: 6.095767498016357 \n",
      "Epoch: 10360 | MAE Train Loss: 2.603733539581299 | MAE Test Loss: 6.087836265563965 \n",
      "Epoch: 10370 | MAE Train Loss: 2.600304365158081 | MAE Test Loss: 6.079559326171875 \n",
      "Epoch: 10380 | MAE Train Loss: 2.596865177154541 | MAE Test Loss: 6.071282863616943 \n",
      "Epoch: 10390 | MAE Train Loss: 2.593432664871216 | MAE Test Loss: 6.063693046569824 \n",
      "Epoch: 10400 | MAE Train Loss: 2.590001344680786 | MAE Test Loss: 6.055416584014893 \n",
      "Epoch: 10410 | MAE Train Loss: 2.5865626335144043 | MAE Test Loss: 6.047139644622803 \n",
      "Epoch: 10420 | MAE Train Loss: 2.583131790161133 | MAE Test Loss: 6.039549827575684 \n",
      "Epoch: 10430 | MAE Train Loss: 2.579697847366333 | MAE Test Loss: 6.031272888183594 \n",
      "Epoch: 10440 | MAE Train Loss: 2.576259136199951 | MAE Test Loss: 6.02299690246582 \n",
      "Epoch: 10450 | MAE Train Loss: 2.5728306770324707 | MAE Test Loss: 6.015406131744385 \n",
      "Epoch: 10460 | MAE Train Loss: 2.569394588470459 | MAE Test Loss: 6.007128715515137 \n",
      "Epoch: 10470 | MAE Train Loss: 2.565958023071289 | MAE Test Loss: 5.999540328979492 \n",
      "Epoch: 10480 | MAE Train Loss: 2.5625295639038086 | MAE Test Loss: 5.991262912750244 \n",
      "Epoch: 10490 | MAE Train Loss: 2.559091806411743 | MAE Test Loss: 5.982986927032471 \n",
      "Epoch: 10500 | MAE Train Loss: 2.555656909942627 | MAE Test Loss: 5.975396633148193 \n",
      "Epoch: 10510 | MAE Train Loss: 2.552227020263672 | MAE Test Loss: 5.967120170593262 \n",
      "Epoch: 10520 | MAE Train Loss: 2.548788070678711 | MAE Test Loss: 5.958843231201172 \n",
      "Epoch: 10530 | MAE Train Loss: 2.545356035232544 | MAE Test Loss: 5.951253414154053 \n",
      "Epoch: 10540 | MAE Train Loss: 2.5419232845306396 | MAE Test Loss: 5.942976951599121 \n",
      "Epoch: 10550 | MAE Train Loss: 2.538485050201416 | MAE Test Loss: 5.934700965881348 \n",
      "Epoch: 10560 | MAE Train Loss: 2.535055637359619 | MAE Test Loss: 5.927109718322754 \n",
      "Epoch: 10570 | MAE Train Loss: 2.531620740890503 | MAE Test Loss: 5.9188337326049805 \n",
      "Epoch: 10580 | MAE Train Loss: 2.5281822681427 | MAE Test Loss: 5.911243915557861 \n",
      "Epoch: 10590 | MAE Train Loss: 2.524754047393799 | MAE Test Loss: 5.90296745300293 \n",
      "Epoch: 10600 | MAE Train Loss: 2.5213170051574707 | MAE Test Loss: 5.894690036773682 \n",
      "Epoch: 10610 | MAE Train Loss: 2.517881393432617 | MAE Test Loss: 5.887100696563721 \n",
      "Epoch: 10620 | MAE Train Loss: 2.5144526958465576 | MAE Test Loss: 5.878823757171631 \n",
      "Epoch: 10630 | MAE Train Loss: 2.511013984680176 | MAE Test Loss: 5.870547294616699 \n",
      "Epoch: 10640 | MAE Train Loss: 2.507580280303955 | MAE Test Loss: 5.862957954406738 \n",
      "Epoch: 10650 | MAE Train Loss: 2.5041496753692627 | MAE Test Loss: 5.85468053817749 \n",
      "Epoch: 10660 | MAE Train Loss: 2.5007107257843018 | MAE Test Loss: 5.846404075622559 \n",
      "Epoch: 10670 | MAE Train Loss: 2.497279644012451 | MAE Test Loss: 5.8388142585754395 \n",
      "Epoch: 10680 | MAE Train Loss: 2.4938464164733887 | MAE Test Loss: 5.830538272857666 \n",
      "Epoch: 10690 | MAE Train Loss: 2.4904072284698486 | MAE Test Loss: 5.822260856628418 \n",
      "Epoch: 10700 | MAE Train Loss: 2.486978530883789 | MAE Test Loss: 5.814671516418457 \n",
      "Epoch: 10710 | MAE Train Loss: 2.4835431575775146 | MAE Test Loss: 5.806394100189209 \n",
      "Epoch: 10720 | MAE Train Loss: 2.4801058769226074 | MAE Test Loss: 5.798804759979248 \n",
      "Epoch: 10730 | MAE Train Loss: 2.476677656173706 | MAE Test Loss: 5.790527820587158 \n",
      "Epoch: 10740 | MAE Train Loss: 2.4732398986816406 | MAE Test Loss: 5.782250881195068 \n",
      "Epoch: 10750 | MAE Train Loss: 2.4698047637939453 | MAE Test Loss: 5.774662494659424 \n",
      "Epoch: 10760 | MAE Train Loss: 2.4663758277893066 | MAE Test Loss: 5.766384601593018 \n",
      "Epoch: 10770 | MAE Train Loss: 2.4629364013671875 | MAE Test Loss: 5.758108139038086 \n",
      "Epoch: 10780 | MAE Train Loss: 2.4595038890838623 | MAE Test Loss: 5.750518321990967 \n",
      "Epoch: 10790 | MAE Train Loss: 2.4560723304748535 | MAE Test Loss: 5.742241382598877 \n",
      "Epoch: 10800 | MAE Train Loss: 2.4526333808898926 | MAE Test Loss: 5.733964920043945 \n",
      "Epoch: 10810 | MAE Train Loss: 2.4492030143737793 | MAE Test Loss: 5.726375102996826 \n",
      "Epoch: 10820 | MAE Train Loss: 2.4457690715789795 | MAE Test Loss: 5.7180986404418945 \n",
      "Epoch: 10830 | MAE Train Loss: 2.4423301219940186 | MAE Test Loss: 5.709822177886963 \n",
      "Epoch: 10840 | MAE Train Loss: 2.438901901245117 | MAE Test Loss: 5.7022318840026855 \n",
      "Epoch: 10850 | MAE Train Loss: 2.4354658126831055 | MAE Test Loss: 5.693955421447754 \n",
      "Epoch: 10860 | MAE Train Loss: 2.4320290088653564 | MAE Test Loss: 5.686365604400635 \n",
      "Epoch: 10870 | MAE Train Loss: 2.428601026535034 | MAE Test Loss: 5.678088665008545 \n",
      "Epoch: 10880 | MAE Train Loss: 2.4251625537872314 | MAE Test Loss: 5.669812202453613 \n",
      "Epoch: 10890 | MAE Train Loss: 2.4217281341552734 | MAE Test Loss: 5.662221908569336 \n",
      "Epoch: 10900 | MAE Train Loss: 2.4182980060577393 | MAE Test Loss: 5.653946399688721 \n",
      "Epoch: 10910 | MAE Train Loss: 2.414863348007202 | MAE Test Loss: 5.646014213562012 \n",
      "Epoch: 10920 | MAE Train Loss: 2.4114277362823486 | MAE Test Loss: 5.637738227844238 \n",
      "Epoch: 10930 | MAE Train Loss: 2.4079906940460205 | MAE Test Loss: 5.630148887634277 \n",
      "Epoch: 10940 | MAE Train Loss: 2.404562473297119 | MAE Test Loss: 5.621872425079346 \n",
      "Epoch: 10950 | MAE Train Loss: 2.4011242389678955 | MAE Test Loss: 5.613596439361572 \n",
      "Epoch: 10960 | MAE Train Loss: 2.3976902961730957 | MAE Test Loss: 5.606007099151611 \n",
      "Epoch: 10970 | MAE Train Loss: 2.3942599296569824 | MAE Test Loss: 5.597731113433838 \n",
      "Epoch: 10980 | MAE Train Loss: 2.3908212184906006 | MAE Test Loss: 5.589454650878906 \n",
      "Epoch: 10990 | MAE Train Loss: 2.387389659881592 | MAE Test Loss: 5.5818657875061035 \n",
      "Epoch: 11000 | MAE Train Loss: 2.3839569091796875 | MAE Test Loss: 5.57358980178833 \n",
      "Epoch: 11010 | MAE Train Loss: 2.3805179595947266 | MAE Test Loss: 5.565313816070557 \n",
      "Epoch: 11020 | MAE Train Loss: 2.377089023590088 | MAE Test Loss: 5.557724475860596 \n",
      "Epoch: 11030 | MAE Train Loss: 2.3736538887023926 | MAE Test Loss: 5.549448490142822 \n",
      "Epoch: 11040 | MAE Train Loss: 2.3702163696289062 | MAE Test Loss: 5.541859149932861 \n",
      "Epoch: 11050 | MAE Train Loss: 2.366788387298584 | MAE Test Loss: 5.533583164215088 \n",
      "Epoch: 11060 | MAE Train Loss: 2.363351345062256 | MAE Test Loss: 5.525306701660156 \n",
      "Epoch: 11070 | MAE Train Loss: 2.3599159717559814 | MAE Test Loss: 5.5177178382873535 \n",
      "Epoch: 11080 | MAE Train Loss: 2.3564867973327637 | MAE Test Loss: 5.50944185256958 \n",
      "Epoch: 11090 | MAE Train Loss: 2.353048324584961 | MAE Test Loss: 5.501165866851807 \n",
      "Epoch: 11100 | MAE Train Loss: 2.3496158123016357 | MAE Test Loss: 5.493235111236572 \n",
      "Epoch: 11110 | MAE Train Loss: 2.3461785316467285 | MAE Test Loss: 5.485645771026611 \n",
      "Epoch: 11120 | MAE Train Loss: 2.3427505493164062 | MAE Test Loss: 5.477369785308838 \n",
      "Epoch: 11130 | MAE Train Loss: 2.33931303024292 | MAE Test Loss: 5.469093322753906 \n",
      "Epoch: 11140 | MAE Train Loss: 2.335878849029541 | MAE Test Loss: 5.461503505706787 \n",
      "Epoch: 11150 | MAE Train Loss: 2.332448959350586 | MAE Test Loss: 5.453227519989014 \n",
      "Epoch: 11160 | MAE Train Loss: 2.329010486602783 | MAE Test Loss: 5.444952011108398 \n",
      "Epoch: 11170 | MAE Train Loss: 2.3255774974823 | MAE Test Loss: 5.437362194061279 \n",
      "Epoch: 11180 | MAE Train Loss: 2.32214617729187 | MAE Test Loss: 5.429086685180664 \n",
      "Epoch: 11190 | MAE Train Loss: 2.3187074661254883 | MAE Test Loss: 5.420811176300049 \n",
      "Epoch: 11200 | MAE Train Loss: 2.315276622772217 | MAE Test Loss: 5.41322135925293 \n",
      "Epoch: 11210 | MAE Train Loss: 2.311843156814575 | MAE Test Loss: 5.404945373535156 \n",
      "Epoch: 11220 | MAE Train Loss: 2.3084042072296143 | MAE Test Loss: 5.396668910980225 \n",
      "Epoch: 11230 | MAE Train Loss: 2.304976224899292 | MAE Test Loss: 5.389080047607422 \n",
      "Epoch: 11240 | MAE Train Loss: 2.3015401363372803 | MAE Test Loss: 5.380804061889648 \n",
      "Epoch: 11250 | MAE Train Loss: 2.2981033325195312 | MAE Test Loss: 5.373214244842529 \n",
      "Epoch: 11260 | MAE Train Loss: 2.294675350189209 | MAE Test Loss: 5.364938735961914 \n",
      "Epoch: 11270 | MAE Train Loss: 2.2912375926971436 | MAE Test Loss: 5.356662750244141 \n",
      "Epoch: 11280 | MAE Train Loss: 2.2878031730651855 | MAE Test Loss: 5.3490729331970215 \n",
      "Epoch: 11290 | MAE Train Loss: 2.2843730449676514 | MAE Test Loss: 5.340796947479248 \n",
      "Epoch: 11300 | MAE Train Loss: 2.2809345722198486 | MAE Test Loss: 5.332520961761475 \n",
      "Epoch: 11310 | MAE Train Loss: 2.2775020599365234 | MAE Test Loss: 5.3249311447143555 \n",
      "Epoch: 11320 | MAE Train Loss: 2.2740702629089355 | MAE Test Loss: 5.31665563583374 \n",
      "Epoch: 11330 | MAE Train Loss: 2.270631790161133 | MAE Test Loss: 5.308380126953125 \n",
      "Epoch: 11340 | MAE Train Loss: 2.2672019004821777 | MAE Test Loss: 5.300790786743164 \n",
      "Epoch: 11350 | MAE Train Loss: 2.2637674808502197 | MAE Test Loss: 5.292513847351074 \n",
      "Epoch: 11360 | MAE Train Loss: 2.260329008102417 | MAE Test Loss: 5.284924507141113 \n",
      "Epoch: 11370 | MAE Train Loss: 2.256901264190674 | MAE Test Loss: 5.276648998260498 \n",
      "Epoch: 11380 | MAE Train Loss: 2.253464460372925 | MAE Test Loss: 5.268373012542725 \n",
      "Epoch: 11390 | MAE Train Loss: 2.250028133392334 | MAE Test Loss: 5.2607831954956055 \n",
      "Epoch: 11400 | MAE Train Loss: 2.2466001510620117 | MAE Test Loss: 5.252507209777832 \n",
      "Epoch: 11410 | MAE Train Loss: 2.243161678314209 | MAE Test Loss: 5.2442307472229 \n",
      "Epoch: 11420 | MAE Train Loss: 2.23972749710083 | MAE Test Loss: 5.236641883850098 \n",
      "Epoch: 11430 | MAE Train Loss: 2.236297130584717 | MAE Test Loss: 5.228365898132324 \n",
      "Epoch: 11440 | MAE Train Loss: 2.232858896255493 | MAE Test Loss: 5.220089912414551 \n",
      "Epoch: 11450 | MAE Train Loss: 2.2294273376464844 | MAE Test Loss: 5.21250057220459 \n",
      "Epoch: 11460 | MAE Train Loss: 2.22599458694458 | MAE Test Loss: 5.204225063323975 \n",
      "Epoch: 11470 | MAE Train Loss: 2.2225561141967773 | MAE Test Loss: 5.195948600769043 \n",
      "Epoch: 11480 | MAE Train Loss: 2.2191267013549805 | MAE Test Loss: 5.188360214233398 \n",
      "Epoch: 11490 | MAE Train Loss: 2.215691328048706 | MAE Test Loss: 5.18008279800415 \n",
      "Epoch: 11500 | MAE Train Loss: 2.212254047393799 | MAE Test Loss: 5.172493934631348 \n",
      "Epoch: 11510 | MAE Train Loss: 2.2088255882263184 | MAE Test Loss: 5.164217948913574 \n",
      "Epoch: 11520 | MAE Train Loss: 2.2053885459899902 | MAE Test Loss: 5.155941486358643 \n",
      "Epoch: 11530 | MAE Train Loss: 2.201953172683716 | MAE Test Loss: 5.148352146148682 \n",
      "Epoch: 11540 | MAE Train Loss: 2.1985244750976562 | MAE Test Loss: 5.140075206756592 \n",
      "Epoch: 11550 | MAE Train Loss: 2.1950855255126953 | MAE Test Loss: 5.131799221038818 \n",
      "Epoch: 11560 | MAE Train Loss: 2.191652297973633 | MAE Test Loss: 5.124209403991699 \n",
      "Epoch: 11570 | MAE Train Loss: 2.1882214546203613 | MAE Test Loss: 5.115933418273926 \n",
      "Epoch: 11580 | MAE Train Loss: 2.1847825050354004 | MAE Test Loss: 5.107657432556152 \n",
      "Epoch: 11590 | MAE Train Loss: 2.18135142326355 | MAE Test Loss: 5.100067138671875 \n",
      "Epoch: 11600 | MAE Train Loss: 2.1779181957244873 | MAE Test Loss: 5.091791152954102 \n",
      "Epoch: 11610 | MAE Train Loss: 2.1744799613952637 | MAE Test Loss: 5.083514213562012 \n",
      "Epoch: 11620 | MAE Train Loss: 2.171050786972046 | MAE Test Loss: 5.075926303863525 \n",
      "Epoch: 11630 | MAE Train Loss: 2.1676154136657715 | MAE Test Loss: 5.067648410797119 \n",
      "Epoch: 11640 | MAE Train Loss: 2.1641783714294434 | MAE Test Loss: 5.060059070587158 \n",
      "Epoch: 11650 | MAE Train Loss: 2.160749912261963 | MAE Test Loss: 5.051783084869385 \n",
      "Epoch: 11660 | MAE Train Loss: 2.1573119163513184 | MAE Test Loss: 5.043506622314453 \n",
      "Epoch: 11670 | MAE Train Loss: 2.1538772583007812 | MAE Test Loss: 5.035916328430176 \n",
      "Epoch: 11680 | MAE Train Loss: 2.150447368621826 | MAE Test Loss: 5.027640342712402 \n",
      "Epoch: 11690 | MAE Train Loss: 2.1470091342926025 | MAE Test Loss: 5.019364356994629 \n",
      "Epoch: 11700 | MAE Train Loss: 2.1435763835906982 | MAE Test Loss: 5.01177453994751 \n",
      "Epoch: 11710 | MAE Train Loss: 2.1401450634002686 | MAE Test Loss: 5.003498077392578 \n",
      "Epoch: 11720 | MAE Train Loss: 2.1367058753967285 | MAE Test Loss: 4.9952216148376465 \n",
      "Epoch: 11730 | MAE Train Loss: 2.1332759857177734 | MAE Test Loss: 4.9876322746276855 \n",
      "Epoch: 11740 | MAE Train Loss: 2.1298415660858154 | MAE Test Loss: 4.979355812072754 \n",
      "Epoch: 11750 | MAE Train Loss: 2.1264030933380127 | MAE Test Loss: 4.971765995025635 \n",
      "Epoch: 11760 | MAE Train Loss: 2.1229751110076904 | MAE Test Loss: 4.963490009307861 \n",
      "Epoch: 11770 | MAE Train Loss: 2.1195385456085205 | MAE Test Loss: 4.955214023590088 \n",
      "Epoch: 11780 | MAE Train Loss: 2.116102457046509 | MAE Test Loss: 4.9476237297058105 \n",
      "Epoch: 11790 | MAE Train Loss: 2.1126742362976074 | MAE Test Loss: 4.939347743988037 \n",
      "Epoch: 11800 | MAE Train Loss: 2.1092352867126465 | MAE Test Loss: 4.9310712814331055 \n",
      "Epoch: 11810 | MAE Train Loss: 2.105801820755005 | MAE Test Loss: 4.923481464385986 \n",
      "Epoch: 11820 | MAE Train Loss: 2.1023709774017334 | MAE Test Loss: 4.9152045249938965 \n",
      "Epoch: 11830 | MAE Train Loss: 2.0989327430725098 | MAE Test Loss: 4.906929016113281 \n",
      "Epoch: 11840 | MAE Train Loss: 2.0955004692077637 | MAE Test Loss: 4.8993401527404785 \n",
      "Epoch: 11850 | MAE Train Loss: 2.0920677185058594 | MAE Test Loss: 4.8910627365112305 \n",
      "Epoch: 11860 | MAE Train Loss: 2.0886292457580566 | MAE Test Loss: 4.882786750793457 \n",
      "Epoch: 11870 | MAE Train Loss: 2.0851998329162598 | MAE Test Loss: 4.875196933746338 \n",
      "Epoch: 11880 | MAE Train Loss: 2.0817651748657227 | MAE Test Loss: 4.8669209480285645 \n",
      "Epoch: 11890 | MAE Train Loss: 2.078327178955078 | MAE Test Loss: 4.859330654144287 \n",
      "Epoch: 11900 | MAE Train Loss: 2.074899196624756 | MAE Test Loss: 4.851054668426514 \n",
      "Epoch: 11910 | MAE Train Loss: 2.0714619159698486 | MAE Test Loss: 4.84277868270874 \n",
      "Epoch: 11920 | MAE Train Loss: 2.068026304244995 | MAE Test Loss: 4.835188865661621 \n",
      "Epoch: 11930 | MAE Train Loss: 2.0645973682403564 | MAE Test Loss: 4.8269124031066895 \n",
      "Epoch: 11940 | MAE Train Loss: 2.0611588954925537 | MAE Test Loss: 4.818636417388916 \n",
      "Epoch: 11950 | MAE Train Loss: 2.057725667953491 | MAE Test Loss: 4.811046600341797 \n",
      "Epoch: 11960 | MAE Train Loss: 2.0542945861816406 | MAE Test Loss: 4.802770614624023 \n",
      "Epoch: 11970 | MAE Train Loss: 2.050855875015259 | MAE Test Loss: 4.794493675231934 \n",
      "Epoch: 11980 | MAE Train Loss: 2.0474250316619873 | MAE Test Loss: 4.786904335021973 \n",
      "Epoch: 11990 | MAE Train Loss: 2.0439913272857666 | MAE Test Loss: 4.778627872467041 \n",
      "Epoch: 12000 | MAE Train Loss: 2.040552854537964 | MAE Test Loss: 4.770351886749268 \n",
      "Epoch: 12010 | MAE Train Loss: 2.0371246337890625 | MAE Test Loss: 4.762762546539307 \n",
      "Epoch: 12020 | MAE Train Loss: 2.03368878364563 | MAE Test Loss: 4.754485607147217 \n",
      "Epoch: 12030 | MAE Train Loss: 2.0302517414093018 | MAE Test Loss: 4.746896266937256 \n",
      "Epoch: 12040 | MAE Train Loss: 2.0268235206604004 | MAE Test Loss: 4.738620281219482 \n",
      "Epoch: 12050 | MAE Train Loss: 2.023385524749756 | MAE Test Loss: 4.730343818664551 \n",
      "Epoch: 12060 | MAE Train Loss: 2.0199508666992188 | MAE Test Loss: 4.7227559089660645 \n",
      "Epoch: 12070 | MAE Train Loss: 2.0165212154388428 | MAE Test Loss: 4.714478015899658 \n",
      "Epoch: 12080 | MAE Train Loss: 2.0130820274353027 | MAE Test Loss: 4.706202030181885 \n",
      "Epoch: 12090 | MAE Train Loss: 2.0096499919891357 | MAE Test Loss: 4.698612213134766 \n",
      "Epoch: 12100 | MAE Train Loss: 2.006218194961548 | MAE Test Loss: 4.690335750579834 \n",
      "Epoch: 12110 | MAE Train Loss: 2.002779483795166 | MAE Test Loss: 4.6820597648620605 \n",
      "Epoch: 12120 | MAE Train Loss: 1.9993493556976318 | MAE Test Loss: 4.674469947814941 \n",
      "Epoch: 12130 | MAE Train Loss: 1.9959148168563843 | MAE Test Loss: 4.666193962097168 \n",
      "Epoch: 12140 | MAE Train Loss: 1.9924767017364502 | MAE Test Loss: 4.658604621887207 \n",
      "Epoch: 12150 | MAE Train Loss: 1.9890486001968384 | MAE Test Loss: 4.650328159332275 \n",
      "Epoch: 12160 | MAE Train Loss: 1.985612154006958 | MAE Test Loss: 4.642051696777344 \n",
      "Epoch: 12170 | MAE Train Loss: 1.9821758270263672 | MAE Test Loss: 4.634462356567383 \n",
      "Epoch: 12180 | MAE Train Loss: 1.9787477254867554 | MAE Test Loss: 4.626185894012451 \n",
      "Epoch: 12190 | MAE Train Loss: 1.9753090143203735 | MAE Test Loss: 4.6179094314575195 \n",
      "Epoch: 12200 | MAE Train Loss: 1.9718749523162842 | MAE Test Loss: 4.610321521759033 \n",
      "Epoch: 12210 | MAE Train Loss: 1.96844482421875 | MAE Test Loss: 4.602043628692627 \n",
      "Epoch: 12220 | MAE Train Loss: 1.9650061130523682 | MAE Test Loss: 4.5937676429748535 \n",
      "Epoch: 12230 | MAE Train Loss: 1.9615741968154907 | MAE Test Loss: 4.586178302764893 \n",
      "Epoch: 12240 | MAE Train Loss: 1.9581416845321655 | MAE Test Loss: 4.577901840209961 \n",
      "Epoch: 12250 | MAE Train Loss: 1.9547027349472046 | MAE Test Loss: 4.569624900817871 \n",
      "Epoch: 12260 | MAE Train Loss: 1.9512736797332764 | MAE Test Loss: 4.56203556060791 \n",
      "Epoch: 12270 | MAE Train Loss: 1.947838544845581 | MAE Test Loss: 4.553759574890137 \n",
      "Epoch: 12280 | MAE Train Loss: 1.9444010257720947 | MAE Test Loss: 4.546169757843018 \n",
      "Epoch: 12290 | MAE Train Loss: 1.940972924232483 | MAE Test Loss: 4.537893295288086 \n",
      "Epoch: 12300 | MAE Train Loss: 1.9375358819961548 | MAE Test Loss: 4.529616832733154 \n",
      "Epoch: 12310 | MAE Train Loss: 1.9340999126434326 | MAE Test Loss: 4.522027492523193 \n",
      "Epoch: 12320 | MAE Train Loss: 1.930670976638794 | MAE Test Loss: 4.513751029968262 \n",
      "Epoch: 12330 | MAE Train Loss: 1.9272327423095703 | MAE Test Loss: 4.505475044250488 \n",
      "Epoch: 12340 | MAE Train Loss: 1.9237991571426392 | MAE Test Loss: 4.497885227203369 \n",
      "Epoch: 12350 | MAE Train Loss: 1.9203681945800781 | MAE Test Loss: 4.489609241485596 \n",
      "Epoch: 12360 | MAE Train Loss: 1.9169296026229858 | MAE Test Loss: 4.4813313484191895 \n",
      "Epoch: 12370 | MAE Train Loss: 1.9134982824325562 | MAE Test Loss: 4.473742961883545 \n",
      "Epoch: 12380 | MAE Train Loss: 1.9100650548934937 | MAE Test Loss: 4.465466499328613 \n",
      "Epoch: 12390 | MAE Train Loss: 1.9066264629364014 | MAE Test Loss: 4.45719051361084 \n",
      "Epoch: 12400 | MAE Train Loss: 1.9031976461410522 | MAE Test Loss: 4.449600696563721 \n",
      "Epoch: 12410 | MAE Train Loss: 1.8997619152069092 | MAE Test Loss: 4.441324710845947 \n",
      "Epoch: 12420 | MAE Train Loss: 1.896324872970581 | MAE Test Loss: 4.433734893798828 \n",
      "Epoch: 12430 | MAE Train Loss: 1.8928970098495483 | MAE Test Loss: 4.4254584312438965 \n",
      "Epoch: 12440 | MAE Train Loss: 1.889458417892456 | MAE Test Loss: 4.417181968688965 \n",
      "Epoch: 12450 | MAE Train Loss: 1.8860238790512085 | MAE Test Loss: 4.409592151641846 \n",
      "Epoch: 12460 | MAE Train Loss: 1.8825944662094116 | MAE Test Loss: 4.401316165924072 \n",
      "Epoch: 12470 | MAE Train Loss: 1.8791558742523193 | MAE Test Loss: 4.393040180206299 \n",
      "Epoch: 12480 | MAE Train Loss: 1.8757236003875732 | MAE Test Loss: 4.38545036315918 \n",
      "Epoch: 12490 | MAE Train Loss: 1.872291922569275 | MAE Test Loss: 4.377173900604248 \n",
      "Epoch: 12500 | MAE Train Loss: 1.868852972984314 | MAE Test Loss: 4.368896961212158 \n",
      "Epoch: 12510 | MAE Train Loss: 1.8654228448867798 | MAE Test Loss: 4.361307621002197 \n",
      "Epoch: 12520 | MAE Train Loss: 1.8619883060455322 | MAE Test Loss: 4.353031635284424 \n",
      "Epoch: 12530 | MAE Train Loss: 1.8585500717163086 | MAE Test Loss: 4.345441818237305 \n",
      "Epoch: 12540 | MAE Train Loss: 1.8551219701766968 | MAE Test Loss: 4.337165355682373 \n",
      "Epoch: 12550 | MAE Train Loss: 1.8516852855682373 | MAE Test Loss: 4.3288893699646 \n",
      "Epoch: 12560 | MAE Train Loss: 1.848249077796936 | MAE Test Loss: 4.3212995529174805 \n",
      "Epoch: 12570 | MAE Train Loss: 1.8448207378387451 | MAE Test Loss: 4.313023090362549 \n",
      "Epoch: 12580 | MAE Train Loss: 1.8413822650909424 | MAE Test Loss: 4.304747104644775 \n",
      "Epoch: 12590 | MAE Train Loss: 1.8379485607147217 | MAE Test Loss: 4.297157287597656 \n",
      "Epoch: 12600 | MAE Train Loss: 1.8345180749893188 | MAE Test Loss: 4.288880825042725 \n",
      "Epoch: 12610 | MAE Train Loss: 1.8310792446136475 | MAE Test Loss: 4.280604362487793 \n",
      "Epoch: 12620 | MAE Train Loss: 1.8276478052139282 | MAE Test Loss: 4.273015022277832 \n",
      "Epoch: 12630 | MAE Train Loss: 1.8242149353027344 | MAE Test Loss: 4.2647385597229 \n",
      "Epoch: 12640 | MAE Train Loss: 1.8207759857177734 | MAE Test Loss: 4.256462574005127 \n",
      "Epoch: 12650 | MAE Train Loss: 1.8173469305038452 | MAE Test Loss: 4.248873233795166 \n",
      "Epoch: 12660 | MAE Train Loss: 1.8139121532440186 | MAE Test Loss: 4.240595817565918 \n",
      "Epoch: 12670 | MAE Train Loss: 1.810474157333374 | MAE Test Loss: 4.233006477355957 \n",
      "Epoch: 12680 | MAE Train Loss: 1.8070461750030518 | MAE Test Loss: 4.224730491638184 \n",
      "Epoch: 12690 | MAE Train Loss: 1.8036086559295654 | MAE Test Loss: 4.21645450592041 \n",
      "Epoch: 12700 | MAE Train Loss: 1.8001735210418701 | MAE Test Loss: 4.208865165710449 \n",
      "Epoch: 12710 | MAE Train Loss: 1.7967445850372314 | MAE Test Loss: 4.200589179992676 \n",
      "Epoch: 12720 | MAE Train Loss: 1.7933059930801392 | MAE Test Loss: 4.192313194274902 \n",
      "Epoch: 12730 | MAE Train Loss: 1.7898727655410767 | MAE Test Loss: 4.184723854064941 \n",
      "Epoch: 12740 | MAE Train Loss: 1.7864418029785156 | MAE Test Loss: 4.176447868347168 \n",
      "Epoch: 12750 | MAE Train Loss: 1.7830032110214233 | MAE Test Loss: 4.1681718826293945 \n",
      "Epoch: 12760 | MAE Train Loss: 1.7795722484588623 | MAE Test Loss: 4.160582542419434 \n",
      "Epoch: 12770 | MAE Train Loss: 1.7761389017105103 | MAE Test Loss: 4.15230655670166 \n",
      "Epoch: 12780 | MAE Train Loss: 1.7727000713348389 | MAE Test Loss: 4.144030570983887 \n",
      "Epoch: 12790 | MAE Train Loss: 1.7692714929580688 | MAE Test Loss: 4.1364426612854 \n",
      "Epoch: 12800 | MAE Train Loss: 1.7658361196517944 | MAE Test Loss: 4.128164768218994 \n",
      "Epoch: 12810 | MAE Train Loss: 1.7623993158340454 | MAE Test Loss: 4.120575904846191 \n",
      "Epoch: 12820 | MAE Train Loss: 1.7589709758758545 | MAE Test Loss: 4.112299919128418 \n",
      "Epoch: 12830 | MAE Train Loss: 1.755533218383789 | MAE Test Loss: 4.1040239334106445 \n",
      "Epoch: 12840 | MAE Train Loss: 1.7520984411239624 | MAE Test Loss: 4.096434593200684 \n",
      "Epoch: 12850 | MAE Train Loss: 1.7486686706542969 | MAE Test Loss: 4.08815860748291 \n",
      "Epoch: 12860 | MAE Train Loss: 1.7452303171157837 | MAE Test Loss: 4.079882621765137 \n",
      "Epoch: 12870 | MAE Train Loss: 1.7417978048324585 | MAE Test Loss: 4.072293281555176 \n",
      "Epoch: 12880 | MAE Train Loss: 1.738365888595581 | MAE Test Loss: 4.064016819000244 \n",
      "Epoch: 12890 | MAE Train Loss: 1.7349331378936768 | MAE Test Loss: 4.056086540222168 \n",
      "Epoch: 12900 | MAE Train Loss: 1.7314952611923218 | MAE Test Loss: 4.0478105545043945 \n",
      "Epoch: 12910 | MAE Train Loss: 1.7280609607696533 | MAE Test Loss: 4.040220737457275 \n",
      "Epoch: 12920 | MAE Train Loss: 1.7246309518814087 | MAE Test Loss: 4.031944751739502 \n",
      "Epoch: 12930 | MAE Train Loss: 1.7211921215057373 | MAE Test Loss: 4.023669242858887 \n",
      "Epoch: 12940 | MAE Train Loss: 1.7177600860595703 | MAE Test Loss: 4.016079425811768 \n",
      "Epoch: 12950 | MAE Train Loss: 1.7143287658691406 | MAE Test Loss: 4.007803440093994 \n",
      "Epoch: 12960 | MAE Train Loss: 1.710889458656311 | MAE Test Loss: 3.9995274543762207 \n",
      "Epoch: 12970 | MAE Train Loss: 1.7074592113494873 | MAE Test Loss: 3.9919381141662598 \n",
      "Epoch: 12980 | MAE Train Loss: 1.7040252685546875 | MAE Test Loss: 3.9836621284484863 \n",
      "Epoch: 12990 | MAE Train Loss: 1.7005866765975952 | MAE Test Loss: 3.9760727882385254 \n",
      "Epoch: 13000 | MAE Train Loss: 1.6971588134765625 | MAE Test Loss: 3.967796802520752 \n",
      "Epoch: 13010 | MAE Train Loss: 1.693722128868103 | MAE Test Loss: 3.959519863128662 \n",
      "Epoch: 13020 | MAE Train Loss: 1.6902862787246704 | MAE Test Loss: 3.9519314765930176 \n",
      "Epoch: 13030 | MAE Train Loss: 1.6868579387664795 | MAE Test Loss: 3.943655490875244 \n",
      "Epoch: 13040 | MAE Train Loss: 1.6834192276000977 | MAE Test Loss: 3.9353795051574707 \n",
      "Epoch: 13050 | MAE Train Loss: 1.6799854040145874 | MAE Test Loss: 3.9277901649475098 \n",
      "Epoch: 13060 | MAE Train Loss: 1.6765550374984741 | MAE Test Loss: 3.9195141792297363 \n",
      "Epoch: 13070 | MAE Train Loss: 1.673116683959961 | MAE Test Loss: 3.9112377166748047 \n",
      "Epoch: 13080 | MAE Train Loss: 1.6696850061416626 | MAE Test Loss: 3.903648853302002 \n",
      "Epoch: 13090 | MAE Train Loss: 1.6662523746490479 | MAE Test Loss: 3.8953723907470703 \n",
      "Epoch: 13100 | MAE Train Loss: 1.6628139019012451 | MAE Test Loss: 3.887096881866455 \n",
      "Epoch: 13110 | MAE Train Loss: 1.6593844890594482 | MAE Test Loss: 3.879507064819336 \n",
      "Epoch: 13120 | MAE Train Loss: 1.655949354171753 | MAE Test Loss: 3.8712315559387207 \n",
      "Epoch: 13130 | MAE Train Loss: 1.6525119543075562 | MAE Test Loss: 3.8636417388916016 \n",
      "Epoch: 13140 | MAE Train Loss: 1.6490837335586548 | MAE Test Loss: 3.8553662300109863 \n",
      "Epoch: 13150 | MAE Train Loss: 1.6456464529037476 | MAE Test Loss: 3.8470890522003174 \n",
      "Epoch: 13160 | MAE Train Loss: 1.6422111988067627 | MAE Test Loss: 3.8395004272460938 \n",
      "Epoch: 13170 | MAE Train Loss: 1.6387821435928345 | MAE Test Loss: 3.831223964691162 \n",
      "Epoch: 13180 | MAE Train Loss: 1.6353435516357422 | MAE Test Loss: 3.822948455810547 \n",
      "Epoch: 13190 | MAE Train Loss: 1.6319106817245483 | MAE Test Loss: 3.8153586387634277 \n",
      "Epoch: 13200 | MAE Train Loss: 1.6284793615341187 | MAE Test Loss: 3.8070826530456543 \n",
      "Epoch: 13210 | MAE Train Loss: 1.6250407695770264 | MAE Test Loss: 3.798807144165039 \n",
      "Epoch: 13220 | MAE Train Loss: 1.6216099262237549 | MAE Test Loss: 3.7912182807922363 \n",
      "Epoch: 13230 | MAE Train Loss: 1.6181758642196655 | MAE Test Loss: 3.7829418182373047 \n",
      "Epoch: 13240 | MAE Train Loss: 1.6147377490997314 | MAE Test Loss: 3.7746658325195312 \n",
      "Epoch: 13250 | MAE Train Loss: 1.611309289932251 | MAE Test Loss: 3.7670764923095703 \n",
      "Epoch: 13260 | MAE Train Loss: 1.6078736782073975 | MAE Test Loss: 3.7588000297546387 \n",
      "Epoch: 13270 | MAE Train Loss: 1.6044368743896484 | MAE Test Loss: 3.7512104511260986 \n",
      "Epoch: 13280 | MAE Train Loss: 1.601008653640747 | MAE Test Loss: 3.742933750152588 \n",
      "Epoch: 13290 | MAE Train Loss: 1.5975704193115234 | MAE Test Loss: 3.7346572875976562 \n",
      "Epoch: 13300 | MAE Train Loss: 1.5941356420516968 | MAE Test Loss: 3.727067232131958 \n",
      "Epoch: 13310 | MAE Train Loss: 1.5907058715820312 | MAE Test Loss: 3.7187905311584473 \n",
      "Epoch: 13320 | MAE Train Loss: 1.587267279624939 | MAE Test Loss: 3.7105140686035156 \n",
      "Epoch: 13330 | MAE Train Loss: 1.5838347673416138 | MAE Test Loss: 3.7029240131378174 \n",
      "Epoch: 13340 | MAE Train Loss: 1.5804026126861572 | MAE Test Loss: 3.6946473121643066 \n",
      "Epoch: 13350 | MAE Train Loss: 1.576964020729065 | MAE Test Loss: 3.686370372772217 \n",
      "Epoch: 13360 | MAE Train Loss: 1.5735337734222412 | MAE Test Loss: 3.6787807941436768 \n",
      "Epoch: 13370 | MAE Train Loss: 1.5700995922088623 | MAE Test Loss: 3.670504093170166 \n",
      "Epoch: 13380 | MAE Train Loss: 1.56666100025177 | MAE Test Loss: 3.662914276123047 \n",
      "Epoch: 13390 | MAE Train Loss: 1.5632325410842896 | MAE Test Loss: 3.654637098312378 \n",
      "Epoch: 13400 | MAE Train Loss: 1.5597960948944092 | MAE Test Loss: 3.6463611125946045 \n",
      "Epoch: 13410 | MAE Train Loss: 1.5563600063323975 | MAE Test Loss: 3.6387710571289062 \n",
      "Epoch: 13420 | MAE Train Loss: 1.5529316663742065 | MAE Test Loss: 3.6304943561553955 \n",
      "Epoch: 13430 | MAE Train Loss: 1.5494929552078247 | MAE Test Loss: 3.6222176551818848 \n",
      "Epoch: 13440 | MAE Train Loss: 1.546059012413025 | MAE Test Loss: 3.614626407623291 \n",
      "Epoch: 13450 | MAE Train Loss: 1.542628526687622 | MAE Test Loss: 3.6063523292541504 \n",
      "Epoch: 13460 | MAE Train Loss: 1.5391900539398193 | MAE Test Loss: 3.598074436187744 \n",
      "Epoch: 13470 | MAE Train Loss: 1.5357577800750732 | MAE Test Loss: 3.590484619140625 \n",
      "Epoch: 13480 | MAE Train Loss: 1.5323251485824585 | MAE Test Loss: 3.5822081565856934 \n",
      "Epoch: 13490 | MAE Train Loss: 1.5288867950439453 | MAE Test Loss: 3.5739314556121826 \n",
      "Epoch: 13500 | MAE Train Loss: 1.5254571437835693 | MAE Test Loss: 3.5663414001464844 \n",
      "Epoch: 13510 | MAE Train Loss: 1.5220218896865845 | MAE Test Loss: 3.5580646991729736 \n",
      "Epoch: 13520 | MAE Train Loss: 1.5185844898223877 | MAE Test Loss: 3.5504746437072754 \n",
      "Epoch: 13530 | MAE Train Loss: 1.5151562690734863 | MAE Test Loss: 3.5421981811523438 \n",
      "Epoch: 13540 | MAE Train Loss: 1.5117193460464478 | MAE Test Loss: 3.533921718597412 \n",
      "Epoch: 13550 | MAE Train Loss: 1.5082833766937256 | MAE Test Loss: 3.526331663131714 \n",
      "Epoch: 13560 | MAE Train Loss: 1.5048542022705078 | MAE Test Loss: 3.518054962158203 \n",
      "Epoch: 13570 | MAE Train Loss: 1.501415491104126 | MAE Test Loss: 3.5097782611846924 \n",
      "Epoch: 13580 | MAE Train Loss: 1.4979827404022217 | MAE Test Loss: 3.502188205718994 \n",
      "Epoch: 13590 | MAE Train Loss: 1.4945509433746338 | MAE Test Loss: 3.4939122200012207 \n",
      "Epoch: 13600 | MAE Train Loss: 1.4911123514175415 | MAE Test Loss: 3.4856362342834473 \n",
      "Epoch: 13610 | MAE Train Loss: 1.4876816272735596 | MAE Test Loss: 3.4780452251434326 \n",
      "Epoch: 13620 | MAE Train Loss: 1.4842478036880493 | MAE Test Loss: 3.46976900100708 \n",
      "Epoch: 13630 | MAE Train Loss: 1.480808973312378 | MAE Test Loss: 3.4614920616149902 \n",
      "Epoch: 13640 | MAE Train Loss: 1.477380633354187 | MAE Test Loss: 3.453902006149292 \n",
      "Epoch: 13650 | MAE Train Loss: 1.4739445447921753 | MAE Test Loss: 3.4456257820129395 \n",
      "Epoch: 13660 | MAE Train Loss: 1.4705077409744263 | MAE Test Loss: 3.438035249710083 \n",
      "Epoch: 13670 | MAE Train Loss: 1.467079758644104 | MAE Test Loss: 3.4297585487365723 \n",
      "Epoch: 13680 | MAE Train Loss: 1.4636414051055908 | MAE Test Loss: 3.421482563018799 \n",
      "Epoch: 13690 | MAE Train Loss: 1.4602066278457642 | MAE Test Loss: 3.4138922691345215 \n",
      "Epoch: 13700 | MAE Train Loss: 1.4567768573760986 | MAE Test Loss: 3.4056155681610107 \n",
      "Epoch: 13710 | MAE Train Loss: 1.4533380270004272 | MAE Test Loss: 3.397339344024658 \n",
      "Epoch: 13720 | MAE Train Loss: 1.4499059915542603 | MAE Test Loss: 3.389749050140381 \n",
      "Epoch: 13730 | MAE Train Loss: 1.4464733600616455 | MAE Test Loss: 3.3814728260040283 \n",
      "Epoch: 13740 | MAE Train Loss: 1.4430347681045532 | MAE Test Loss: 3.3731961250305176 \n",
      "Epoch: 13750 | MAE Train Loss: 1.4396049976348877 | MAE Test Loss: 3.365604877471924 \n",
      "Epoch: 13760 | MAE Train Loss: 1.4361703395843506 | MAE Test Loss: 3.3573296070098877 \n",
      "Epoch: 13770 | MAE Train Loss: 1.4327322244644165 | MAE Test Loss: 3.3497395515441895 \n",
      "Epoch: 13780 | MAE Train Loss: 1.4293040037155151 | MAE Test Loss: 3.3414626121520996 \n",
      "Epoch: 13790 | MAE Train Loss: 1.425866961479187 | MAE Test Loss: 3.333186388015747 \n",
      "Epoch: 13800 | MAE Train Loss: 1.422431230545044 | MAE Test Loss: 3.325596332550049 \n",
      "Epoch: 13810 | MAE Train Loss: 1.4190025329589844 | MAE Test Loss: 3.317319393157959 \n",
      "Epoch: 13820 | MAE Train Loss: 1.4155638217926025 | MAE Test Loss: 3.3090434074401855 \n",
      "Epoch: 13830 | MAE Train Loss: 1.412130355834961 | MAE Test Loss: 3.301453113555908 \n",
      "Epoch: 13840 | MAE Train Loss: 1.4086992740631104 | MAE Test Loss: 3.2931766510009766 \n",
      "Epoch: 13850 | MAE Train Loss: 1.4052609205245972 | MAE Test Loss: 3.284899950027466 \n",
      "Epoch: 13860 | MAE Train Loss: 1.4018293619155884 | MAE Test Loss: 3.2773098945617676 \n",
      "Epoch: 13870 | MAE Train Loss: 1.3983961343765259 | MAE Test Loss: 3.269033432006836 \n",
      "Epoch: 13880 | MAE Train Loss: 1.394957423210144 | MAE Test Loss: 3.260756731033325 \n",
      "Epoch: 13890 | MAE Train Loss: 1.3915283679962158 | MAE Test Loss: 3.253166675567627 \n",
      "Epoch: 13900 | MAE Train Loss: 1.3880928754806519 | MAE Test Loss: 3.2448887825012207 \n",
      "Epoch: 13910 | MAE Train Loss: 1.3846603631973267 | MAE Test Loss: 3.2369589805603027 \n",
      "Epoch: 13920 | MAE Train Loss: 1.381221890449524 | MAE Test Loss: 3.228682041168213 \n",
      "Epoch: 13930 | MAE Train Loss: 1.377790927886963 | MAE Test Loss: 3.221092700958252 \n",
      "Epoch: 13940 | MAE Train Loss: 1.3743572235107422 | MAE Test Loss: 3.212815523147583 \n",
      "Epoch: 13950 | MAE Train Loss: 1.37091863155365 | MAE Test Loss: 3.2045390605926514 \n",
      "Epoch: 13960 | MAE Train Loss: 1.3674899339675903 | MAE Test Loss: 3.1969494819641113 \n",
      "Epoch: 13970 | MAE Train Loss: 1.3640542030334473 | MAE Test Loss: 3.188673496246338 \n",
      "Epoch: 13980 | MAE Train Loss: 1.3606170415878296 | MAE Test Loss: 3.1810824871063232 \n",
      "Epoch: 13990 | MAE Train Loss: 1.3571890592575073 | MAE Test Loss: 3.1728057861328125 \n",
      "Epoch: 14000 | MAE Train Loss: 1.3537508249282837 | MAE Test Loss: 3.164529323577881 \n",
      "Epoch: 14010 | MAE Train Loss: 1.3503161668777466 | MAE Test Loss: 3.1569392681121826 \n",
      "Epoch: 14020 | MAE Train Loss: 1.346886396408081 | MAE Test Loss: 3.14866304397583 \n",
      "Epoch: 14030 | MAE Train Loss: 1.3434475660324097 | MAE Test Loss: 3.1403861045837402 \n",
      "Epoch: 14040 | MAE Train Loss: 1.340015172958374 | MAE Test Loss: 3.132796049118042 \n",
      "Epoch: 14050 | MAE Train Loss: 1.336582899093628 | MAE Test Loss: 3.1245198249816895 \n",
      "Epoch: 14060 | MAE Train Loss: 1.3331444263458252 | MAE Test Loss: 3.116243362426758 \n",
      "Epoch: 14070 | MAE Train Loss: 1.3297145366668701 | MAE Test Loss: 3.1086525917053223 \n",
      "Epoch: 14080 | MAE Train Loss: 1.3262799978256226 | MAE Test Loss: 3.1003761291503906 \n",
      "Epoch: 14090 | MAE Train Loss: 1.3228416442871094 | MAE Test Loss: 3.0927863121032715 \n",
      "Epoch: 14100 | MAE Train Loss: 1.319413423538208 | MAE Test Loss: 3.084510087966919 \n",
      "Epoch: 14110 | MAE Train Loss: 1.315976619720459 | MAE Test Loss: 3.076233386993408 \n",
      "Epoch: 14120 | MAE Train Loss: 1.3125407695770264 | MAE Test Loss: 3.0686421394348145 \n",
      "Epoch: 14130 | MAE Train Loss: 1.3091120719909668 | MAE Test Loss: 3.0603675842285156 \n",
      "Epoch: 14140 | MAE Train Loss: 1.3056732416152954 | MAE Test Loss: 3.0520901679992676 \n",
      "Epoch: 14150 | MAE Train Loss: 1.3022396564483643 | MAE Test Loss: 3.0445003509521484 \n",
      "Epoch: 14160 | MAE Train Loss: 1.2988088130950928 | MAE Test Loss: 3.0362231731414795 \n",
      "Epoch: 14170 | MAE Train Loss: 1.295370101928711 | MAE Test Loss: 3.027946949005127 \n",
      "Epoch: 14180 | MAE Train Loss: 1.2919385433197021 | MAE Test Loss: 3.020357131958008 \n",
      "Epoch: 14190 | MAE Train Loss: 1.2885057926177979 | MAE Test Loss: 3.012080669403076 \n",
      "Epoch: 14200 | MAE Train Loss: 1.2850669622421265 | MAE Test Loss: 3.0038037300109863 \n",
      "Epoch: 14210 | MAE Train Loss: 1.2816376686096191 | MAE Test Loss: 2.996213436126709 \n",
      "Epoch: 14220 | MAE Train Loss: 1.2782022953033447 | MAE Test Loss: 2.9879372119903564 \n",
      "Epoch: 14230 | MAE Train Loss: 1.274765133857727 | MAE Test Loss: 2.980347156524658 \n",
      "Epoch: 14240 | MAE Train Loss: 1.2713367938995361 | MAE Test Loss: 2.9720702171325684 \n",
      "Epoch: 14250 | MAE Train Loss: 1.2678992748260498 | MAE Test Loss: 2.963793992996216 \n",
      "Epoch: 14260 | MAE Train Loss: 1.2644639015197754 | MAE Test Loss: 2.9562039375305176 \n",
      "Epoch: 14270 | MAE Train Loss: 1.2610347270965576 | MAE Test Loss: 2.9479267597198486 \n",
      "Epoch: 14280 | MAE Train Loss: 1.2575958967208862 | MAE Test Loss: 2.9396519660949707 \n",
      "Epoch: 14290 | MAE Train Loss: 1.2541632652282715 | MAE Test Loss: 2.932060718536377 \n",
      "Epoch: 14300 | MAE Train Loss: 1.2507312297821045 | MAE Test Loss: 2.9237842559814453 \n",
      "Epoch: 14310 | MAE Train Loss: 1.2472927570343018 | MAE Test Loss: 2.9155077934265137 \n",
      "Epoch: 14320 | MAE Train Loss: 1.2438621520996094 | MAE Test Loss: 2.9079174995422363 \n",
      "Epoch: 14330 | MAE Train Loss: 1.2404282093048096 | MAE Test Loss: 2.8996410369873047 \n",
      "Epoch: 14340 | MAE Train Loss: 1.2369896173477173 | MAE Test Loss: 2.891364336013794 \n",
      "Epoch: 14350 | MAE Train Loss: 1.2335612773895264 | MAE Test Loss: 2.8837742805480957 \n",
      "Epoch: 14360 | MAE Train Loss: 1.2301241159439087 | MAE Test Loss: 2.875497817993164 \n",
      "Epoch: 14370 | MAE Train Loss: 1.2266881465911865 | MAE Test Loss: 2.867907762527466 \n",
      "Epoch: 14380 | MAE Train Loss: 1.2232601642608643 | MAE Test Loss: 2.859631299972534 \n",
      "Epoch: 14390 | MAE Train Loss: 1.219821810722351 | MAE Test Loss: 2.8513550758361816 \n",
      "Epoch: 14400 | MAE Train Loss: 1.2163872718811035 | MAE Test Loss: 2.843764543533325 \n",
      "Epoch: 14410 | MAE Train Loss: 1.2129571437835693 | MAE Test Loss: 2.8354885578155518 \n",
      "Epoch: 14420 | MAE Train Loss: 1.2095186710357666 | MAE Test Loss: 2.827211856842041 \n",
      "Epoch: 14430 | MAE Train Loss: 1.2060863971710205 | MAE Test Loss: 2.8196213245391846 \n",
      "Epoch: 14440 | MAE Train Loss: 1.2026541233062744 | MAE Test Loss: 2.8113465309143066 \n",
      "Epoch: 14450 | MAE Train Loss: 1.1992156505584717 | MAE Test Loss: 2.803070545196533 \n",
      "Epoch: 14460 | MAE Train Loss: 1.1957858800888062 | MAE Test Loss: 2.7954814434051514 \n",
      "Epoch: 14470 | MAE Train Loss: 1.1923514604568481 | MAE Test Loss: 2.787205457687378 \n",
      "Epoch: 14480 | MAE Train Loss: 1.1889135837554932 | MAE Test Loss: 2.779616594314575 \n",
      "Epoch: 14490 | MAE Train Loss: 1.1854854822158813 | MAE Test Loss: 2.771341323852539 \n",
      "Epoch: 14500 | MAE Train Loss: 1.1820489168167114 | MAE Test Loss: 2.763065814971924 \n",
      "Epoch: 14510 | MAE Train Loss: 1.1786123514175415 | MAE Test Loss: 2.755476474761963 \n",
      "Epoch: 14520 | MAE Train Loss: 1.1751846075057983 | MAE Test Loss: 2.7472004890441895 \n",
      "Epoch: 14530 | MAE Train Loss: 1.1717461347579956 | MAE Test Loss: 2.738924503326416 \n",
      "Epoch: 14540 | MAE Train Loss: 1.1683127880096436 | MAE Test Loss: 2.7313361167907715 \n",
      "Epoch: 14550 | MAE Train Loss: 1.1648821830749512 | MAE Test Loss: 2.723060131072998 \n",
      "Epoch: 14560 | MAE Train Loss: 1.1614434719085693 | MAE Test Loss: 2.7147839069366455 \n",
      "Epoch: 14570 | MAE Train Loss: 1.1580122709274292 | MAE Test Loss: 2.707195281982422 \n",
      "Epoch: 14580 | MAE Train Loss: 1.1545792818069458 | MAE Test Loss: 2.6989197731018066 \n",
      "Epoch: 14590 | MAE Train Loss: 1.1511409282684326 | MAE Test Loss: 2.690643787384033 \n",
      "Epoch: 14600 | MAE Train Loss: 1.1477117538452148 | MAE Test Loss: 2.6830551624298096 \n",
      "Epoch: 14610 | MAE Train Loss: 1.144276738166809 | MAE Test Loss: 2.674779176712036 \n",
      "Epoch: 14620 | MAE Train Loss: 1.1408393383026123 | MAE Test Loss: 2.6671905517578125 \n",
      "Epoch: 14630 | MAE Train Loss: 1.13741135597229 | MAE Test Loss: 2.6589150428771973 \n",
      "Epoch: 14640 | MAE Train Loss: 1.1339739561080933 | MAE Test Loss: 2.650639057159424 \n",
      "Epoch: 14650 | MAE Train Loss: 1.1305381059646606 | MAE Test Loss: 2.643049955368042 \n",
      "Epoch: 14660 | MAE Train Loss: 1.1271097660064697 | MAE Test Loss: 2.6347737312316895 \n",
      "Epoch: 14670 | MAE Train Loss: 1.123671531677246 | MAE Test Loss: 2.626497983932495 \n",
      "Epoch: 14680 | MAE Train Loss: 1.1202383041381836 | MAE Test Loss: 2.6189098358154297 \n",
      "Epoch: 14690 | MAE Train Loss: 1.116807222366333 | MAE Test Loss: 2.6106338500976562 \n",
      "Epoch: 14700 | MAE Train Loss: 1.1133687496185303 | MAE Test Loss: 2.602358341217041 \n",
      "Epoch: 14710 | MAE Train Loss: 1.1099379062652588 | MAE Test Loss: 2.59476900100708 \n",
      "Epoch: 14720 | MAE Train Loss: 1.1065044403076172 | MAE Test Loss: 2.5864930152893066 \n",
      "Epoch: 14730 | MAE Train Loss: 1.1030659675598145 | MAE Test Loss: 2.5782172679901123 \n",
      "Epoch: 14740 | MAE Train Loss: 1.0996373891830444 | MAE Test Loss: 2.5706286430358887 \n",
      "Epoch: 14750 | MAE Train Loss: 1.0962018966674805 | MAE Test Loss: 2.5623531341552734 \n",
      "Epoch: 14760 | MAE Train Loss: 1.0927650928497314 | MAE Test Loss: 2.5547642707824707 \n",
      "Epoch: 14770 | MAE Train Loss: 1.0893371105194092 | MAE Test Loss: 2.5464882850646973 \n",
      "Epoch: 14780 | MAE Train Loss: 1.0858991146087646 | MAE Test Loss: 2.538212299346924 \n",
      "Epoch: 14790 | MAE Train Loss: 1.0824637413024902 | MAE Test Loss: 2.5306236743927 \n",
      "Epoch: 14800 | MAE Train Loss: 1.0790350437164307 | MAE Test Loss: 2.5223476886749268 \n",
      "Epoch: 14810 | MAE Train Loss: 1.075596570968628 | MAE Test Loss: 2.5140719413757324 \n",
      "Epoch: 14820 | MAE Train Loss: 1.0721639394760132 | MAE Test Loss: 2.506483554840088 \n",
      "Epoch: 14830 | MAE Train Loss: 1.0687321424484253 | MAE Test Loss: 2.4982078075408936 \n",
      "Epoch: 14840 | MAE Train Loss: 1.0652996301651 | MAE Test Loss: 2.49027681350708 \n",
      "Epoch: 14850 | MAE Train Loss: 1.0618616342544556 | MAE Test Loss: 2.4820008277893066 \n",
      "Epoch: 14860 | MAE Train Loss: 1.0584266185760498 | MAE Test Loss: 2.474412202835083 \n",
      "Epoch: 14870 | MAE Train Loss: 1.0549975633621216 | MAE Test Loss: 2.4661364555358887 \n",
      "Epoch: 14880 | MAE Train Loss: 1.0515590906143188 | MAE Test Loss: 2.4578604698181152 \n",
      "Epoch: 14890 | MAE Train Loss: 1.0481265783309937 | MAE Test Loss: 2.4502720832824707 \n",
      "Epoch: 14900 | MAE Train Loss: 1.0446946620941162 | MAE Test Loss: 2.4419960975646973 \n",
      "Epoch: 14910 | MAE Train Loss: 1.041256308555603 | MAE Test Loss: 2.433720111846924 \n",
      "Epoch: 14920 | MAE Train Loss: 1.0378262996673584 | MAE Test Loss: 2.4261314868927 \n",
      "Epoch: 14930 | MAE Train Loss: 1.03439199924469 | MAE Test Loss: 2.4178555011749268 \n",
      "Epoch: 14940 | MAE Train Loss: 1.0309536457061768 | MAE Test Loss: 2.410266399383545 \n",
      "Epoch: 14950 | MAE Train Loss: 1.0275259017944336 | MAE Test Loss: 2.401991367340088 \n",
      "Epoch: 14960 | MAE Train Loss: 1.0240895748138428 | MAE Test Loss: 2.3937149047851562 \n",
      "Epoch: 14970 | MAE Train Loss: 1.0206533670425415 | MAE Test Loss: 2.3861262798309326 \n",
      "Epoch: 14980 | MAE Train Loss: 1.0172253847122192 | MAE Test Loss: 2.377850294113159 \n",
      "Epoch: 14990 | MAE Train Loss: 1.013786792755127 | MAE Test Loss: 2.369574785232544 \n",
      "Epoch: 15000 | MAE Train Loss: 1.0103524923324585 | MAE Test Loss: 2.361985445022583 \n",
      "Epoch: 15010 | MAE Train Loss: 1.0069228410720825 | MAE Test Loss: 2.3537096977233887 \n",
      "Epoch: 15020 | MAE Train Loss: 1.0034841299057007 | MAE Test Loss: 2.3454341888427734 \n",
      "Epoch: 15030 | MAE Train Loss: 1.0000524520874023 | MAE Test Loss: 2.3378453254699707 \n",
      "Epoch: 15040 | MAE Train Loss: 0.9966198205947876 | MAE Test Loss: 2.3295695781707764 \n",
      "Epoch: 15050 | MAE Train Loss: 0.9931815266609192 | MAE Test Loss: 2.3212943077087402 \n",
      "Epoch: 15060 | MAE Train Loss: 0.9897491335868835 | MAE Test Loss: 2.3133633136749268 \n",
      "Epoch: 15070 | MAE Train Loss: 0.9863152503967285 | MAE Test Loss: 2.305774211883545 \n",
      "Epoch: 15080 | MAE Train Loss: 0.9828853607177734 | MAE Test Loss: 2.2974982261657715 \n",
      "Epoch: 15090 | MAE Train Loss: 0.979446530342102 | MAE Test Loss: 2.2892227172851562 \n",
      "Epoch: 15100 | MAE Train Loss: 0.9760152101516724 | MAE Test Loss: 2.2816338539123535 \n",
      "Epoch: 15110 | MAE Train Loss: 0.9725824594497681 | MAE Test Loss: 2.2733585834503174 \n",
      "Epoch: 15120 | MAE Train Loss: 0.9691439867019653 | MAE Test Loss: 2.265082597732544 \n",
      "Epoch: 15130 | MAE Train Loss: 0.9657148122787476 | MAE Test Loss: 2.2574939727783203 \n",
      "Epoch: 15140 | MAE Train Loss: 0.9622796773910522 | MAE Test Loss: 2.2492175102233887 \n",
      "Epoch: 15150 | MAE Train Loss: 0.9588422775268555 | MAE Test Loss: 2.241628646850586 \n",
      "Epoch: 15160 | MAE Train Loss: 0.9554142951965332 | MAE Test Loss: 2.2333531379699707 \n",
      "Epoch: 15170 | MAE Train Loss: 0.9519772529602051 | MAE Test Loss: 2.2250771522521973 \n",
      "Epoch: 15180 | MAE Train Loss: 0.9485417604446411 | MAE Test Loss: 2.2174885272979736 \n",
      "Epoch: 15190 | MAE Train Loss: 0.9451128840446472 | MAE Test Loss: 2.2092125415802 \n",
      "Epoch: 15200 | MAE Train Loss: 0.941674530506134 | MAE Test Loss: 2.200936794281006 \n",
      "Epoch: 15210 | MAE Train Loss: 0.9382411241531372 | MAE Test Loss: 2.193347930908203 \n",
      "Epoch: 15220 | MAE Train Loss: 0.9348105192184448 | MAE Test Loss: 2.1850719451904297 \n",
      "Epoch: 15230 | MAE Train Loss: 0.9313716888427734 | MAE Test Loss: 2.1767966747283936 \n",
      "Epoch: 15240 | MAE Train Loss: 0.9279406666755676 | MAE Test Loss: 2.1692070960998535 \n",
      "Epoch: 15250 | MAE Train Loss: 0.9245043992996216 | MAE Test Loss: 2.1612770557403564 \n",
      "Epoch: 15260 | MAE Train Loss: 0.9210753440856934 | MAE Test Loss: 2.153001308441162 \n",
      "Epoch: 15270 | MAE Train Loss: 0.9176369905471802 | MAE Test Loss: 2.1447253227233887 \n",
      "Epoch: 15280 | MAE Train Loss: 0.9142040014266968 | MAE Test Loss: 2.137136936187744 \n",
      "Epoch: 15290 | MAE Train Loss: 0.9107729196548462 | MAE Test Loss: 2.1288599967956543 \n",
      "Epoch: 15300 | MAE Train Loss: 0.9073342084884644 | MAE Test Loss: 2.1205849647521973 \n",
      "Epoch: 15310 | MAE Train Loss: 0.9039033651351929 | MAE Test Loss: 2.1129958629608154 \n",
      "Epoch: 15320 | MAE Train Loss: 0.9004701375961304 | MAE Test Loss: 2.1047203540802 \n",
      "Epoch: 15330 | MAE Train Loss: 0.8970314860343933 | MAE Test Loss: 2.096444606781006 \n",
      "Epoch: 15340 | MAE Train Loss: 0.8936031460762024 | MAE Test Loss: 2.0888562202453613 \n",
      "Epoch: 15350 | MAE Train Loss: 0.890167236328125 | MAE Test Loss: 2.0805797576904297 \n",
      "Epoch: 15360 | MAE Train Loss: 0.8867306709289551 | MAE Test Loss: 2.072991132736206 \n",
      "Epoch: 15370 | MAE Train Loss: 0.8833025693893433 | MAE Test Loss: 2.0647149085998535 \n",
      "Epoch: 15380 | MAE Train Loss: 0.8798648715019226 | MAE Test Loss: 2.0564396381378174 \n",
      "Epoch: 15390 | MAE Train Loss: 0.8764301538467407 | MAE Test Loss: 2.0488510131835938 \n",
      "Epoch: 15400 | MAE Train Loss: 0.87300044298172 | MAE Test Loss: 2.0405755043029785 \n",
      "Epoch: 15410 | MAE Train Loss: 0.8695621490478516 | MAE Test Loss: 2.032299518585205 \n",
      "Epoch: 15420 | MAE Train Loss: 0.8661298751831055 | MAE Test Loss: 2.024710178375244 \n",
      "Epoch: 15430 | MAE Train Loss: 0.862697958946228 | MAE Test Loss: 2.016434907913208 \n",
      "Epoch: 15440 | MAE Train Loss: 0.8592594265937805 | MAE Test Loss: 2.0081584453582764 \n",
      "Epoch: 15450 | MAE Train Loss: 0.8558292388916016 | MAE Test Loss: 2.0005691051483154 \n",
      "Epoch: 15460 | MAE Train Loss: 0.852395236492157 | MAE Test Loss: 1.9922935962677002 \n",
      "Epoch: 15470 | MAE Train Loss: 0.8489567637443542 | MAE Test Loss: 1.9843631982803345 \n",
      "Epoch: 15480 | MAE Train Loss: 0.8455244302749634 | MAE Test Loss: 1.9760879278182983 \n",
      "Epoch: 15490 | MAE Train Loss: 0.8420926928520203 | MAE Test Loss: 1.9684985876083374 \n",
      "Epoch: 15500 | MAE Train Loss: 0.8386604189872742 | MAE Test Loss: 1.9602229595184326 \n",
      "Epoch: 15510 | MAE Train Loss: 0.8352218866348267 | MAE Test Loss: 1.9519474506378174 \n",
      "Epoch: 15520 | MAE Train Loss: 0.8317916989326477 | MAE Test Loss: 1.9443576335906982 \n",
      "Epoch: 15530 | MAE Train Loss: 0.8283576965332031 | MAE Test Loss: 1.9360824823379517 \n",
      "Epoch: 15540 | MAE Train Loss: 0.8249193429946899 | MAE Test Loss: 1.9284934997558594 \n",
      "Epoch: 15550 | MAE Train Loss: 0.8214914202690125 | MAE Test Loss: 1.9202178716659546 \n",
      "Epoch: 15560 | MAE Train Loss: 0.8180548548698425 | MAE Test Loss: 1.9119415283203125 \n",
      "Epoch: 15570 | MAE Train Loss: 0.8146188855171204 | MAE Test Loss: 1.904353380203247 \n",
      "Epoch: 15580 | MAE Train Loss: 0.8111909031867981 | MAE Test Loss: 1.8960773944854736 \n",
      "Epoch: 15590 | MAE Train Loss: 0.8077523112297058 | MAE Test Loss: 1.8878017663955688 \n",
      "Epoch: 15600 | MAE Train Loss: 0.8043185472488403 | MAE Test Loss: 1.8802127838134766 \n",
      "Epoch: 15610 | MAE Train Loss: 0.8008878827095032 | MAE Test Loss: 1.8719371557235718 \n",
      "Epoch: 15620 | MAE Train Loss: 0.7974499464035034 | MAE Test Loss: 1.8636611700057983 \n",
      "Epoch: 15630 | MAE Train Loss: 0.7940183877944946 | MAE Test Loss: 1.8560718297958374 \n",
      "Epoch: 15640 | MAE Train Loss: 0.7905856370925903 | MAE Test Loss: 1.8477966785430908 \n",
      "Epoch: 15650 | MAE Train Loss: 0.7871471643447876 | MAE Test Loss: 1.839521050453186 \n",
      "Epoch: 15660 | MAE Train Loss: 0.783714771270752 | MAE Test Loss: 1.8315902948379517 \n",
      "Epoch: 15670 | MAE Train Loss: 0.7802812457084656 | MAE Test Loss: 1.8240013122558594 \n",
      "Epoch: 15680 | MAE Train Loss: 0.7768505215644836 | MAE Test Loss: 1.8157256841659546 \n",
      "Epoch: 15690 | MAE Train Loss: 0.7734122276306152 | MAE Test Loss: 1.8074496984481812 \n",
      "Epoch: 15700 | MAE Train Loss: 0.7699810266494751 | MAE Test Loss: 1.7998603582382202 \n",
      "Epoch: 15710 | MAE Train Loss: 0.7665480375289917 | MAE Test Loss: 1.7915852069854736 \n",
      "Epoch: 15720 | MAE Train Loss: 0.763109564781189 | MAE Test Loss: 1.7833092212677002 \n",
      "Epoch: 15730 | MAE Train Loss: 0.7596803903579712 | MAE Test Loss: 1.775720238685608 \n",
      "Epoch: 15740 | MAE Train Loss: 0.7562452554702759 | MAE Test Loss: 1.7674449682235718 \n",
      "Epoch: 15750 | MAE Train Loss: 0.7528077960014343 | MAE Test Loss: 1.7598564624786377 \n",
      "Epoch: 15760 | MAE Train Loss: 0.7493799328804016 | MAE Test Loss: 1.7515796422958374 \n",
      "Epoch: 15770 | MAE Train Loss: 0.7459424734115601 | MAE Test Loss: 1.7433040142059326 \n",
      "Epoch: 15780 | MAE Train Loss: 0.74250727891922 | MAE Test Loss: 1.7357155084609985 \n",
      "Epoch: 15790 | MAE Train Loss: 0.7390784025192261 | MAE Test Loss: 1.727439522743225 \n",
      "Epoch: 15800 | MAE Train Loss: 0.7356401681900024 | MAE Test Loss: 1.7191635370254517 \n",
      "Epoch: 15810 | MAE Train Loss: 0.7322069406509399 | MAE Test Loss: 1.711574912071228 \n",
      "Epoch: 15820 | MAE Train Loss: 0.7287756204605103 | MAE Test Loss: 1.7032989263534546 \n",
      "Epoch: 15830 | MAE Train Loss: 0.7253372073173523 | MAE Test Loss: 1.6950232982635498 \n",
      "Epoch: 15840 | MAE Train Loss: 0.7219066619873047 | MAE Test Loss: 1.6874347925186157 \n",
      "Epoch: 15850 | MAE Train Loss: 0.7184731364250183 | MAE Test Loss: 1.6791584491729736 \n",
      "Epoch: 15860 | MAE Train Loss: 0.7150346636772156 | MAE Test Loss: 1.6708828210830688 \n",
      "Epoch: 15870 | MAE Train Loss: 0.7116060256958008 | MAE Test Loss: 1.6632941961288452 \n",
      "Epoch: 15880 | MAE Train Loss: 0.7081705927848816 | MAE Test Loss: 1.6550182104110718 \n",
      "Epoch: 15890 | MAE Train Loss: 0.7047381401062012 | MAE Test Loss: 1.6470874547958374 \n",
      "Epoch: 15900 | MAE Train Loss: 0.7012995481491089 | MAE Test Loss: 1.6388118267059326 \n",
      "Epoch: 15910 | MAE Train Loss: 0.6978693008422852 | MAE Test Loss: 1.6312229633331299 \n",
      "Epoch: 15920 | MAE Train Loss: 0.694435715675354 | MAE Test Loss: 1.622947335243225 \n",
      "Epoch: 15930 | MAE Train Loss: 0.6909972429275513 | MAE Test Loss: 1.6146713495254517 \n",
      "Epoch: 15940 | MAE Train Loss: 0.6875686645507812 | MAE Test Loss: 1.6070823669433594 \n",
      "Epoch: 15950 | MAE Train Loss: 0.6841328740119934 | MAE Test Loss: 1.5988070964813232 \n",
      "Epoch: 15960 | MAE Train Loss: 0.6806963086128235 | MAE Test Loss: 1.5912193059921265 \n",
      "Epoch: 15970 | MAE Train Loss: 0.6772683262825012 | MAE Test Loss: 1.582942247390747 \n",
      "Epoch: 15980 | MAE Train Loss: 0.6738302111625671 | MAE Test Loss: 1.5746662616729736 \n",
      "Epoch: 15990 | MAE Train Loss: 0.6703957319259644 | MAE Test Loss: 1.56707763671875 \n",
      "Epoch: 16000 | MAE Train Loss: 0.6669660806655884 | MAE Test Loss: 1.5588016510009766 \n",
      "Epoch: 16010 | MAE Train Loss: 0.6635274887084961 | MAE Test Loss: 1.5505263805389404 \n",
      "Epoch: 16020 | MAE Train Loss: 0.6600950956344604 | MAE Test Loss: 1.5429370403289795 \n",
      "Epoch: 16030 | MAE Train Loss: 0.6566631197929382 | MAE Test Loss: 1.534661889076233 \n",
      "Epoch: 16040 | MAE Train Loss: 0.6532252430915833 | MAE Test Loss: 1.5263855457305908 \n",
      "Epoch: 16050 | MAE Train Loss: 0.6497949361801147 | MAE Test Loss: 1.5187965631484985 \n",
      "Epoch: 16060 | MAE Train Loss: 0.6463607549667358 | MAE Test Loss: 1.5105209350585938 \n",
      "Epoch: 16070 | MAE Train Loss: 0.6429222822189331 | MAE Test Loss: 1.5025901794433594 \n",
      "Epoch: 16080 | MAE Train Loss: 0.6394899487495422 | MAE Test Loss: 1.4943145513534546 \n",
      "Epoch: 16090 | MAE Train Loss: 0.6360577344894409 | MAE Test Loss: 1.4867255687713623 \n",
      "Epoch: 16100 | MAE Train Loss: 0.6326255798339844 | MAE Test Loss: 1.4784504175186157 \n",
      "Epoch: 16110 | MAE Train Loss: 0.6291874051094055 | MAE Test Loss: 1.4701744318008423 \n",
      "Epoch: 16120 | MAE Train Loss: 0.6257575750350952 | MAE Test Loss: 1.4625850915908813 \n",
      "Epoch: 16130 | MAE Train Loss: 0.6223233342170715 | MAE Test Loss: 1.4543098211288452 \n",
      "Epoch: 16140 | MAE Train Loss: 0.6188849210739136 | MAE Test Loss: 1.4467204809188843 \n",
      "Epoch: 16150 | MAE Train Loss: 0.6154571771621704 | MAE Test Loss: 1.4384453296661377 \n",
      "Epoch: 16160 | MAE Train Loss: 0.6120203733444214 | MAE Test Loss: 1.4301693439483643 \n",
      "Epoch: 16170 | MAE Train Loss: 0.6085845828056335 | MAE Test Loss: 1.4225810766220093 \n",
      "Epoch: 16180 | MAE Train Loss: 0.6051561236381531 | MAE Test Loss: 1.4143043756484985 \n",
      "Epoch: 16190 | MAE Train Loss: 0.6017179489135742 | MAE Test Loss: 1.406028389930725 \n",
      "Epoch: 16200 | MAE Train Loss: 0.598284125328064 | MAE Test Loss: 1.3984397649765015 \n",
      "Epoch: 16210 | MAE Train Loss: 0.5948538780212402 | MAE Test Loss: 1.3901641368865967 \n",
      "Epoch: 16220 | MAE Train Loss: 0.5914151072502136 | MAE Test Loss: 1.3818886280059814 \n",
      "Epoch: 16230 | MAE Train Loss: 0.5879836082458496 | MAE Test Loss: 1.3742992877960205 \n",
      "Epoch: 16240 | MAE Train Loss: 0.5845509767532349 | MAE Test Loss: 1.3660240173339844 \n",
      "Epoch: 16250 | MAE Train Loss: 0.5811126828193665 | MAE Test Loss: 1.357748031616211 \n",
      "Epoch: 16260 | MAE Train Loss: 0.5776832699775696 | MAE Test Loss: 1.3501583337783813 \n",
      "Epoch: 16270 | MAE Train Loss: 0.5742484927177429 | MAE Test Loss: 1.3418834209442139 \n",
      "Epoch: 16280 | MAE Train Loss: 0.5708106756210327 | MAE Test Loss: 1.334294080734253 \n",
      "Epoch: 16290 | MAE Train Loss: 0.5673829913139343 | MAE Test Loss: 1.3260189294815063 \n",
      "Epoch: 16300 | MAE Train Loss: 0.5639463663101196 | MAE Test Loss: 1.3180873394012451 \n",
      "Epoch: 16310 | MAE Train Loss: 0.5605133175849915 | MAE Test Loss: 1.3098125457763672 \n",
      "Epoch: 16320 | MAE Train Loss: 0.5570749044418335 | MAE Test Loss: 1.301536202430725 \n",
      "Epoch: 16330 | MAE Train Loss: 0.55364590883255 | MAE Test Loss: 1.2939472198486328 \n",
      "Epoch: 16340 | MAE Train Loss: 0.5502107739448547 | MAE Test Loss: 1.2856719493865967 \n",
      "Epoch: 16350 | MAE Train Loss: 0.5467733144760132 | MAE Test Loss: 1.278082251548767 \n",
      "Epoch: 16360 | MAE Train Loss: 0.54334557056427 | MAE Test Loss: 1.2698074579238892 \n",
      "Epoch: 16370 | MAE Train Loss: 0.5399082899093628 | MAE Test Loss: 1.2615314722061157 \n",
      "Epoch: 16380 | MAE Train Loss: 0.5364727973937988 | MAE Test Loss: 1.253942847251892 \n",
      "Epoch: 16390 | MAE Train Loss: 0.5330437421798706 | MAE Test Loss: 1.24566650390625 \n",
      "Epoch: 16400 | MAE Train Loss: 0.5296056866645813 | MAE Test Loss: 1.2373912334442139 \n",
      "Epoch: 16410 | MAE Train Loss: 0.5261724591255188 | MAE Test Loss: 1.2298015356063843 \n",
      "Epoch: 16420 | MAE Train Loss: 0.5227413177490234 | MAE Test Loss: 1.2215263843536377 \n",
      "Epoch: 16430 | MAE Train Loss: 0.5193027257919312 | MAE Test Loss: 1.2132503986358643 \n",
      "Epoch: 16440 | MAE Train Loss: 0.5158718824386597 | MAE Test Loss: 1.205661416053772 \n",
      "Epoch: 16450 | MAE Train Loss: 0.5124384760856628 | MAE Test Loss: 1.1973861455917358 \n",
      "Epoch: 16460 | MAE Train Loss: 0.5090001821517944 | MAE Test Loss: 1.1891101598739624 \n",
      "Epoch: 16470 | MAE Train Loss: 0.5055715441703796 | MAE Test Loss: 1.1815211772918701 \n",
      "Epoch: 16480 | MAE Train Loss: 0.5021360516548157 | MAE Test Loss: 1.1732451915740967 \n",
      "Epoch: 16490 | MAE Train Loss: 0.4986990988254547 | MAE Test Loss: 1.165656328201294 \n",
      "Epoch: 16500 | MAE Train Loss: 0.49527138471603394 | MAE Test Loss: 1.1573810577392578 \n",
      "Epoch: 16510 | MAE Train Loss: 0.49183329939842224 | MAE Test Loss: 1.1491050720214844 \n",
      "Epoch: 16520 | MAE Train Loss: 0.48839864134788513 | MAE Test Loss: 1.141516089439392 \n",
      "Epoch: 16530 | MAE Train Loss: 0.4849625527858734 | MAE Test Loss: 1.1328986883163452 \n",
      "Epoch: 16540 | MAE Train Loss: 0.4815341830253601 | MAE Test Loss: 1.1253101825714111 \n",
      "Epoch: 16550 | MAE Train Loss: 0.47809839248657227 | MAE Test Loss: 1.1170337200164795 \n",
      "Epoch: 16560 | MAE Train Loss: 0.4746617376804352 | MAE Test Loss: 1.1094448566436768 \n",
      "Epoch: 16570 | MAE Train Loss: 0.47123393416404724 | MAE Test Loss: 1.101169228553772 \n",
      "Epoch: 16580 | MAE Train Loss: 0.4677960276603699 | MAE Test Loss: 1.0928936004638672 \n",
      "Epoch: 16590 | MAE Train Loss: 0.46436136960983276 | MAE Test Loss: 1.085304617881775 \n",
      "Epoch: 16600 | MAE Train Loss: 0.46093159914016724 | MAE Test Loss: 1.0770286321640015 \n",
      "Epoch: 16610 | MAE Train Loss: 0.45749321579933167 | MAE Test Loss: 1.0687534809112549 \n",
      "Epoch: 16620 | MAE Train Loss: 0.45406094193458557 | MAE Test Loss: 1.0611636638641357 \n",
      "Epoch: 16630 | MAE Train Loss: 0.4506288468837738 | MAE Test Loss: 1.0528888702392578 \n",
      "Epoch: 16640 | MAE Train Loss: 0.4471905827522278 | MAE Test Loss: 1.0446128845214844 \n",
      "Epoch: 16650 | MAE Train Loss: 0.4437602162361145 | MAE Test Loss: 1.0370235443115234 \n",
      "Epoch: 16660 | MAE Train Loss: 0.4403262138366699 | MAE Test Loss: 1.0287479162216187 \n",
      "Epoch: 16670 | MAE Train Loss: 0.4368876516819 | MAE Test Loss: 1.0211585760116577 \n",
      "Epoch: 16680 | MAE Train Loss: 0.4334598481655121 | MAE Test Loss: 1.0128834247589111 \n",
      "Epoch: 16690 | MAE Train Loss: 0.4300234913825989 | MAE Test Loss: 1.0046074390411377 \n",
      "Epoch: 16700 | MAE Train Loss: 0.4265875816345215 | MAE Test Loss: 0.9970188140869141 \n",
      "Epoch: 16710 | MAE Train Loss: 0.4231528341770172 | MAE Test Loss: 0.9884017705917358 \n",
      "Epoch: 16720 | MAE Train Loss: 0.41972285509109497 | MAE Test Loss: 0.9808124303817749 \n",
      "Epoch: 16730 | MAE Train Loss: 0.41628867387771606 | MAE Test Loss: 0.9725364446640015 \n",
      "Epoch: 16740 | MAE Train Loss: 0.4128502905368805 | MAE Test Loss: 0.9649471044540405 \n",
      "Epoch: 16750 | MAE Train Loss: 0.40942248702049255 | MAE Test Loss: 0.9566718935966492 \n",
      "Epoch: 16760 | MAE Train Loss: 0.40598613023757935 | MAE Test Loss: 0.9483955502510071 \n",
      "Epoch: 16770 | MAE Train Loss: 0.4025501310825348 | MAE Test Loss: 0.9408073425292969 \n",
      "Epoch: 16780 | MAE Train Loss: 0.39912232756614685 | MAE Test Loss: 0.932534396648407 \n",
      "Epoch: 16790 | MAE Train Loss: 0.3956843316555023 | MAE Test Loss: 0.9242603182792664 \n",
      "Epoch: 16800 | MAE Train Loss: 0.39225101470947266 | MAE Test Loss: 0.916674017906189 \n",
      "Epoch: 16810 | MAE Train Loss: 0.3888210356235504 | MAE Test Loss: 0.9084011316299438 \n",
      "Epoch: 16820 | MAE Train Loss: 0.38538751006126404 | MAE Test Loss: 0.9004729986190796 \n",
      "Epoch: 16830 | MAE Train Loss: 0.3819515109062195 | MAE Test Loss: 0.892199695110321 \n",
      "Epoch: 16840 | MAE Train Loss: 0.3785158097743988 | MAE Test Loss: 0.8846119046211243 \n",
      "Epoch: 16850 | MAE Train Loss: 0.37508171796798706 | MAE Test Loss: 0.8759979009628296 \n",
      "Epoch: 16860 | MAE Train Loss: 0.3716520369052887 | MAE Test Loss: 0.868411660194397 \n",
      "Epoch: 16870 | MAE Train Loss: 0.3682186007499695 | MAE Test Loss: 0.860137939453125 \n",
      "Epoch: 16880 | MAE Train Loss: 0.36478090286254883 | MAE Test Loss: 0.8522106409072876 \n",
      "Epoch: 16890 | MAE Train Loss: 0.36134880781173706 | MAE Test Loss: 0.8439369201660156 \n",
      "Epoch: 16900 | MAE Train Loss: 0.35791701078414917 | MAE Test Loss: 0.8363502621650696 \n",
      "Epoch: 16910 | MAE Train Loss: 0.35448533296585083 | MAE Test Loss: 0.828076958656311 \n",
      "Epoch: 16920 | MAE Train Loss: 0.35104674100875854 | MAE Test Loss: 0.8198035955429077 \n",
      "Epoch: 16930 | MAE Train Loss: 0.3476184904575348 | MAE Test Loss: 0.8122169375419617 \n",
      "Epoch: 16940 | MAE Train Loss: 0.3441844880580902 | MAE Test Loss: 0.8039439916610718 \n",
      "Epoch: 16950 | MAE Train Loss: 0.34074658155441284 | MAE Test Loss: 0.7956706881523132 \n",
      "Epoch: 16960 | MAE Train Loss: 0.3373185992240906 | MAE Test Loss: 0.7880828976631165 \n",
      "Epoch: 16970 | MAE Train Loss: 0.3338831067085266 | MAE Test Loss: 0.7798107266426086 \n",
      "Epoch: 16980 | MAE Train Loss: 0.33044710755348206 | MAE Test Loss: 0.7722240686416626 \n",
      "Epoch: 16990 | MAE Train Loss: 0.327019602060318 | MAE Test Loss: 0.7639507055282593 \n",
      "Epoch: 17000 | MAE Train Loss: 0.32358217239379883 | MAE Test Loss: 0.7556774020195007 \n",
      "Epoch: 17010 | MAE Train Loss: 0.3201480805873871 | MAE Test Loss: 0.748090386390686 \n",
      "Epoch: 17020 | MAE Train Loss: 0.3167187571525574 | MAE Test Loss: 0.7398170232772827 \n",
      "Epoch: 17030 | MAE Train Loss: 0.31328049302101135 | MAE Test Loss: 0.7315441370010376 \n",
      "Epoch: 17040 | MAE Train Loss: 0.30984801054000854 | MAE Test Loss: 0.7239570617675781 \n",
      "Epoch: 17050 | MAE Train Loss: 0.30641794204711914 | MAE Test Loss: 0.7156841158866882 \n",
      "Epoch: 17060 | MAE Train Loss: 0.30297955870628357 | MAE Test Loss: 0.7074108123779297 \n",
      "Epoch: 17070 | MAE Train Loss: 0.29954975843429565 | MAE Test Loss: 0.6998241543769836 \n",
      "Epoch: 17080 | MAE Train Loss: 0.29611653089523315 | MAE Test Loss: 0.6915504336357117 \n",
      "Epoch: 17090 | MAE Train Loss: 0.2926783561706543 | MAE Test Loss: 0.6832793951034546 \n",
      "Epoch: 17100 | MAE Train Loss: 0.28925055265426636 | MAE Test Loss: 0.6756908297538757 \n",
      "Epoch: 17110 | MAE Train Loss: 0.2858152389526367 | MAE Test Loss: 0.6674175262451172 \n",
      "Epoch: 17120 | MAE Train Loss: 0.28237906098365784 | MAE Test Loss: 0.6598308682441711 \n",
      "Epoch: 17130 | MAE Train Loss: 0.27895134687423706 | MAE Test Loss: 0.6515579223632812 \n",
      "Epoch: 17140 | MAE Train Loss: 0.2755139470100403 | MAE Test Loss: 0.6432846188545227 \n",
      "Epoch: 17150 | MAE Train Loss: 0.2720794677734375 | MAE Test Loss: 0.6356975436210632 \n",
      "Epoch: 17160 | MAE Train Loss: 0.268644243478775 | MAE Test Loss: 0.6270831823348999 \n",
      "Epoch: 17170 | MAE Train Loss: 0.26521626114845276 | MAE Test Loss: 0.6194965243339539 \n",
      "Epoch: 17180 | MAE Train Loss: 0.26178112626075745 | MAE Test Loss: 0.6112232208251953 \n",
      "Epoch: 17190 | MAE Train Loss: 0.25834932923316956 | MAE Test Loss: 0.6032951474189758 \n",
      "Epoch: 17200 | MAE Train Loss: 0.254911333322525 | MAE Test Loss: 0.5950214266777039 \n",
      "Epoch: 17210 | MAE Train Loss: 0.2514810562133789 | MAE Test Loss: 0.5874351263046265 \n",
      "Epoch: 17220 | MAE Train Loss: 0.24804572761058807 | MAE Test Loss: 0.579507052898407 \n",
      "Epoch: 17230 | MAE Train Loss: 0.24461659789085388 | MAE Test Loss: 0.5712341070175171 \n",
      "Epoch: 17240 | MAE Train Loss: 0.2411785125732422 | MAE Test Loss: 0.5629608035087585 \n",
      "Epoch: 17250 | MAE Train Loss: 0.23774585127830505 | MAE Test Loss: 0.5553730130195618 \n",
      "Epoch: 17260 | MAE Train Loss: 0.23431530594825745 | MAE Test Loss: 0.5470997095108032 \n",
      "Epoch: 17270 | MAE Train Loss: 0.2308771163225174 | MAE Test Loss: 0.5388282537460327 \n",
      "Epoch: 17280 | MAE Train Loss: 0.22744664549827576 | MAE Test Loss: 0.5312408208847046 \n",
      "Epoch: 17290 | MAE Train Loss: 0.2240142822265625 | MAE Test Loss: 0.5229679346084595 \n",
      "Epoch: 17300 | MAE Train Loss: 0.22057600319385529 | MAE Test Loss: 0.5146942138671875 \n",
      "Epoch: 17310 | MAE Train Loss: 0.21714754402637482 | MAE Test Loss: 0.507107138633728 \n",
      "Epoch: 17320 | MAE Train Loss: 0.21371260285377502 | MAE Test Loss: 0.49883460998535156 \n",
      "Epoch: 17330 | MAE Train Loss: 0.21027497947216034 | MAE Test Loss: 0.4912475645542145 \n",
      "Epoch: 17340 | MAE Train Loss: 0.20684918761253357 | MAE Test Loss: 0.4829746186733246 \n",
      "Epoch: 17350 | MAE Train Loss: 0.20341166853904724 | MAE Test Loss: 0.47470131516456604 \n",
      "Epoch: 17360 | MAE Train Loss: 0.1999768316745758 | MAE Test Loss: 0.46711426973342896 \n",
      "Epoch: 17370 | MAE Train Loss: 0.19654837250709534 | MAE Test Loss: 0.458840548992157 \n",
      "Epoch: 17380 | MAE Train Loss: 0.19311046600341797 | MAE Test Loss: 0.45056837797164917 \n",
      "Epoch: 17390 | MAE Train Loss: 0.18967752158641815 | MAE Test Loss: 0.44298094511032104 \n",
      "Epoch: 17400 | MAE Train Loss: 0.1862470656633377 | MAE Test Loss: 0.4347076416015625 \n",
      "Epoch: 17410 | MAE Train Loss: 0.18280944228172302 | MAE Test Loss: 0.4264346957206726 \n",
      "Epoch: 17420 | MAE Train Loss: 0.17937831580638885 | MAE Test Loss: 0.41884803771972656 \n",
      "Epoch: 17430 | MAE Train Loss: 0.17594575881958008 | MAE Test Loss: 0.410574734210968 \n",
      "Epoch: 17440 | MAE Train Loss: 0.17250776290893555 | MAE Test Loss: 0.4023014008998871 \n",
      "Epoch: 17450 | MAE Train Loss: 0.16907843947410583 | MAE Test Loss: 0.3947151303291321 \n",
      "Epoch: 17460 | MAE Train Loss: 0.1656448394060135 | MAE Test Loss: 0.3864414095878601 \n",
      "Epoch: 17470 | MAE Train Loss: 0.162207692861557 | MAE Test Loss: 0.37885475158691406 \n",
      "Epoch: 17480 | MAE Train Loss: 0.15878000855445862 | MAE Test Loss: 0.3705814480781555 \n",
      "Epoch: 17490 | MAE Train Loss: 0.15534400939941406 | MAE Test Loss: 0.36265334486961365 \n",
      "Epoch: 17500 | MAE Train Loss: 0.1519111692905426 | MAE Test Loss: 0.35438042879104614 \n",
      "Epoch: 17510 | MAE Train Loss: 0.14847469329833984 | MAE Test Loss: 0.3461070954799652 \n",
      "Epoch: 17520 | MAE Train Loss: 0.14504222571849823 | MAE Test Loss: 0.3381786346435547 \n",
      "Epoch: 17530 | MAE Train Loss: 0.1416091024875641 | MAE Test Loss: 0.33059197664260864 \n",
      "Epoch: 17540 | MAE Train Loss: 0.1381787359714508 | MAE Test Loss: 0.32231903076171875 \n",
      "Epoch: 17550 | MAE Train Loss: 0.13474521040916443 | MAE Test Loss: 0.31439095735549927 \n",
      "Epoch: 17560 | MAE Train Loss: 0.13130931556224823 | MAE Test Loss: 0.3061183989048004 \n",
      "Epoch: 17570 | MAE Train Loss: 0.12787361443042755 | MAE Test Loss: 0.2985309660434723 \n",
      "Epoch: 17580 | MAE Train Loss: 0.12444619834423065 | MAE Test Loss: 0.29025763273239136 \n",
      "Epoch: 17590 | MAE Train Loss: 0.12100811302661896 | MAE Test Loss: 0.28198471665382385 \n",
      "Epoch: 17600 | MAE Train Loss: 0.11757459491491318 | MAE Test Loss: 0.2743972837924957 \n",
      "Epoch: 17610 | MAE Train Loss: 0.11414451897144318 | MAE Test Loss: 0.26612433791160583 \n",
      "Epoch: 17620 | MAE Train Loss: 0.1107061356306076 | MAE Test Loss: 0.25785142183303833 \n",
      "Epoch: 17630 | MAE Train Loss: 0.1072760596871376 | MAE Test Loss: 0.2502647340297699 \n",
      "Epoch: 17640 | MAE Train Loss: 0.10384359210729599 | MAE Test Loss: 0.2419910430908203 \n",
      "Epoch: 17650 | MAE Train Loss: 0.10040588676929474 | MAE Test Loss: 0.23371772468090057 \n",
      "Epoch: 17660 | MAE Train Loss: 0.09697618335485458 | MAE Test Loss: 0.22613105177879333 \n",
      "Epoch: 17670 | MAE Train Loss: 0.09354238212108612 | MAE Test Loss: 0.2178596556186676 \n",
      "Epoch: 17680 | MAE Train Loss: 0.09010477364063263 | MAE Test Loss: 0.2102714478969574 \n",
      "Epoch: 17690 | MAE Train Loss: 0.08667688071727753 | MAE Test Loss: 0.2019977569580078 \n",
      "Epoch: 17700 | MAE Train Loss: 0.08324136584997177 | MAE Test Loss: 0.1937248259782791 \n",
      "Epoch: 17710 | MAE Train Loss: 0.07980547100305557 | MAE Test Loss: 0.18613815307617188 \n",
      "Epoch: 17720 | MAE Train Loss: 0.07637796550989151 | MAE Test Loss: 0.17786483466625214 \n",
      "Epoch: 17730 | MAE Train Loss: 0.07293977588415146 | MAE Test Loss: 0.1695915162563324 \n",
      "Epoch: 17740 | MAE Train Loss: 0.06950549781322479 | MAE Test Loss: 0.16200485825538635 \n",
      "Epoch: 17750 | MAE Train Loss: 0.06607703864574432 | MAE Test Loss: 0.1537315398454666 \n",
      "Epoch: 17760 | MAE Train Loss: 0.06263885647058487 | MAE Test Loss: 0.14545822143554688 \n",
      "Epoch: 17770 | MAE Train Loss: 0.05920715257525444 | MAE Test Loss: 0.13787154853343964 \n",
      "Epoch: 17780 | MAE Train Loss: 0.05577583238482475 | MAE Test Loss: 0.12959785759449005 \n",
      "Epoch: 17790 | MAE Train Loss: 0.05233754962682724 | MAE Test Loss: 0.1213252991437912 \n",
      "Epoch: 17800 | MAE Train Loss: 0.0489080436527729 | MAE Test Loss: 0.113739013671875 \n",
      "Epoch: 17810 | MAE Train Loss: 0.04547452926635742 | MAE Test Loss: 0.10546531528234482 \n",
      "Epoch: 17820 | MAE Train Loss: 0.04203653335571289 | MAE Test Loss: 0.09753723442554474 \n",
      "Epoch: 17830 | MAE Train Loss: 0.03860463947057724 | MAE Test Loss: 0.08926429599523544 \n",
      "Epoch: 17840 | MAE Train Loss: 0.03517274931073189 | MAE Test Loss: 0.08167877048254013 \n",
      "Epoch: 17850 | MAE Train Loss: 0.03174161911010742 | MAE Test Loss: 0.0734046921133995 \n",
      "Epoch: 17860 | MAE Train Loss: 0.028308486565947533 | MAE Test Loss: 0.06547622382640839 \n",
      "Epoch: 17870 | MAE Train Loss: 0.024871826171875 | MAE Test Loss: 0.0572025291621685 \n",
      "Epoch: 17880 | MAE Train Loss: 0.021437739953398705 | MAE Test Loss: 0.049616239964962006 \n",
      "Epoch: 17890 | MAE Train Loss: 0.01800842210650444 | MAE Test Loss: 0.04134254530072212 \n",
      "Epoch: 17900 | MAE Train Loss: 0.01457061804831028 | MAE Test Loss: 0.0330684669315815 \n",
      "Epoch: 17910 | MAE Train Loss: 0.011138534173369408 | MAE Test Loss: 0.025482559576630592 \n",
      "Epoch: 17920 | MAE Train Loss: 0.007707404904067516 | MAE Test Loss: 0.017209624871611595 \n",
      "Epoch: 17930 | MAE Train Loss: 0.004269313998520374 | MAE Test Loss: 0.008936310186982155 \n",
      "Epoch: 17940 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 17950 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 17960 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 17970 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 17980 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 17990 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18000 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18010 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18020 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18030 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18040 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18050 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18060 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18070 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18080 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18090 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18100 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18110 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18120 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18130 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18140 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18150 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18160 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18170 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18180 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18190 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18200 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18210 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18220 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18230 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18240 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18250 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18260 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18270 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18280 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18290 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18300 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18310 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18320 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18330 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18340 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18350 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18360 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18370 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18380 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18390 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18400 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18410 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18420 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18430 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18440 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18450 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18460 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18470 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18480 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18490 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18500 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18510 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18520 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18530 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18540 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18550 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18560 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18570 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18580 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18590 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18600 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18610 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18620 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18630 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18640 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18650 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18660 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18670 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18680 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18690 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18700 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18710 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18720 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18730 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18740 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18750 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18760 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18770 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18780 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18790 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18800 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18810 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18820 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18830 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18840 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18850 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18860 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18870 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18880 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18890 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18900 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18910 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18920 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18930 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18940 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18950 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18960 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18970 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18980 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 18990 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19000 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19010 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19020 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19030 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19040 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19050 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19060 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19070 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19080 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19090 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19100 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19110 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19120 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19130 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19140 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19150 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19160 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19170 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19180 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19190 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19200 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19210 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19220 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19230 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19240 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19250 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19260 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19270 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19280 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19290 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19300 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19310 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19320 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19330 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19340 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19350 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19360 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19370 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19380 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19390 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19400 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19410 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19420 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19430 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19440 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19450 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19460 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19470 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19480 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19490 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19500 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19510 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19520 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19530 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19540 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19550 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19560 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19570 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19580 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19590 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19600 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19610 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19620 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19630 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19640 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19650 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19660 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19670 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19680 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19690 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19700 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19710 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19720 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19730 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19740 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19750 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19760 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19770 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19780 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19790 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19800 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19810 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19820 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19830 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19840 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19850 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19860 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19870 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19880 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19890 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19900 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19910 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19920 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19930 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19940 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19950 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19960 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19970 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19980 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n",
      "Epoch: 19990 | MAE Train Loss: 0.007565593812614679 | MAE Test Loss: 0.0014816283946856856 \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs (how many times the model will pass over the training data)\n",
    "epochs = 20000\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "\n",
    "    # Put model in training mode (this is the default state of a model)\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_0(X_train)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_0(X_test)\n",
    "\n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x89da20>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model learned the following values for weights and bias:\n",
      "OrderedDict([('weights', tensor([-9.9912])), ('bias', tensor([50.0041]))])\n",
      "\n",
      "And the original values for weights and bias are:\n",
      "weights: -10.0, bias: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Find our model's learned parameters\n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "print(model_0.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0112],\n",
       "        [41.8114],\n",
       "        [41.6115],\n",
       "        [41.4117],\n",
       "        [41.2119],\n",
       "        [41.0121],\n",
       "        [40.8123],\n",
       "        [40.6124],\n",
       "        [40.4126],\n",
       "        [40.2128]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "# 2. Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  # 3. Make sure the calculations are done with the model and data on the same device\n",
    "  # in our case, we haven't setup device-agnostic code yet so our data and model are\n",
    "  # on the CPU by default.\n",
    "  # model_0.to(device)\n",
    "  # X_test = X_test.to(device)\n",
    "  y_preds = model_0(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\01_pytorch_workflow_model_0.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new instance of our model (this will be instantiated with random weights)\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Put the loaded model into evaluation mode\n",
    "loaded_model_0.eval()\n",
    "\n",
    "# 2. Use the inference mode context manager to make predictions\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare previous model predictions with loaded model predictions (these should be the same)\n",
    "y_preds == loaded_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# USANDO LA FUNCION NN.LINEAR EN VEZ DE DEFINIRLA NOSOTROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModelV2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Subclass nn.Module to make our model\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        self.linear_layer = nn.Linear(in_features=1, \n",
    "                                      out_features=1)\n",
    "    \n",
    "    # Define the forward computation (input data x flows through nn.Linear())\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to GPU if it's availalble, otherwise it'll default to CPU\n",
    "model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 100 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 200 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 300 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 400 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 500 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 600 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 700 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 800 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n",
      "Epoch: 900 | Train loss: 0.004988288972526789 | Test loss: 0.010038375854492188\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs \n",
    "epochs = 1000\n",
    "\n",
    "# Put data on the available device\n",
    "# Without this, error will happen (not all model/data on device)\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train() # train mode is on by default after construction\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval() # put the model in evaluation mode for testing (inference)\n",
    "    # 1. Forward pass\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "    \n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model learned the following values for weights and bias:\n",
      "OrderedDict([('linear_layer.weight', tensor([[-9.9969]], device='cuda:0')),\n",
      "             ('linear_layer.bias', tensor([49.9938], device='cuda:0'))])\n",
      "\n",
      "And the original values for weights and bias are:\n",
      "weights: -10.0, bias: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Find our model's learned parameters\n",
    "from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "pprint(model_1.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41.9963],\n",
       "        [41.7964],\n",
       "        [41.5964],\n",
       "        [41.3965],\n",
       "        [41.1965],\n",
       "        [40.9966],\n",
       "        [40.7967],\n",
       "        [40.5967],\n",
       "        [40.3968],\n",
       "        [40.1968]], device='cuda:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn model into evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_1(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_predictions(predictions=y_preds) # -> won't work... data not on CPU\n",
    "\n",
    "# Put data on the CPU and plot it\n",
    "plot_predictions(predictions=y_preds.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
